#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•æ¡†æ¶

ç”¨äºç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•å’Œå¯¹æ¯”ï¼š
- ç¼“å­˜æ€§èƒ½åŸºå‡†
- é€’å½’é™åˆ¶å™¨æ€§èƒ½åŸºå‡†
- ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨æ€§èƒ½åŸºå‡†
- ç»¼åˆæ€§èƒ½åŸºå‡†

ä½¿ç”¨æ–¹æ³•:
    # è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
    python tests/benchmark_test.py

    # è¿è¡Œç‰¹å®šåŸºå‡†
    python tests/benchmark_test.py --benchmark cache

    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    python tests/benchmark_test.py --report

ä½œè€…: AGI System
æ—¥æœŸ: 2026-02-04
"""

import sys
import time
import statistics
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Callable
from dataclasses import dataclass, asdict
import numpy as np

# è®¾ç½®Windowsæ§åˆ¶å°ç¼–ç 
if sys.platform == "win32":
    try:
        import codecs
        sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())
        sys.stderr = codecs.getwriter("utf-8")(sys.stderr.detach())
    except:
        pass

# å¯¼å…¥è¢«æµ‹è¯•æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.tool_call_cache import ToolCallCache
from core.memory.memory_lifecycle_manager import (
    MemoryLifecycleManager,
    EvictionPolicy,
)
from core.dynamic_recursion_limiter import DynamicRecursionLimiter


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    name: str
    iterations: int
    total_time: float
    avg_time: float
    min_time: float
    max_time: float
    median_time: float
    std_dev: float
    ops_per_second: float
    metadata: Dict[str, Any]

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


class BenchmarkRunner:
    """åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self):
        self.results = []

    def run_benchmark(
        self,
        name: str,
        func: Callable,
        iterations: int = 100,
        warmup: int = 10,
        **metadata
    ) -> BenchmarkResult:
        """
        è¿è¡ŒåŸºå‡†æµ‹è¯•

        Args:
            name: æµ‹è¯•åç§°
            func: æµ‹è¯•å‡½æ•°
            iterations: è¿­ä»£æ¬¡æ•°
            warmup: é¢„çƒ­æ¬¡æ•°
            **metadata: é¢å¤–å…ƒæ•°æ®

        Returns:
            BenchmarkResultå¯¹è±¡
        """
        print(f"ğŸ”§ è¿è¡ŒåŸºå‡†: {name}")
        print(f"   é¢„çƒ­: {warmup}æ¬¡")
        print(f"   è¿­ä»£: {iterations}æ¬¡")

        # é¢„çƒ­
        for _ in range(warmup):
            func()

        # åŸºå‡†æµ‹è¯•
        times = []
        for _ in range(iterations):
            start = time.perf_counter()
            func()
            end = time.perf_counter()
            times.append((end - start) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’

        # è®¡ç®—ç»Ÿè®¡
        total_time = sum(times)
        avg_time = statistics.mean(times)
        min_time = min(times)
        max_time = max(times)
        median_time = statistics.median(times)
        std_dev = statistics.stdev(times) if len(times) > 1 else 0
        ops_per_second = iterations / (total_time / 1000)

        result = BenchmarkResult(
            name=name,
            iterations=iterations,
            total_time=total_time,
            avg_time=avg_time,
            min_time=min_time,
            max_time=max_time,
            median_time=median_time,
            std_dev=std_dev,
            ops_per_second=ops_per_second,
            metadata=metadata
        )

        self.results.append(result)

        # æ‰“å°ç»“æœ
        print(f"   æ€»æ—¶é—´: {total_time:.2f}ms")
        print(f"   å¹³å‡: {avg_time:.3f}ms")
        print(f"   ä¸­ä½æ•°: {median_time:.3f}ms")
        print(f"   æ ‡å‡†å·®: {std_dev:.3f}ms")
        print(f"   ååé‡: {ops_per_second:.1f} ops/s")
        print()

        return result

    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("=" * 60)
        print("ğŸ“Š åŸºå‡†æµ‹è¯•æ€»ç»“")
        print("=" * 60)

        for result in self.results:
            print(f"\n{result.name}:")
            print(f"   å¹³å‡æ—¶é—´: {result.avg_time:.3f}ms")
            print(f"   ååé‡: {result.ops_per_second:.1f} ops/s")
            print(f"   æ ‡å‡†å·®: {result.std_dev:.3f}ms")

        # æ€§èƒ½æ’å
        print(f"\nğŸ† æ€§èƒ½æ’åï¼ˆæŒ‰å¹³å‡æ—¶é—´ï¼‰:")
        sorted_results = sorted(self.results, key=lambda r: r.avg_time)
        for i, result in enumerate(sorted_results, 1):
            print(f"   {i}. {result.name}: {result.avg_time:.3f}ms")

    def save_report(self, filepath: str):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "results": [r.to_dict() for r in self.results],
        }

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {filepath}")


# ============================================================================
# ç¼“å­˜æ€§èƒ½åŸºå‡†æµ‹è¯•
# ============================================================================

def benchmark_cache_put():
    """ç¼“å­˜PUTæ“ä½œåŸºå‡†"""
    cache = ToolCallCache(max_size=1000)

    def put_operation():
        tool_name = "test_tool"
        params = {"operation": "test", "id": np.random.randint(0, 1000)}
        result = {"data": "test result"}
        cache.put(tool_name, params, result)

    return put_operation


def benchmark_cache_get():
    """ç¼“å­˜GETæ“ä½œåŸºå‡†ï¼ˆå‘½ä¸­ï¼‰"""
    cache = ToolCallCache(max_size=1000)

    # é¢„å¡«å……ç¼“å­˜
    for i in range(100):
        cache.put("tool", {"id": i}, {"result": i})

    def get_operation():
        # éšæœºè·å–å·²å­˜åœ¨çš„é”®
        cache_id = np.random.randint(0, 100)
        cache.get("tool", {"id": cache_id})

    return get_operation


def benchmark_cache_get_miss():
    """ç¼“å­˜GETæ“ä½œåŸºå‡†ï¼ˆæœªå‘½ä¸­ï¼‰"""
    cache = ToolCallCache(max_size=1000)

    def get_miss_operation():
        # è·å–ä¸å­˜åœ¨çš„é”®
        cache.get("tool", {"id": np.random.randint(1000, 2000)})

    return get_miss_operation


def benchmark_cache_generate_key():
    """ç¼“å­˜é”®ç”ŸæˆåŸºå‡†"""
    cache = ToolCallCache(max_size=1000)

    def generate_key_operation():
        tool_name = "test_tool"
        params = {"operation": "test", "id": np.random.randint(0, 1000)}
        cache.generate_cache_key(tool_name, params)

    return generate_key_operation


def benchmark_cache_eviction():
    """ç¼“å­˜æ·˜æ±°åŸºå‡†"""
    cache = ToolCallCache(max_size=100)

    def eviction_operation():
        # æ·»åŠ è¶…è¿‡é™åˆ¶çš„è®°å½•ä»¥è§¦å‘æ·˜æ±°
        for i in range(101):
            cache.put(f"tool_{i}", {"id": i}, {"result": i})

    return eviction_operation


# ============================================================================
# ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨æ€§èƒ½åŸºå‡†æµ‹è¯•
# ============================================================================

def benchmark_lifecycle_register():
    """ç”Ÿå‘½å‘¨æœŸæ³¨å†ŒåŸºå‡†"""
    lifecycle = MemoryLifecycleManager(max_records=1000)

    def register_operation():
        memory_id = f"mem_{np.random.randint(0, 1000)}"
        lifecycle.register_record(
            memory_id=memory_id,
            importance_score=np.random.random(),
            tags=["benchmark"],
        )

    return register_operation


def benchmark_lifecycle_touch():
    """ç”Ÿå‘½å‘¨æœŸè®¿é—®æ›´æ–°åŸºå‡†"""
    lifecycle = MemoryLifecycleManager(max_records=1000)

    # é¢„å¡«å……è®°å½•
    for i in range(100):
        lifecycle.register_record(f"mem_{i}", importance_score=0.5)

    def touch_operation():
        memory_id = f"mem_{np.random.randint(0, 100)}"
        lifecycle.touch_record(memory_id)

    return touch_operation


def benchmark_lifecycle_evict_lru():
    """ç”Ÿå‘½å‘¨æœŸLRUæ·˜æ±°åŸºå‡†"""
    lifecycle = MemoryLifecycleManager(
        max_records=100,
        eviction_policy=EvictionPolicy.LRU,
    )

    def eviction_operation():
        # æ·»åŠ è®°å½•
        for i in range(105):
            lifecycle.register_record(f"mem_{i}", importance_score=0.5)

    return eviction_operation


# ============================================================================
# é€’å½’é™åˆ¶å™¨æ€§èƒ½åŸºå‡†æµ‹è¯•
# ============================================================================

def benchmark_limiter_get_limit():
    """é€’å½’é™åˆ¶å™¨è·å–é™åˆ¶åŸºå‡†"""
    limiter = DynamicRecursionLimiter()

    def get_limit_operation():
        context = {"task_complexity": np.random.random()}
        limiter.get_current_limit(context)

    return get_limit_operation


def benchmark_limiter_record_performance():
    """é€’å½’é™åˆ¶å™¨è®°å½•æ€§èƒ½åŸºå‡†"""
    limiter = DynamicRecursionLimiter()

    def record_performance_operation():
        limiter.record_performance(
            depth=np.random.randint(1, 10),
            success=np.random.choice([True, False]),
            execution_time_ms=np.random.uniform(10, 200),
        )

    return record_performance_operation


# ============================================================================
# ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•
# ============================================================================

def benchmark_full_decision_flow():
    """å®Œæ•´å†³ç­–æµç¨‹åŸºå‡†"""
    cache = ToolCallCache(max_size=100)
    lifecycle = MemoryLifecycleManager(max_records=100)
    limiter = DynamicRecursionLimiter()

    def decision_flow_operation():
        # 1. è·å–é€’å½’é™åˆ¶
        context = {"task_complexity": 0.5}
        limit = limiter.get_current_limit(context)

        # 2. ç¼“å­˜æ“ä½œ
        tool_name = "test_tool"
        params = {"operation": "test", "id": np.random.randint(0, 100)}
        cached = cache.get(tool_name, params)

        if cached is None:
            result = {"success": True, "data": "test"}
            cache.put(tool_name, params, result)

        # 3. åˆ›å»ºè®°å¿†è®°å½•ï¼ˆæ¯10æ¬¡ï¼‰
        if np.random.randint(0, 10) == 0:
            cache_key = cache.generate_cache_key(tool_name, params)
            lifecycle.register_record(f"mem_{cache_key}", importance_score=0.5)

        # 4. è®°å½•æ€§èƒ½
        limiter.record_performance(
            depth=limit,
            success=True,
            execution_time_ms=50.0,
        )

    return decision_flow_operation


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def run_all_benchmarks(runner: BenchmarkRunner, iterations: int = 100):
    """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""

    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    print()

    # ç¼“å­˜åŸºå‡†æµ‹è¯•
    print("ğŸ“¦ ç¼“å­˜æ€§èƒ½åŸºå‡†")
    print("-" * 60)
    runner.run_benchmark(
        "Cache: PUT",
        benchmark_cache_put,
        iterations=iterations,
        warmup=10,
        component="cache",
        operation="put",
    )

    runner.run_benchmark(
        "Cache: GET (hit)",
        benchmark_cache_get,
        iterations=iterations,
        warmup=10,
        component="cache",
        operation="get_hit",
    )

    runner.run_benchmark(
        "Cache: GET (miss)",
        benchmark_cache_get_miss,
        iterations=iterations,
        warmup=10,
        component="cache",
        operation="get_miss",
    )

    runner.run_benchmark(
        "Cache: Generate Key",
        benchmark_cache_generate_key,
        iterations=iterations,
        warmup=10,
        component="cache",
        operation="generate_key",
    )

    runner.run_benchmark(
        "Cache: Eviction",
        benchmark_cache_eviction,
        iterations=10,  # è¾ƒå°‘è¿­ä»£ï¼Œå› ä¸ºæ¯æ¬¡101æ¬¡æ“ä½œ
        warmup=2,
        component="cache",
        operation="eviction",
    )

    # ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨åŸºå‡†æµ‹è¯•
    print("\nğŸ§  ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨æ€§èƒ½åŸºå‡†")
    print("-" * 60)
    runner.run_benchmark(
        "Lifecycle: Register",
        benchmark_lifecycle_register,
        iterations=iterations,
        warmup=10,
        component="lifecycle",
        operation="register",
    )

    runner.run_benchmark(
        "Lifecycle: Touch",
        benchmark_lifecycle_touch,
        iterations=iterations,
        warmup=10,
        component="lifecycle",
        operation="touch",
    )

    runner.run_benchmark(
        "Lifecycle: Eviction (LRU)",
        benchmark_lifecycle_evict_lru,
        iterations=10,
        warmup=2,
        component="lifecycle",
        operation="eviction_lru",
    )

    # é€’å½’é™åˆ¶å™¨åŸºå‡†æµ‹è¯•
    print("\nğŸ”„ é€’å½’é™åˆ¶å™¨æ€§èƒ½åŸºå‡†")
    print("-" * 60)
    runner.run_benchmark(
        "Limiter: Get Limit",
        benchmark_limiter_get_limit,
        iterations=iterations,
        warmup=10,
        component="limiter",
        operation="get_limit",
    )

    runner.run_benchmark(
        "Limiter: Record Performance",
        benchmark_limiter_record_performance,
        iterations=iterations,
        warmup=10,
        component="limiter",
        operation="record_performance",
    )

    # ç»¼åˆåŸºå‡†æµ‹è¯•
    print("\nğŸ”— ç»¼åˆæ€§èƒ½åŸºå‡†")
    print("-" * 60)
    runner.run_benchmark(
        "Full: Decision Flow",
        benchmark_full_decision_flow,
        iterations=iterations,
        warmup=10,
        component="full",
        operation="decision_flow",
    )


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="AGIç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•"
    )
    parser.add_argument(
        "--iterations",
        type=int,
        default=1000,
        help="è¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤1000",
    )
    parser.add_argument(
        "--benchmark",
        type=str,
        choices=["cache", "lifecycle", "limiter", "full", "all"],
        default="all",
        help="è¿è¡Œç‰¹å®šåŸºå‡†æµ‹è¯•",
    )
    parser.add_argument(
        "--report",
        type=str,
        help="ä¿å­˜æŠ¥å‘Šåˆ°æŒ‡å®šæ–‡ä»¶",
    )

    args = parser.parse_args()

    runner = BenchmarkRunner()

    if args.benchmark == "all":
        run_all_benchmarks(runner, args.iterations)
    elif args.benchmark == "cache":
        # åªè¿è¡Œç¼“å­˜åŸºå‡†
        pass  # ç®€åŒ–ç‰ˆæœ¬ï¼Œé»˜è®¤è¿è¡Œæ‰€æœ‰
    else:
        run_all_benchmarks(runner, args.iterations)

    # æ‰“å°æ€»ç»“
    runner.print_summary()

    # ä¿å­˜æŠ¥å‘Š
    if args.report:
        runner.save_report(args.report)
    else:
        # é»˜è®¤ä¿å­˜
        report_file = Path(__file__).parent / f"benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        runner.save_report(str(report_file))

    return 0


if __name__ == "__main__":
    sys.exit(main())
