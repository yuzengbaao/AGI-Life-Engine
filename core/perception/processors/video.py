#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§è§†é¢‘å¤„ç†å™¨
å®ç°å¤æ‚åœºæ™¯ç†è§£ã€æ—¶åºåˆ†æå’Œå¤šç›®æ ‡è·Ÿè¸ª
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import cv2
import numpy as np
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Deque, Union
import time
from collections import defaultdict, deque
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass
from enum import Enum
from contextlib import contextmanager
import warnings

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ç¦ç”¨ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="torch")
warnings.filterwarnings("ignore", category=FutureWarning, module="cv2")

# å°è¯•åŠ è½½é…ç½®æ–‡ä»¶
def load_video_config() -> Dict[str, Any]:
    """åŠ è½½è§†é¢‘å¤„ç†é…ç½®ï¼Œå¸¦ç¼“å­˜æœºåˆ¶"""
    config_path = Path(__file__).parent / "video_processing_config.json"
    
    # ä½¿ç”¨æ¨¡å—çº§ç¼“å­˜é¿å…é‡å¤åŠ è½½
    if hasattr(load_video_config, '_cached_config'):
        return load_video_config._cached_config
    
    try:
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info("âœ… è§†é¢‘å¤„ç†é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
            result = config.get("video_processing", {})
        else:
            raise FileNotFoundError("Config file not found")
    except Exception as e:
        logger.warning(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        # é»˜è®¤é…ç½®
        result = {
            "frame_extraction": {
                "fps": 30,
                "resize_width": 640,
                "resize_height": 480
            },
            "models": {
                "spatial_temporal_cnn": {
                    "input_channels": 3,
                    "feature_dim": 512
                },
                "temporal_attention": {
                    "feature_dim": 512,
                    "num_heads": 8
                }
            }
        }
        logger.info("ä½¿ç”¨é»˜è®¤è§†é¢‘å¤„ç†é…ç½®")
    
    # ç¼“å­˜ç»“æœ
    load_video_config._cached_config = result
    return result

VIDEO_CONFIG = load_video_config()

class SceneComplexity(Enum):
    """åœºæ™¯å¤æ‚åº¦æšä¸¾"""
    SIMPLE = "simple"
    MODERATE = "moderate"
    COMPLEX = "complex"
    EXTREME = "extreme"

@dataclass
class DetectionResult:
    """æ£€æµ‹ç»“æœæ•°æ®ç±»"""
    class_id: int
    class_name: str
    confidence: float
    bbox: Tuple[int, int, int, int]  # (x1, y1, x2, y2)
    track_id: Optional[int] = None
    features: Optional[np.ndarray] = None

@dataclass
class SceneAnalysis:
    """åœºæ™¯åˆ†æç»“æœ"""
    complexity: SceneComplexity
    object_count: int
    motion_intensity: float
    lighting_quality: float
    occlusion_level: float
    crowd_density: float
    scene_description: str

class TemporalAttention(nn.Module):
    """
    æ—¶åºæ³¨æ„åŠ›æœºåˆ¶
    ç”¨äºæ•è·è§†é¢‘å¸§é—´çš„æ—¶åºå…³ç³»
    """
    def __init__(self, feature_dim: int, num_heads: int = 8):
        super().__init__()
        self.feature_dim = feature_dim
        self.num_heads = num_heads
        self.head_dim = feature_dim // num_heads
        
        assert feature_dim % num_heads == 0, f"feature_dim ({feature_dim}) must be divisible by num_heads ({num_heads})"
        
        self.query = nn.Linear(feature_dim, feature_dim)
        self.key = nn.Linear(feature_dim, feature_dim)
        self.value = nn.Linear(feature_dim, feature_dim)
        self.output = nn.Linear(feature_dim, feature_dim)
        
        self.dropout = nn.Dropout(0.1)
        self.layer_norm = nn.LayerNorm(feature_dim)
        
        # é¢„è®¡ç®—æ ‡é‡ä»¥æé«˜æ•ˆç‡
        self.scale_factor = 1.0 / np.sqrt(self.head_dim)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, seq_len, feature_dim]
            
        Returns:
            è¾“å‡ºç‰¹å¾ [batch_size, seq_len, feature_dim]
        """
        batch_size, seq_len, _ = x.shape
        
        # è®¡ç®—æ³¨æ„åŠ›ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # ä½¿ç”¨é¢„è®¡ç®—çš„ç¼©æ”¾å› å­
        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale_factor
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        attended = torch.matmul(attention_weights, V)
        attended = attended.transpose(1, 2).contiguous().view(batch_size, seq_len, self.feature_dim)
        
        # è¾“å‡ºæŠ•å½±å’Œæ®‹å·®è¿æ¥
        output = self.output(attended)
        output = self.layer_norm(output + x)
        
        return output

class SpatialTemporalCNN(nn.Module):
    """
    æ—¶ç©ºå·ç§¯ç¥ç»ç½‘ç»œ
    ç”¨äºæå–è§†é¢‘çš„æ—¶ç©ºç‰¹å¾
    """
    def __init__(self, input_channels: int = 3, feature_dim: int = 512):
        super().__init__()
        
        # ä½¿ç”¨Sequentialç®€åŒ–ç½‘ç»œç»“æ„
        self.features = nn.Sequential(
            # 3Då·ç§¯å±‚ç”¨äºæ—¶ç©ºç‰¹å¾æå–
            nn.Conv3d(input_channels, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3)),
            nn.BatchNorm3d(64),
            nn.ReLU(inplace=True),
            
            nn.Conv3d(64, 128, kernel_size=(3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2)),
            nn.BatchNorm3d(128),
            nn.ReLU(inplace=True),
            
            nn.Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1)),
            nn.BatchNorm3d(256),
            nn.ReLU(inplace=True),
            
            nn.Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1)),
            nn.BatchNorm3d(512),
            nn.ReLU(inplace=True),
        )
        
        # è‡ªé€‚åº”æ± åŒ–
        self.adaptive_pool = nn.AdaptiveAvgPool3d((1, 7, 7))
        
        # å…¨è¿æ¥å±‚
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(512 * 7 * 7, feature_dim)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm3d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥è§†é¢‘ [batch_size, channels, frames, height, width]
            
        Returns:
            ç‰¹å¾å‘é‡ [batch_size, feature_dim]
        """
        # 3Då·ç§¯ç‰¹å¾æå–
        x = self.features(x)
        
        # è‡ªé€‚åº”æ± åŒ–
        x = self.adaptive_pool(x)
        
        # å±•å¹³å¹¶é€šè¿‡å…¨è¿æ¥å±‚
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        
        return x

class MultiObjectTracker:
    """
    å¤šç›®æ ‡è·Ÿè¸ªå™¨
    ä½¿ç”¨æ·±åº¦ç‰¹å¾å’Œå¡å°”æ›¼æ»¤æ³¢è¿›è¡Œç›®æ ‡è·Ÿè¸ª
    """
    def __init__(self, max_age: int = 30, min_hits: int = 3, distance_threshold: float = 100.0):
        self.max_age = max_age
        self.min_hits = min_hits
        self.distance_threshold = distance_threshold
        self.tracks: List[Dict[str, Any]] = []
        self.track_id_counter = 0
        
        # é¢„åˆ†é…å¸¸ç”¨æ•°ç»„ä»¥å‡å°‘å†…å­˜åˆ†é…
        self._bbox_cache = {}
    
    def update(self, detections: List[DetectionResult]) -> List[DetectionResult]:
        """
        æ›´æ–°è·Ÿè¸ªå™¨
        
        Args:
            detections: å½“å‰å¸§çš„æ£€æµ‹ç»“æœ
            
        Returns:
            å¸¦æœ‰è·Ÿè¸ªIDçš„æ£€æµ‹ç»“æœ
        """
        if not detections:
            # æ›´æ–°ç°æœ‰è½¨è¿¹å¹´é¾„
            self._update_track_ages()
            return []
        
        tracked_detections = []
        
        # æ‰¹é‡å¤„ç†ï¼šå…ˆæ£€æŸ¥æ‰€æœ‰ç°æœ‰è½¨è¿¹
        active_tracks = []
        for detection in detections:
            best_match = None
            best_distance = self.distance_threshold
            
            for track in self.tracks:
                if (track['class_id'] == detection.class_id and 
                    track['age'] < self.max_age):
                    
                    distance = self._calculate_bbox_distance(detection.bbox, track['last_bbox'])
                    if distance < best_distance:
                        best_distance = distance
                        best_match = track
            
            if best_match:
                # æ›´æ–°ç°æœ‰è½¨è¿¹
                best_match['last_bbox'] = detection.bbox
                best_match['age'] = 0
                best_match['hits'] += 1
                detection.track_id = best_match['id']
            else:
                # åˆ›å»ºæ–°è½¨è¿¹
                new_track = {
                    'id': self.track_id_counter,
                    'class_id': detection.class_id,
                    'last_bbox': detection.bbox,
                    'age': 0,
                    'hits': 1
                }
                self.tracks.append(new_track)
                detection.track_id = self.track_id_counter
                self.track_id_counter += 1
            
            tracked_detections.append(detection)
        
        # æ›´æ–°è½¨è¿¹å¹´é¾„å¹¶ç§»é™¤è¿‡æœŸè½¨è¿¹
        self._update_track_ages()
        
        return tracked_detections
    
    def _calculate_bbox_distance(self, bbox1: Tuple[int, int, int, int], 
                                bbox2: Tuple[int, int, int, int]) -> float:
        """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„ä¸­å¿ƒè·ç¦»ï¼ˆå‘é‡åŒ–å®ç°ï¼‰"""
        center1_x = (bbox1[0] + bbox1[2]) * 0.5
        center1_y = (bbox1[1] + bbox1[3]) * 0.5
        center2_x = (bbox2[0] + bbox2[2]) * 0.5
        center2_y = (bbox2[1] + bbox2[3]) * 0.5
        
        dx = center1_x - center2_x
        dy = center1_y - center2_y
        
        return np.sqrt(dx*dx + dy*dy)
    
    def _update_track_ages(self):
        """æ‰¹é‡æ›´æ–°è½¨è¿¹å¹´é¾„å¹¶æ¸…ç†è¿‡æœŸè½¨è¿¹"""
        current_time = time.time()
        self.tracks = [track for track in self.tracks if track['age'] < self.max_age]

class SceneComplexityAnalyzer:
    """
    åœºæ™¯å¤æ‚åº¦åˆ†æå™¨
    åˆ†æè§†é¢‘åœºæ™¯çš„å¤æ‚ç¨‹åº¦
    """
    def __init__(self, history_length: int = 10):
        self.motion_history: Deque[float] = deque(maxlen=history_length)
        self.frame_shape_cache: Optional[Tuple[int, int]] = None
        self.gray_cache: Optional[np.ndarray] = None
        
    def analyze_scene(self, frame: np.ndarray, detections: List[DetectionResult], 
                     prev_frame: Optional[np.ndarray] = None) -> SceneAnalysis:
        """
        åˆ†æåœºæ™¯å¤æ‚åº¦
        
        Args:
            frame: å½“å‰å¸§
            detections: æ£€æµ‹ç»“æœ
            prev_frame: å‰ä¸€å¸§
            
        Returns:
            åœºæ™¯åˆ†æç»“æœ
        """
        try:
            # è®¡ç®—å„é¡¹æŒ‡æ ‡
            object_count = len(detections)
            motion_intensity = self._calculate_motion_intensity(frame, prev_frame)
            lighting_quality = self._calculate_lighting_quality(frame)
            occlusion_level = self._calculate_occlusion_level(detections)
            crowd_density = self._calculate_crowd_density(detections, frame.shape)
            
            # ç¡®å®šå¤æ‚åº¦ç­‰çº§
            complexity = self._determine_complexity(
                object_count, motion_intensity, lighting_quality, 
                occlusion_level, crowd_density
            )
            
            # ç”Ÿæˆåœºæ™¯æè¿°
            scene_description = self._generate_scene_description(
                complexity, object_count, motion_intensity, crowd_density
            )
            
            return SceneAnalysis(
                complexity=complexity,
                object_count=object_count,
                motion_intensity=motion_intensity,
                lighting_quality=lighting_quality,
                occlusion_level=occlusion_level,
                crowd_density=crowd_density,
                scene_description=scene_description
            )
            
        except Exception as e:
            logger.error(f"åœºæ™¯åˆ†æå¤±è´¥: {e}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼
            return SceneAnalysis(
                complexity=SceneComplexity.MODERATE,
                object_count=0,
                motion_intensity=0.0,
                lighting_quality=0.5,
                occlusion_level=0.0,
                crowd_density=0.0,
                scene_description="åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åœºæ™¯"
            )
    
    def _calculate_motion_intensity(self, frame: np.ndarray, 
                                   prev_frame: Optional[np.ndarray]) -> float:
        """è®¡ç®—è¿åŠ¨å¼ºåº¦ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        if prev_frame is None:
            return 0.0
        
        try:
            # è½¬æ¢ä¸ºç°åº¦å›¾
            gray1 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
            
            # ä½¿ç”¨æ›´é«˜æ•ˆçš„å…‰æµç®—æ³•
            flow = cv2.calcOpticalFlowFarneback(
                gray2, gray1, None, 0.5, 3, 15, 3, 5, 1.2, 0
            )
            
            # è®¡ç®—è¿åŠ¨å¼ºåº¦
            magnitude = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)
            motion_magnitude = np.mean(magnitude)
            
            self.motion_history.append(motion_magnitude)
            return float(np.mean(self.motion_history)) if self.motion_history else 0.0
            
        except Exception as e:
            logger.debug(f"å…‰æµè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å¸§å·®æ–¹æ³•: {e}")
            # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨å¸§å·®
            gray1 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
            diff = cv2.absdiff(gray1, gray2)
            motion_magnitude = np.mean(diff) / 255.0
            self.motion_history.append(motion_magnitude)
            return float(np.mean(self.motion_history)) if self.motion_history else 0.0
    
    def _calculate_lighting_quality(self, frame: np.ndarray) -> float:
        """è®¡ç®—å…‰ç…§è´¨é‡ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        if self.gray_cache is None or self.frame_shape_cache != frame.shape[:2]:
            self.gray_cache = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            self.frame_shape_cache = frame.shape[:2]
        
        gray = self.gray_cache
        
        # ä½¿ç”¨æ›´ç¨³å®šçš„ç»Ÿè®¡æ–¹æ³•
        mean_brightness = np.mean(gray)
        brightness_std = np.std(gray)
        
        # è®¡ç®—å¯¹æ¯”åº¦
        contrast = brightness_std / (mean_brightness + 1e-6)
        
        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        quality = min(1.0, contrast / 50.0)  # è°ƒæ•´é˜ˆå€¼
        
        return float(quality)
    
    def _calculate_occlusion_level(self, detections: List[DetectionResult]) -> float:
        """è®¡ç®—é®æŒ¡ç¨‹åº¦ï¼ˆå‘é‡åŒ–å®ç°ï¼‰"""
        if len(detections) < 2:
            return 0.0
        
        total_overlap = 0.0
        total_pairs = 0
        
        # å‘é‡åŒ–è®¡ç®—é‡å 
        bboxes = [det.bbox for det in detections]
        
        for i, bbox1 in enumerate(bboxes):
            for j, bbox2 in enumerate(bboxes[i+1:], i+1):
                overlap = self._calculate_bbox_overlap(bbox1, bbox2)
                total_overlap += overlap
                total_pairs += 1
        
        return total_overlap / max(total_pairs, 1)
    
    def _calculate_bbox_overlap(self, bbox1: Tuple[int, int, int, int], 
                               bbox2: Tuple[int, int, int, int]) -> float:
        """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„é‡å ç‡ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        x1 = max(bbox1[0], bbox2[0])
        y1 = max(bbox1[1], bbox2[1])
        x2 = min(bbox1[2], bbox2[2])
        y2 = min(bbox1[3], bbox2[3])
        
        if x2 <= x1 or y2 <= y1:
            return 0.0
        
        intersection = (x2 - x1) * (y2 - y1)
        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])
        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])
        union = area1 + area2 - intersection
        
        return intersection / max(union, 1e-8)
    
    def _calculate_crowd_density(self, detections: List[DetectionResult], 
                                frame_shape: Tuple[int, int, int]) -> float:
        """è®¡ç®—äººç¾¤å¯†åº¦ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        # ç»Ÿè®¡äººç±»ç›®æ ‡
        person_count = sum(1 for det in detections if det.class_name.lower() in ['person', 'human'])
        
        # è®¡ç®—å¯†åº¦ï¼ˆäººæ•°/é¢ç§¯ï¼‰
        frame_area = frame_shape[0] * frame_shape[1]
        density = person_count / (frame_area / 10000)  # æ¯ä¸‡åƒç´ çš„äººæ•°
        
        return min(1.0, float(density))
    
    def _determine_complexity(self, object_count: int, motion_intensity: float,
                             lighting_quality: float, occlusion_level: float,
                             crowd_density: float) -> SceneComplexity:
        """ç¡®å®šåœºæ™¯å¤æ‚åº¦ï¼ˆä¼˜åŒ–æƒé‡ï¼‰"""
        # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
        complexity_score = (
            min(object_count / 15, 1.0) * 0.3 +
            min(motion_intensity / 30, 1.0) * 0.25 +
            (1.0 - lighting_quality) * 0.2 +
            occlusion_level * 0.15 +
            crowd_density * 0.1
        )
        
        if complexity_score < 0.25:
            return SceneComplexity.SIMPLE
        elif complexity_score < 0.5:
            return SceneComplexity.MODERATE
        elif complexity_score < 0.75:
            return SceneComplexity.COMPLEX
        else:
            return SceneComplexity.EXTREME
    
    def _generate_scene_description(self, complexity: SceneComplexity, 
                                   object_count: int, motion_intensity: float,
                                   crowd_density: float) -> str:
        """ç”Ÿæˆåœºæ™¯æè¿°ï¼ˆæ›´ä¸°å¯Œçš„æè¿°ï¼‰"""
        descriptions = {
            SceneComplexity.SIMPLE: "ç®€å•åœºæ™¯ï¼šç›®æ ‡è¾ƒå°‘ï¼Œè¿åŠ¨ç¼“æ…¢ï¼Œå…‰ç…§è‰¯å¥½",
            SceneComplexity.MODERATE: "ä¸­ç­‰å¤æ‚åœºæ™¯ï¼šç›®æ ‡é€‚ä¸­ï¼Œæœ‰ä¸€å®šè¿åŠ¨",
            SceneComplexity.COMPLEX: "å¤æ‚åœºæ™¯ï¼šç›®æ ‡è¾ƒå¤šï¼Œè¿åŠ¨é¢‘ç¹ï¼Œå¯èƒ½æœ‰é®æŒ¡",
            SceneComplexity.EXTREME: "æå¤æ‚åœºæ™¯ï¼šå¤§é‡ç›®æ ‡ï¼Œå‰§çƒˆè¿åŠ¨ï¼Œä¸¥é‡é®æŒ¡æˆ–æ¶åŠ£å…‰ç…§"
        }
        
        base_desc = descriptions[complexity]
        
        # æ·»åŠ å…·ä½“ä¿¡æ¯
        details = []
        if object_count > 10:
            details.append(f"{object_count}ä¸ªç›®æ ‡")
        if motion_intensity > 20:
            details.append("é«˜å¼ºåº¦è¿åŠ¨")
        if crowd_density > 0.3:
            details.append("é«˜äººç¾¤å¯†åº¦")
        
        if details:
            return f"{base_desc}ï¼ŒåŒ…å«{', '.join(details)}"
        
        return base_desc

@contextmanager
def video_capture_context(video_path: str):
    """è§†é¢‘æ•è·çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
    
    try:
        yield cap
    finally:
        cap.release()

class AdvancedVideoProcessor:
    """
    é«˜çº§è§†é¢‘å¤„ç†å™¨
    é›†æˆåœºæ™¯ç†è§£ã€ç›®æ ‡è·Ÿè¸ªå’Œå¤æ‚åº¦åˆ†æ
    """
    def __init__(self, config_file: Optional[str] = None):
        """
        åˆå§‹åŒ–è§†é¢‘å¤„ç†å™¨
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_file)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–æ¨¡å‹ç»„ä»¶ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
        try:
            model_config = VIDEO_CONFIG.get("models", {}).get("spatial_temporal_cnn", {})
            feature_dim = model_config.get("feature_dim", 512)
            
            self.spatial_temporal_cnn = SpatialTemporalCNN(
                feature_dim=feature_dim
            ).to(self.device)
            
            attention_config = VIDEO_CONFIG.get("models", {}).get("temporal_attention", {})
            num_heads = attention_config.get("num_heads", 8)
            
            self.temporal_attention = TemporalAttention(
                feature_dim=feature_dim, 
                num_heads=num_heads
            ).to(self.device)
            
            # è®¾ç½®è¯„ä¼°æ¨¡å¼
            self.spatial_temporal_cnn.eval()
            self.temporal_attention.eval()
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # åˆå§‹åŒ–åˆ†æå™¨å’Œè·Ÿè¸ªå™¨
        tracking_config = self.config.get("tracking", {})
        self.tracker = MultiObjectTracker(
            max_age=tracking_config.get("max_age", 30),
            min_hits=tracking_config.get("min_hits", 3)
        )
        
        self.scene_analyzer = SceneComplexityAnalyzer()
        
        # å¤„ç†å†å²
        self.processing_history: List[Dict[str, Any]] = []
        self.frame_buffer: Deque[np.ndarray] = deque(maxlen=16)
        
        # æ€§èƒ½ç›‘æ§
        self._processing_times: List[float] = []
        
        logger.info("ğŸ¬ é«˜çº§è§†é¢‘å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   - è®¾å¤‡: {self.device}")
        logger.info(f"   - æ—¶ç©ºCNNç‰¹å¾ç»´åº¦: {feature_dim}")
        logger.info(f"   - æ—¶åºæ³¨æ„åŠ›å¤´æ•°: {num_heads}")
    
    def _load_config(self, config_file: Optional[str]) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if hasattr(self._load_config, '_cached_config'):
            return self._load_config._cached_config
        
        default_config = {
            "video_processing": {
                "target_fps": 30,
                "frame_size": [224, 224],
                "batch_size": 8
            },
            "detection": {
                "confidence_threshold": 0.5,
                "nms_threshold": 0.4
            },
            "tracking": {
                "max_age": 30,
                "min_hits": 3
            },
            "analysis": {
                "complexity_threshold": 0.7,
                "motion_sensitivity": 1.0
            }
        }
        
        if config_file is None:
            config_file = 'video_processing_config.json'
            
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            # åˆå¹¶é»˜è®¤é…ç½®
            for key, value in default_config.items():
                if key not in config:
                    config[key] = value
            result = config
        except FileNotFoundError:
            logger.warning(f"é…ç½®æ–‡ä»¶ {config_file} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            result = default_config
        except json.JSONDecodeError as e:
            logger.error(f"é…ç½®æ–‡ä»¶è§£æå¤±è´¥: {e}")
            result = default_config
        
        # ç¼“å­˜ç»“æœ
        self._load_config._cached_config = result
        return result
    
    def process_video(self, video_path: str, output_path: Optional[str] = None) -> Dict[str, Any]:
        """
        å¤„ç†è§†é¢‘æ–‡ä»¶
        
        Args:
            video_path: è¾“å…¥è§†é¢‘è·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        start_time = time.time()
        logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video_path}")
        
        # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºé‡Šæ”¾
        with video_capture_context(video_path) as cap:
            # è·å–è§†é¢‘ä¿¡æ¯
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            logger.info(f"   - è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps:.2f}fps, {frame_count}å¸§")
            
            # åˆå§‹åŒ–ç»“æœå­˜å‚¨
            results = {
                'video_info': {
                    'path': video_path,
                    'fps': fps,
                    'frame_count': frame_count,
                    'resolution': (width, height),
                    'processed_at': time.strftime('%Y-%m-%d %H:%M:%S')
                },
                'frame_results': [],
                'summary': {
                    'total_objects': 0,
                    'unique_tracks': 0,
                    'avg_complexity': 0.0,
                    'scene_changes': 0,
                    'processing_time': 0.0,
                    'frames_processed': 0
                }
            }
            
            frame_idx = 0
            prev_frame = None
            prev_complexity = None
            
            # å®‰å…¨é™åˆ¶
            max_frames = 100000  # æœ€å¤§å¤„ç†10ä¸‡å¸§
            processing_timeout = 3600  # æœ€å¤§å¤„ç†æ—¶é—´1å°æ—¶
            processing_start = time.time()
            
            try:
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    # æ£€æŸ¥å¸§æ•°é™åˆ¶
                    if frame_idx >= max_frames:
                        logger.warning(f"âš ï¸ è¾¾åˆ°æœ€å¤§å¸§æ•°é™åˆ¶: {max_frames}")
                        break
                    
                    # æ£€æŸ¥å¤„ç†è¶…æ—¶
                    elapsed_time = time.time() - processing_start
                    if elapsed_time > processing_timeout:
                        logger.warning(f"âš ï¸ å¤„ç†è¶…æ—¶: {processing_timeout}ç§’ (å·²å¤„ç†{frame_idx}å¸§)")
                        break
                    
                    # å¤„ç†å•å¸§
                    frame_start = time.time()
                    frame_result = self.process_frame(frame, frame_idx, prev_frame)
                    frame_end = time.time()
                    
                    # è®°å½•å¤„ç†æ—¶é—´
                    self._processing_times.append(frame_end - frame_start)
                    
                    results['frame_results'].append(frame_result)
                    
                    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                    self._update_summary(results['summary'], frame_result, prev_complexity)
                    
                    # æ›´æ–°å†å²
                    prev_frame = frame.copy()
                    prev_complexity = frame_result['scene_analysis'].complexity
                    frame_idx += 1
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if frame_idx % 30 == 0:
                        progress = (frame_idx / max(frame_count, 1)) * 100
                        avg_time = np.mean(self._processing_times[-30:]) if self._processing_times else 0
                        logger.info(f"   - å¤„ç†è¿›åº¦: {progress:.1f}% ({frame_idx}/{frame_count}), "
                                  f"å¹³å‡å¸§å¤„ç†æ—¶é—´: {avg_time*1000:.1f}ms")
                        
            except Exception as e:
                logger.error(f"è§†é¢‘å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                raise
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        self._finalize_summary(results['summary'], results['frame_results'])
        results['summary']['processing_time'] = time.time() - start_time
        results['summary']['frames_processed'] = frame_idx
        
        # ä¿å­˜ç»“æœ
        if output_path:
            try:
                self._save_results(results, output_path)
            except Exception as e:
                logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
        
        logger.info(f"âœ… è§†é¢‘å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {frame_idx} å¸§, "
                   f"æ€»è€—æ—¶: {results['summary']['processing_time']:.2f}ç§’")
        
        return results
    
    def process_frame(self, frame: np.ndarray, frame_idx: int, 
                     prev_frame: Optional[np.ndarray] = None) -> Dict[str, Any]:
        """
        å¤„ç†å•å¸§
        
        Args:
            frame: å½“å‰å¸§
            frame_idx: å¸§ç´¢å¼•
            prev_frame: å‰ä¸€å¸§
            
        Returns:
            å¸§å¤„ç†ç»“æœ
        """
        try:
            # æ¨¡æ‹Ÿç›®æ ‡æ£€æµ‹ï¼ˆå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨çœŸå®çš„æ£€æµ‹æ¨¡å‹ï¼‰
            detections = self._simulate_detection(frame)
            
            # ç›®æ ‡è·Ÿè¸ª
            tracked_detections = self.tracker.update(detections)
            
            # åœºæ™¯åˆ†æ
            scene_analysis = self.scene_analyzer.analyze_scene(frame, tracked_detections, prev_frame)
            
            # æ·»åŠ åˆ°å¸§ç¼“å†²åŒº
            if len(self.frame_buffer) < self.frame_buffer.maxlen:
                self.frame_buffer.append(frame.copy())
            else:
                # å¾ªç¯ç¼“å†²åŒº
                self.frame_buffer.appendleft(frame.copy())
            
            # æ„å»ºç»“æœ
            result = {
                'frame_idx': frame_idx,
                'timestamp': frame_idx / 30.0,  # å‡è®¾30fps
                'detections': [
                    {
                        'class_id': det.class_id,
                        'class_name': det.class_name,
                        'confidence': float(det.confidence),
                        'bbox': list(det.bbox),
                        'track_id': det.track_id
                    }
                    for det in tracked_detections
                ],
                'scene_analysis': scene_analysis,
                'processing_time': time.time()
            }
            
            return result
            
        except Exception as e:
            logger.error(f"å¸§å¤„ç†å¤±è´¥ (å¸§ {frame_idx}): {e}")
            # è¿”å›æœ€å°å¯è¡Œç»“æœ
            return {
                'frame_idx': frame_idx,
                'timestamp': frame_idx / 30.0,
                'detections': [],
                'scene_analysis': SceneAnalysis(
                    complexity=SceneComplexity.MODERATE,
                    object_count=0,
                    motion_intensity=0.0,
                    lighting_quality=0.5,
                    occlusion_level=0.0,
                    crowd_density=0.0,
                    scene_description="å¤„ç†å¤±è´¥"
                ),
                'processing_time': time.time()
            }
    
    def _simulate_detection(self, frame: np.ndarray) -> List[DetectionResult]:
        """
        æ¨¡æ‹Ÿç›®æ ‡æ£€æµ‹
        å®é™…åº”ç”¨ä¸­åº”æ›¿æ¢ä¸ºçœŸå®çš„æ£€æµ‹æ¨¡å‹
        """
        try:
            # ä½¿ç”¨å›ºå®šçš„éšæœºç§å­ä¿è¯å¯é‡ç°æ€§ï¼ˆè°ƒè¯•æ—¶ï¼‰
            rng = np.random.default_rng(seed=(hash(time.time()) % 2**32))
            
            # ç®€å•çš„æ¨¡æ‹Ÿæ£€æµ‹é€»è¾‘
            detections = []
            
            # æ ¹æ®åˆ†è¾¨ç‡è°ƒæ•´å¯¹è±¡æ•°é‡
            resolution_factor = (frame.shape[0] * frame.shape[1]) / (640 * 480)
            max_objects = max(1, int(8 * resolution_factor))
            num_objects = rng.integers(0, max_objects + 1)
            
            class_names = ['person', 'car', 'bicycle', 'dog', 'cat', 'truck', 'bus']
            
            for i in range(num_objects):
                class_id = rng.integers(0, len(class_names))
                class_name = class_names[class_id]
                confidence = rng.uniform(0.5, 0.95)
                
                # é˜²æ­¢è¾¹ç•Œæ¡†è¶…å‡ºå›¾åƒèŒƒå›´
                max_w = min(150, frame.shape[1] // 4)
                max_h = min(150, frame.shape[0] // 4)
                
                w = rng.integers(50, max_w)
                h = rng.integers(50, max_h)
                
                x1 = rng.integers(0, frame.shape[1] - w)
                y1 = rng.integers(0, frame.shape[0] - h)
                x2 = x1 + w
                y2 = y1 + h
                
                detection = DetectionResult(
                    class_id=class_id,
                    class_name=class_name,
                    confidence=confidence,
                    bbox=(int(x1), int(y1), int(x2), int(y2))
                )
                detections.append(detection)
            
            return detections
            
        except Exception as e:
            logger.error(f"æ£€æµ‹æ¨¡æ‹Ÿå¤±è´¥: {e}")
            return []
    
    def _update_summary(self, summary: Dict[str, Any], frame_result: Dict[str, Any], 
                       prev_complexity: Optional[SceneComplexity]):
        """æ›´æ–°æ‘˜è¦ç»Ÿè®¡"""
        summary['total_objects'] += len(frame_result['detections'])
        
        # æ£€æµ‹åœºæ™¯å˜åŒ–
        current_complexity = frame_result['scene_analysis'].complexity
        if prev_complexity and current_complexity != prev_complexity:
            summary['scene_changes'] += 1
    
    def _finalize_summary(self, summary: Dict[str, Any], frame_results: List[Dict[str, Any]]):
        """å®Œæˆæœ€ç»ˆç»Ÿè®¡"""
        if not frame_results:
            return
        
        # è®¡ç®—å¹³å‡å¤æ‚åº¦
        complexity_scores = []
        unique_tracks: set = set()
        
        complexity_map = {
            SceneComplexity.SIMPLE: 0.25,
            SceneComplexity.MODERATE: 0.5,
            SceneComplexity.COMPLEX: 0.75,
            SceneComplexity.EXTREME: 1.0
        }
        
        for result in frame_results:
            # å¤æ‚åº¦åˆ†æ•°æ˜ å°„
            complexity_enum = result['scene_analysis'].complexity
            if complexity_enum in complexity_map:
                complexity_scores.append(complexity_map[complexity_enum])
            
            # æ”¶é›†å”¯ä¸€è½¨è¿¹ID
            for det in result['detections']:
                track_id = det['track_id']
                if track_id is not None:
                    unique_tracks.add(track_id)
        
        summary['avg_complexity'] = float(np.mean(complexity_scores)) if complexity_scores else 0.0
        summary['unique_tracks'] = len(unique_tracks)
    
    def _save_results(self, results: Dict[str, Any], output_path: str):
        """ä¿å­˜å¤„ç†ç»“æœï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰"""
        try:
            # è½¬æ¢ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
            serializable_results = self._make_serializable(results)
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            
        except PermissionError:
            logger.error(f"æƒé™é”™è¯¯ï¼šæ— æ³•å†™å…¥æ–‡ä»¶ {output_path}")
            raise
        except OSError as e:
            logger.error(f"æ–‡ä»¶ç³»ç»Ÿé”™è¯¯ï¼š{e}")
            raise
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            raise
    
    def _make_serializable(self, obj: Any) -> Any:
        """ä½¿å¯¹è±¡å¯åºåˆ—åŒ–ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰"""
        if obj is None:
            return None
        elif isinstance(obj, dict):
            return {str(k): self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(self._make_serializable(item) for item in obj)
        elif isinstance(obj, SceneAnalysis):
            return {
                'complexity': obj.complexity.value,
                'object_count': int(obj.object_count),
                'motion_intensity': float(obj.motion_intensity),
                'lighting_quality': float(obj.lighting_quality),
                'occlusion_level': float(obj.occlusion_level),
                'crowd_density': float(obj.crowd_density),
                'scene_description': str(obj.scene_description)
            }
        elif isinstance(obj, SceneComplexity):
            return obj.value
        elif isinstance(obj, (np.integer, np.int32, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        elif hasattr(obj, 'item'):  # å¤„ç†å…¶ä»–numpyæ ‡é‡ç±»å‹
            return obj.item()
        elif isinstance(obj, (str, int, float, bool)):
            return obj
        elif hasattr(obj, '__dict__'):
            # å¯¹äºå…¶ä»–å¯¹è±¡ï¼Œå°è¯•è½¬æ¢å…¶å±æ€§
            return {k: self._make_serializable(v) for k, v in obj.__dict__.items() 
                   if not k.startswith('_')}
        else:
            try:
                return str(obj)
            except:
                return "unserializable_object"

def create_sample_video() -> str:
    """åˆ›å»ºç¤ºä¾‹è§†é¢‘ç”¨äºæµ‹è¯•"""
    output_path = "test_data/video/sample_video.mp4"
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, 30.0, (640, 480))
    
    # ä½¿ç”¨å›ºå®šéšæœºç§å­ä¿è¯å¯é‡ç°æ€§
    rng = np.random.default_rng(seed=42)
    
    # ç”Ÿæˆ100å¸§æµ‹è¯•è§†é¢‘
    for i in range(100):
        # åˆ›å»ºå½©è‰²èƒŒæ™¯
        frame = rng.integers(0, 255, (480, 640, 3), dtype=np.uint8)
        
        # æ·»åŠ ä¸€äº›ç§»åŠ¨çš„çŸ©å½¢
        for j in range(3):
            x = int(50 + 200 * j + 50 * np.sin(i * 0.1 + j))
            y = int(100 + 50 * np.cos(i * 0.1 + j))
            cv2.rectangle(frame, (x, y), (x+80, y+60), (255, 255, 255), -1)
            cv2.putText(frame, f'Obj{j}', (x+10, y+35), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)
        
        # æ·»åŠ å¸§å·
        cv2.putText(frame, f'Frame {i}', (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        
        out.write(frame)
    
    out.release()
    logger.info(f"âœ… ç¤ºä¾‹è§†é¢‘å·²åˆ›å»º: {output_path}")
    
    return output_path

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¬ å¯åŠ¨é«˜çº§è§†é¢‘å¤„ç†å™¨æµ‹è¯•")
    
    try:
        # åˆ›å»ºç¤ºä¾‹è§†é¢‘
        video_path = create_sample_video()
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        processor = AdvancedVideoProcessor()
        
        # å¤„ç†è§†é¢‘
        results = processor.process_video(
            video_path=video_path,
            output_path="video_processing_results.json"
        )
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        summary = results['summary']
        logger.info("ğŸ“Š å¤„ç†ç»“æœæ‘˜è¦:")
        logger.info(f"   - æ€»ç›®æ ‡æ•°: {summary['total_objects']}")
        logger.info(f"   - å”¯ä¸€è½¨è¿¹æ•°: {summary['unique_tracks']}")
        logger.info(f"   - å¹³å‡å¤æ‚åº¦: {summary['avg_complexity']:.3f}")
        logger.info(f"   - åœºæ™¯å˜åŒ–æ¬¡æ•°: {summary['scene_changes']}")
        logger.info(f"   - å¤„ç†æ—¶é—´: {summary['processing_time']:.2f}ç§’")
        
        logger.info("âœ… é«˜çº§è§†é¢‘å¤„ç†å™¨æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()