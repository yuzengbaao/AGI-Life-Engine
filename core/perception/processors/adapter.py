#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ„ŸçŸ¥å¤„ç†å™¨é€‚é…å™¨ - Perception Processor Adapter
é€‚é…AdvancedVideoProcessorå’ŒAdvancedAudioProcessorç”¨äºå®æ—¶æµå¤„ç†

ä½œè€…: AGI System Team
æ—¥æœŸ: 2025-11-21
ç‰ˆæœ¬: 1.0.0
"""

import numpy as np
import logging
from typing import Dict, Any, Optional
from datetime import datetime
import tempfile
import os

from core.perception.processors.video import AdvancedVideoProcessor
from core.perception.processors.audio import AdvancedAudioProcessor, AudioData, AudioTaskType

logger = logging.getLogger(__name__)


class RealtimeVideoAdapter:
    """å®æ—¶è§†é¢‘å¤„ç†é€‚é…å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è§†é¢‘é€‚é…å™¨"""
        self.processor = AdvancedVideoProcessor()
        self.frame_history = []  # ä¿å­˜æœ€è¿‘çš„å¸§ç”¨äºåœºæ™¯åˆ†æ
        self.max_history = 30  # ä¿ç•™1ç§’å†å²(å‡è®¾30fps)
        
        logger.info("ğŸ“¹ å®æ—¶è§†é¢‘é€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def process_frame(self, frame_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å•å¸§
        
        Args:
            frame_data: åŒ…å«frame, timestamp, frame_numberçš„å­—å…¸
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        frame = frame_data['frame']
        timestamp = frame_data['timestamp']
        frame_number = frame_data['frame_number']
        
        try:
            # æ·»åŠ åˆ°å†å²
            self.frame_history.append(frame)
            if len(self.frame_history) > self.max_history:
                self.frame_history.pop(0)
            
            # è°ƒç”¨åº•å±‚å¤„ç†å™¨çš„å•å¸§å¤„ç†
            result = self.processor.process_frame(frame)
            
            # æ·»åŠ æ—¶é—´æˆ³ä¿¡æ¯
            result['timestamp'] = timestamp
            result['frame_number'] = frame_number
            
            return {
                'success': True,
                'frame_number': frame_number,
                'timestamp': timestamp,
                'detections': result.get('detections', []),
                'scene_analysis': result.get('scene_analysis', {}),
                'summary': self._generate_frame_summary(result)
            }
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¸§å¤±è´¥: {e}")
            return {
                'success': False,
                'frame_number': frame_number,
                'timestamp': timestamp,
                'error': str(e)
            }
    
    def _generate_frame_summary(self, result: Dict[str, Any]) -> str:
        """ç”Ÿæˆå¸§æ‘˜è¦
        
        Args:
            result: å¤„ç†ç»“æœ
            
        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        detections = result.get('detections', [])
        scene = result.get('scene_analysis', {})
        
        summary_parts = []
        
        # æ£€æµ‹å¯¹è±¡æ•°é‡
        if detections:
            obj_counts = {}
            for det in detections:
                label = det.get('label', 'unknown')
                obj_counts[label] = obj_counts.get(label, 0) + 1
            
            obj_str = ', '.join([f"{count}ä¸ª{label}" for label, count in obj_counts.items()])
            summary_parts.append(f"æ£€æµ‹åˆ°: {obj_str}")
        
        # åœºæ™¯å¤æ‚åº¦
        complexity = scene.get('complexity', 'unknown')
        if complexity != 'unknown':
            summary_parts.append(f"åœºæ™¯å¤æ‚åº¦: {complexity}")
        
        return '; '.join(summary_parts) if summary_parts else "æ— ç‰¹æ®Šäº‹ä»¶"
    
    def get_scene_summary(self) -> Dict[str, Any]:
        """è·å–åœºæ™¯æ‘˜è¦(åŸºäºå†å²å¸§)
        
        Returns:
            åœºæ™¯æ‘˜è¦å­—å…¸
        """
        if not self.frame_history:
            return {'status': 'no_data'}
        
        try:
            # ä½¿ç”¨æœ€æ–°çš„å¸§è¿›è¡Œåˆ†æ
            latest_frame = self.frame_history[-1]
            result = self.processor.process_frame(latest_frame)
            
            return {
                'status': 'success',
                'frame_count': len(self.frame_history),
                'scene_analysis': result.get('scene_analysis', {}),
                'timestamp': datetime.now()
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–åœºæ™¯æ‘˜è¦å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}


class RealtimeAudioAdapter:
    """å®æ—¶éŸ³é¢‘å¤„ç†é€‚é…å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–éŸ³é¢‘é€‚é…å™¨"""
        self.processor = AdvancedAudioProcessor()
        self.audio_history = []  # ä¿å­˜æœ€è¿‘çš„éŸ³é¢‘å—
        self.max_history = 10  # ä¿ç•™10å—(10ç§’)
        
        logger.info("ğŸµ å®æ—¶éŸ³é¢‘é€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def process_audio_chunk(self, audio_data_dict: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†éŸ³é¢‘å—
        
        Args:
            audio_data_dict: åŒ…å«audio, timestamp, chunk_number, sample_rate, channelsçš„å­—å…¸
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        audio_array = audio_data_dict['audio']
        timestamp = audio_data_dict['timestamp']
        chunk_number = audio_data_dict['chunk_number']
        sample_rate = audio_data_dict['sample_rate']
        channels = audio_data_dict['channels']
        
        try:
            # æ·»åŠ åˆ°å†å²
            self.audio_history.append(audio_data_dict)
            if len(self.audio_history) > self.max_history:
                self.audio_history.pop(0)
            
            # è½¬æ¢ä¸ºAudioDataæ ¼å¼
            # ç¡®ä¿æ˜¯å•å£°é“
            if len(audio_array.shape) > 1:
                audio_array = audio_array.mean(axis=1)
            
            audio_data = AudioData(
                waveform=audio_array.flatten(),
                sample_rate=sample_rate
            )
            
            # è°ƒç”¨åº•å±‚å¤„ç†å™¨
            result = self.processor.process_audio(
                audio_data, 
                AudioTaskType.SPEECH_RECOGNITION
            )
            
            return {
                'success': True,
                'chunk_number': chunk_number,
                'timestamp': timestamp,
                'transcription': result.get('transcription', ''),
                'emotion': result.get('emotion', 'neutral'),
                'features': result.get('features', {}),
                'summary': self._generate_audio_summary(result)
            }
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†éŸ³é¢‘å—å¤±è´¥: {e}")
            return {
                'success': False,
                'chunk_number': chunk_number,
                'timestamp': timestamp,
                'error': str(e)
            }
    
    def _generate_audio_summary(self, result: Dict[str, Any]) -> str:
        """ç”ŸæˆéŸ³é¢‘æ‘˜è¦
        
        Args:
            result: å¤„ç†ç»“æœ
            
        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        summary_parts = []
        
        # è½¬å½•æ–‡æœ¬
        transcription = result.get('transcription', '')
        if transcription:
            summary_parts.append(f"è¯­éŸ³: '{transcription}'")
        
        # æƒ…æ„Ÿ
        emotion = result.get('emotion', '')
        if emotion and emotion != 'neutral':
            summary_parts.append(f"æƒ…æ„Ÿ: {emotion}")
        
        # éŸ³é¢‘ç‰¹å¾
        features = result.get('features', {})
        if 'tempo' in features:
            summary_parts.append(f"èŠ‚å¥: {features['tempo']:.0f}BPM")
        
        return '; '.join(summary_parts) if summary_parts else "æ— è¯­éŸ³å†…å®¹"
    
    def get_audio_summary(self) -> Dict[str, Any]:
        """è·å–éŸ³é¢‘æ‘˜è¦(åŸºäºå†å²å—)
        
        Returns:
            éŸ³é¢‘æ‘˜è¦å­—å…¸
        """
        if not self.audio_history:
            return {'status': 'no_data'}
        
        try:
            # ç»Ÿè®¡æœ€è¿‘çš„è¯­éŸ³å’Œæƒ…æ„Ÿ
            transcriptions = []
            emotions = []
            
            for chunk in self.audio_history[-5:]:  # æœ€è¿‘5å—
                if 'transcription' in chunk:
                    transcriptions.append(chunk['transcription'])
                if 'emotion' in chunk:
                    emotions.append(chunk['emotion'])
            
            return {
                'status': 'success',
                'chunk_count': len(self.audio_history),
                'recent_transcriptions': transcriptions,
                'recent_emotions': emotions,
                'timestamp': datetime.now()
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–éŸ³é¢‘æ‘˜è¦å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 60)
    print("æ„ŸçŸ¥å¤„ç†å™¨é€‚é…å™¨æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•è§†é¢‘é€‚é…å™¨
    print("\nğŸ“¹ æµ‹è¯•è§†é¢‘é€‚é…å™¨...")
    video_adapter = RealtimeVideoAdapter()
    
    # åˆ›å»ºæµ‹è¯•å¸§
    test_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
    frame_data = {
        'frame': test_frame,
        'timestamp': datetime.now(),
        'frame_number': 1
    }
    
    print("å¤„ç†æµ‹è¯•å¸§...")
    result = video_adapter.process_frame(frame_data)
    print(f"ç»“æœ: {result}")
    
    # æµ‹è¯•éŸ³é¢‘é€‚é…å™¨
    print("\nğŸµ æµ‹è¯•éŸ³é¢‘é€‚é…å™¨...")
    audio_adapter = RealtimeAudioAdapter()
    
    # åˆ›å»ºæµ‹è¯•éŸ³é¢‘
    test_audio = np.random.randn(16000, 1).astype(np.float32)  # 1ç§’éŸ³é¢‘
    audio_data = {
        'audio': test_audio,
        'timestamp': datetime.now(),
        'chunk_number': 1,
        'sample_rate': 16000,
        'channels': 1
    }
    
    print("å¤„ç†æµ‹è¯•éŸ³é¢‘...")
    result = audio_adapter.process_audio_chunk(audio_data)
    print(f"ç»“æœ: {result}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆ")
