"""
Whisper ASRé›†æˆæ¨¡å—
æ”¯æŒä¸­æ–‡è¯­éŸ³è¯†åˆ«å’Œå®æ—¶æµå¼å¤„ç†
"""

import torch
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
from dataclasses import dataclass
from enum import Enum
import queue
import threading
import time

logger = logging.getLogger(__name__)


class WhisperModelSize(Enum):
    """Whisperæ¨¡å‹å¤§å°"""
    TINY = "tiny"           # 39M, æœ€å¿«
    BASE = "base"           # 74M, å¹³è¡¡
    SMALL = "small"         # 244M, è¾ƒå¥½
    MEDIUM = "medium"       # 769M, å¾ˆå¥½
    LARGE = "large"         # 1550M, æœ€å¥½
    LARGE_V2 = "large-v2"   # 1550M, æœ€æ–°
    LARGE_V3 = "large-v3"   # 1550M, æœ€æ–°å¢å¼º


class Language(Enum):
    """æ”¯æŒçš„è¯­è¨€"""
    AUTO = "auto"           # è‡ªåŠ¨æ£€æµ‹
    CHINESE = "zh"          # ä¸­æ–‡
    ENGLISH = "en"          # è‹±æ–‡
    JAPANESE = "ja"         # æ—¥æ–‡
    KOREAN = "ko"           # éŸ©æ–‡
    SPANISH = "es"          # è¥¿ç­ç‰™æ–‡
    FRENCH = "fr"           # æ³•æ–‡
    GERMAN = "de"           # å¾·æ–‡


@dataclass
class WhisperResult:
    """Whisperè¯†åˆ«ç»“æœ"""
    text: str                           # è¯†åˆ«æ–‡æœ¬
    language: str                       # æ£€æµ‹è¯­è¨€
    confidence: float                   # ç½®ä¿¡åº¦
    segments: List[Dict[str, Any]]      # åˆ†æ®µä¿¡æ¯
    processing_time: float              # å¤„ç†æ—¶é—´
    is_streaming: bool = False          # æ˜¯å¦æµå¼


class WhisperASR:
    """
    Whisper ASRå¼•æ“
    æ”¯æŒç¦»çº¿è¯­éŸ³è¯†åˆ«å’Œå®æ—¶æµå¼å¤„ç†
    """
    
    def __init__(
        self,
        model_size: WhisperModelSize = WhisperModelSize.BASE,
        device: str = "auto",
        language: Language = Language.AUTO,
        use_faster_whisper: bool = True
    ):
        """
        åˆå§‹åŒ–Whisper ASR
        
        Args:
            model_size: æ¨¡å‹å¤§å°
            device: è®¾å¤‡ (cuda/cpu/auto)
            language: é»˜è®¤è¯­è¨€
            use_faster_whisper: æ˜¯å¦ä½¿ç”¨faster-whisperä¼˜åŒ–ç‰ˆæœ¬
        """
        self.model_size = model_size
        self.language = language
        self.use_faster_whisper = use_faster_whisper
        
        # è®¾å¤‡é€‰æ‹©
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        self.model = None
        self.is_loaded = False
        
        logger.info(f"ğŸ¤ åˆå§‹åŒ–Whisper ASRå¼•æ“")
        logger.info(f"   - æ¨¡å‹: {model_size.value}")
        logger.info(f"   - è®¾å¤‡: {self.device}")
        logger.info(f"   - è¯­è¨€: {language.value}")
        logger.info(f"   - ä¼˜åŒ–ç‰ˆæœ¬: {use_faster_whisper}")
    
    def load_model(self) -> bool:
        """
        åŠ è½½Whisperæ¨¡å‹
        
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        if self.is_loaded:
            return True
            
        try:
            logger.info("â³ æ­£åœ¨åŠ è½½Whisperæ¨¡å‹...")
            
            # æŸ¥æ‰¾æœ¬åœ°æ¨¡å‹è·¯å¾„
            import os
            
            # 1. ä¼˜å…ˆæ£€æŸ¥ç¯å¢ƒå˜é‡
            env_local_path = os.environ.get("WHISPER_MODEL_LOCAL_DIR")
            if env_local_path and os.path.exists(env_local_path):
                local_model_path = env_local_path
            else:
                # 2. è‡ªåŠ¨æ¢æµ‹
                current_path = os.path.abspath(__file__)
                root_dir = os.path.dirname(current_path)
                for _ in range(3):
                    if os.path.exists(os.path.join(root_dir, "agi_chat_enhanced.py")):
                        break
                    root_dir = os.path.dirname(root_dir)
                
                # Check potential paths
                potential_paths = [
                    os.path.join(root_dir, "models", "faster-whisper-base"),
                    os.path.join(root_dir, "models", "whisper-base")
                ]
                
                local_model_path = os.path.join(root_dir, "models", "whisper-base") # default fallback
                for path in potential_paths:
                    if os.path.exists(path) and os.path.exists(os.path.join(path, "model.bin")):
                        local_model_path = path
                        break
            
            if self.use_faster_whisper:
                from faster_whisper import WhisperModel
                
                compute_type = "float16" if self.device == "cuda" else "int8"
                
                # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹
                # check if model.bin exists (required for CTranslate2 format used by faster-whisper)
                is_valid_ct2_model = os.path.exists(os.path.join(local_model_path, "model.bin"))
                
                if os.path.exists(local_model_path) and is_valid_ct2_model:
                    logger.info(f"âœ… æ£€æµ‹åˆ°æœ¬åœ°Whisperæ¨¡å‹(CTranslate2æ ¼å¼): {local_model_path}")
                    model_path_or_size = local_model_path
                else:
                    if os.path.exists(local_model_path) and not is_valid_ct2_model:
                        logger.warning(f"âš ï¸ æœ¬åœ°è·¯å¾„ {local_model_path} å­˜åœ¨ä½†ç¼ºå°‘ model.bin (éCTranslate2æ ¼å¼)ï¼Œå°†å¿½ç•¥å¹¶è‡ªåŠ¨ä¸‹è½½/åŠ è½½ {self.model_size.value}")
                    else:
                        logger.info(f"âš ï¸ æœ¬åœ°æ¨¡å‹æœªæ‰¾åˆ°ï¼Œå°†è‡ªåŠ¨ä¸‹è½½: {self.model_size.value}")
                    model_path_or_size = self.model_size.value
                
                self.model = WhisperModel(
                    model_path_or_size,
                    device=self.device,
                    compute_type=compute_type
                )
            else:
                import whisper
                
                # OpenAI Whisper åº“ç›®å‰ä¸»è¦æ”¯æŒä» hub åŠ è½½æˆ–æŒ‡å®š .pt æ–‡ä»¶
                # è¿™é‡Œç®€å•å¤„ç†ï¼Œå¦‚æœæœ¬åœ°æœ‰æ–‡ä»¶åˆ™ä½¿ç”¨ï¼Œå¦åˆ™ä¸‹è½½
                self.model = whisper.load_model(self.model_size.value, device=self.device)
            
            self.is_loaded = True
            logger.info("âœ… Whisperæ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def transcribe(
        self,
        audio: Union[str, np.ndarray],
        language: Optional[Language] = None,
        task: str = "transcribe",
        **kwargs
    ) -> WhisperResult:
        """
        è½¬å½•éŸ³é¢‘
        
        Args:
            audio: éŸ³é¢‘æ–‡ä»¶è·¯å¾„æˆ–numpyæ•°ç»„
            language: æŒ‡å®šè¯­è¨€(Noneåˆ™ä½¿ç”¨é»˜è®¤)
            task: ä»»åŠ¡ç±»å‹ (transcribeè½¬å½• / translateç¿»è¯‘)
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            è¯†åˆ«ç»“æœ
        """
        if not self.is_loaded:
            if not self.load_model():
                raise RuntimeError("Whisperæ¨¡å‹æœªåŠ è½½")
        
        start_time = time.time()
        
        # ç¡®å®šè¯­è¨€
        lang = language.value if language else self.language.value
        if lang == "auto":
            lang = None  # Noneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
        
        try:
            if self.use_faster_whisper:
                # faster-whisper API
                segments_gen, info = self.model.transcribe(
                    audio,
                    language=lang,
                    task=task,
                    **kwargs
                )
                
                # æ”¶é›†æ‰€æœ‰åˆ†æ®µ
                segments = []
                full_text = []
                
                for segment in segments_gen:
                    segments.append({
                        "start": segment.start,
                        "end": segment.end,
                        "text": segment.text,
                        "confidence": segment.avg_logprob
                    })
                    full_text.append(segment.text)
                
                result_text = " ".join(full_text).strip()
                detected_language = info.language
                confidence = info.language_probability
                
            else:
                # æ ‡å‡†whisper API
                result = self.model.transcribe(
                    audio,
                    language=lang,
                    task=task,
                    **kwargs
                )
                
                result_text = result["text"].strip()
                detected_language = result.get("language", lang or "unknown")
                
                # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
                segments = result.get("segments", [])
                if segments:
                    confidences = [s.get("avg_logprob", 0) for s in segments]
                    confidence = np.exp(np.mean(confidences))
                else:
                    confidence = 0.5
            
            processing_time = time.time() - start_time
            
            return WhisperResult(
                text=result_text,
                language=detected_language,
                confidence=float(confidence),
                segments=segments,
                processing_time=processing_time,
                is_streaming=False
            )
            
        except Exception as e:
            logger.error(f"âŒ Whisperè½¬å½•å¤±è´¥: {e}")
            raise
    
    def transcribe_chinese(
        self,
        audio: Union[str, np.ndarray],
        **kwargs
    ) -> WhisperResult:
        """
        ä¸­æ–‡è¯­éŸ³è¯†åˆ«(ä¾¿æ·æ–¹æ³•)
        
        Args:
            audio: éŸ³é¢‘æ–‡ä»¶è·¯å¾„æˆ–numpyæ•°ç»„
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            è¯†åˆ«ç»“æœ
        """
        return self.transcribe(
            audio,
            language=Language.CHINESE,
            **kwargs
        )
    
    def detect_language(
        self,
        audio: Union[str, np.ndarray],
        top_k: int = 5
    ) -> Dict[str, float]:
        """
        æ£€æµ‹éŸ³é¢‘è¯­è¨€
        
        Args:
            audio: éŸ³é¢‘æ–‡ä»¶è·¯å¾„æˆ–numpyæ•°ç»„
            top_k: è¿”å›å‰kä¸ªå¯èƒ½çš„è¯­è¨€
            
        Returns:
            è¯­è¨€æ¦‚ç‡å­—å…¸
        """
        if not self.is_loaded:
            if not self.load_model():
                raise RuntimeError("Whisperæ¨¡å‹æœªåŠ è½½")
        
        try:
            if self.use_faster_whisper:
                _, info = self.model.transcribe(audio, language=None)
                # faster-whisperåªè¿”å›æœ€å¯èƒ½çš„è¯­è¨€
                return {info.language: info.language_probability}
            else:
                # æ ‡å‡†whisperæ”¯æŒè¯­è¨€æ£€æµ‹
                import whisper
                
                # åŠ è½½éŸ³é¢‘
                if isinstance(audio, str):
                    audio_array = whisper.load_audio(audio)
                else:
                    audio_array = audio
                
                audio_array = whisper.pad_or_trim(audio_array)
                mel = whisper.log_mel_spectrogram(audio_array).to(self.device)
                
                # æ£€æµ‹è¯­è¨€
                _, probs = self.model.detect_language(mel)
                
                # è¿”å›top_k
                sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)
                return dict(sorted_probs[:top_k])
                
        except Exception as e:
            logger.error(f"âŒ è¯­è¨€æ£€æµ‹å¤±è´¥: {e}")
            return {}


class StreamingWhisperASR:
    """
    å®æ—¶æµå¼Whisper ASR
    æ”¯æŒéŸ³é¢‘å—çš„è¿ç»­è¯†åˆ«
    """
    
    def __init__(
        self,
        whisper_asr: WhisperASR,
        chunk_duration: float = 3.0,
        overlap_duration: float = 0.5,
        sample_rate: int = 16000
    ):
        """
        åˆå§‹åŒ–æµå¼ASR
        
        Args:
            whisper_asr: Whisper ASRå®ä¾‹
            chunk_duration: éŸ³é¢‘å—æ—¶é•¿(ç§’)
            overlap_duration: é‡å æ—¶é•¿(ç§’)
            sample_rate: é‡‡æ ·ç‡
        """
        self.asr = whisper_asr
        self.chunk_duration = chunk_duration
        self.overlap_duration = overlap_duration
        self.sample_rate = sample_rate
        
        self.chunk_samples = int(chunk_duration * sample_rate)
        self.overlap_samples = int(overlap_duration * sample_rate)
        
        # éŸ³é¢‘ç¼“å†²åŒº
        self.audio_buffer = np.array([], dtype=np.float32)
        self.buffer_lock = threading.Lock()
        
        # ç»“æœé˜Ÿåˆ—
        self.result_queue = queue.Queue()
        
        # å¤„ç†çº¿ç¨‹
        self.processing = False
        self.process_thread = None
        
        logger.info(f"ğŸ¤ åˆå§‹åŒ–æµå¼Whisper ASR")
        logger.info(f"   - å—æ—¶é•¿: {chunk_duration}s")
        logger.info(f"   - é‡å : {overlap_duration}s")
        logger.info(f"   - é‡‡æ ·ç‡: {sample_rate}Hz")
    
    def start(self):
        """å¯åŠ¨æµå¼å¤„ç†"""
        if self.processing:
            logger.warning("âš ï¸ æµå¼å¤„ç†å·²åœ¨è¿è¡Œ")
            return
        
        self.processing = True
        self.process_thread = threading.Thread(target=self._process_loop, daemon=True)
        self.process_thread.start()
        
        logger.info("âœ… æµå¼å¤„ç†å·²å¯åŠ¨")
    
    def stop(self):
        """åœæ­¢æµå¼å¤„ç†"""
        if not self.processing:
            return
        
        self.processing = False
        if self.process_thread:
            self.process_thread.join(timeout=2.0)
        
        logger.info("âœ… æµå¼å¤„ç†å·²åœæ­¢")
    
    def add_audio(self, audio_chunk: np.ndarray):
        """
        æ·»åŠ éŸ³é¢‘å—
        
        Args:
            audio_chunk: éŸ³é¢‘æ•°æ®(1D numpy array)
        """
        with self.buffer_lock:
            self.audio_buffer = np.append(self.audio_buffer, audio_chunk)
    
    def _process_loop(self):
        """å¤„ç†å¾ªç¯"""
        while self.processing:
            try:
                # æ£€æŸ¥ç¼“å†²åŒºæ˜¯å¦æœ‰è¶³å¤Ÿæ•°æ®
                with self.buffer_lock:
                    if len(self.audio_buffer) < self.chunk_samples:
                        time.sleep(0.1)
                        continue
                    
                    # æå–éŸ³é¢‘å—
                    audio_chunk = self.audio_buffer[:self.chunk_samples].copy()
                    
                    # ä¿ç•™é‡å éƒ¨åˆ†
                    self.audio_buffer = self.audio_buffer[
                        self.chunk_samples - self.overlap_samples:
                    ]
                
                # --- ç®€å• VAD (è¯­éŸ³æ´»åŠ¨æ£€æµ‹) ---
                # è®¡ç®—å‡æ–¹æ ¹ (RMS) èƒ½é‡
                rms = np.sqrt(np.mean(audio_chunk**2))
                # é˜ˆå€¼éœ€è¦æ ¹æ®éº¦å…‹é£è°ƒæ•´ï¼Œ0.01 æ˜¯ä¸€ä¸ªä¿å®ˆå€¼ (å‡è®¾ float32 èŒƒå›´ -1.0 åˆ° 1.0)
                # å¦‚æœå¤ªå®‰é™ï¼Œè·³è¿‡è¯†åˆ«ï¼ŒèŠ‚çœ CPU
                if rms < 0.005: 
                    # logger.debug(f"ğŸ¤« Silence detected (RMS: {rms:.4f}), skipping transcription.")
                    continue

                # è¯†åˆ«éŸ³é¢‘å—
                start_time = time.time()
                
                result = self.asr.transcribe(audio_chunk)
                
                # å¦‚æœç»“æœä¸ºç©ºæˆ–ç½®ä¿¡åº¦å¤ªä½ï¼Œä¹Ÿå¿½ç•¥ (Double Check)
                if not result.text.strip():
                    continue
                    
                result.is_streaming = True
                result.processing_time = time.time() - start_time
                
                # æ”¾å…¥ç»“æœé˜Ÿåˆ—
                self.result_queue.put(result)
                
            except Exception as e:
                logger.error(f"âŒ æµå¼å¤„ç†é”™è¯¯: {e}")
                time.sleep(0.1)
    
    def get_result(self, timeout: float = 0.1) -> Optional[WhisperResult]:
        """
        è·å–è¯†åˆ«ç»“æœ
        
        Args:
            timeout: è¶…æ—¶æ—¶é—´(ç§’)
            
        Returns:
            è¯†åˆ«ç»“æœæˆ–None
        """
        try:
            return self.result_queue.get(timeout=timeout)
        except queue.Empty:
            return None
    
    def clear_buffer(self):
        """æ¸…ç©ºéŸ³é¢‘ç¼“å†²åŒº"""
        with self.buffer_lock:
            self.audio_buffer = np.array([], dtype=np.float32)
        
        # æ¸…ç©ºç»“æœé˜Ÿåˆ—
        while not self.result_queue.empty():
            try:
                self.result_queue.get_nowait()
            except queue.Empty:
                break
        
        logger.info("âœ… ç¼“å†²åŒºå·²æ¸…ç©º")


# ä¾¿æ·å‡½æ•°
def quick_transcribe(
    audio: Union[str, np.ndarray],
    language: str = "auto",
    model_size: str = "base"
) -> str:
    """
    å¿«é€Ÿè½¬å½•(ç®€åŒ–æ¥å£)
    
    Args:
        audio: éŸ³é¢‘æ–‡ä»¶è·¯å¾„æˆ–numpyæ•°ç»„
        language: è¯­è¨€ä»£ç  (auto/zh/en/jaç­‰)
        model_size: æ¨¡å‹å¤§å° (tiny/base/small/medium/large)
        
    Returns:
        è¯†åˆ«æ–‡æœ¬
    """
    # åˆ›å»ºASRå®ä¾‹
    asr = WhisperASR(
        model_size=WhisperModelSize(model_size),
        language=Language(language)
    )
    
    # åŠ è½½æ¨¡å‹
    if not asr.load_model():
        raise RuntimeError("æ¨¡å‹åŠ è½½å¤±è´¥")
    
    # è½¬å½•
    result = asr.transcribe(audio)
    
    return result.text


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO)
    
    print("\n" + "="*70)
    print("ğŸ¤ Whisper ASRæµ‹è¯•")
    print("="*70)
    
    try:
        # æµ‹è¯•1: åŠ è½½æ¨¡å‹
        print("\n1ï¸âƒ£ æµ‹è¯•æ¨¡å‹åŠ è½½...")
        asr = WhisperASR(
            model_size=WhisperModelSize.BASE,
            language=Language.CHINESE
        )
        
        if asr.load_model():
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
            exit(1)
        
        # æµ‹è¯•2: ç”Ÿæˆæµ‹è¯•éŸ³é¢‘
        print("\n2ï¸âƒ£ ç”Ÿæˆæµ‹è¯•éŸ³é¢‘...")
        test_audio = np.random.randn(16000 * 3).astype(np.float32)  # 3ç§’
        print("âœ… æµ‹è¯•éŸ³é¢‘ç”Ÿæˆå®Œæˆ")
        
        # æµ‹è¯•3: è½¬å½•
        print("\n3ï¸âƒ£ æµ‹è¯•è½¬å½•...")
        result = asr.transcribe(test_audio)
        print(f"âœ… è½¬å½•å®Œæˆ")
        print(f"   - æ–‡æœ¬: {result.text}")
        print(f"   - è¯­è¨€: {result.language}")
        print(f"   - ç½®ä¿¡åº¦: {result.confidence:.2f}")
        print(f"   - å¤„ç†æ—¶é—´: {result.processing_time:.4f}s")
        
        # æµ‹è¯•4: æµå¼ASR
        print("\n4ï¸âƒ£ æµ‹è¯•æµå¼ASR...")
        streaming_asr = StreamingWhisperASR(asr)
        streaming_asr.start()
        
        # æ¨¡æ‹Ÿæ·»åŠ éŸ³é¢‘å—
        for i in range(3):
            chunk = np.random.randn(16000).astype(np.float32)  # 1ç§’
            streaming_asr.add_audio(chunk)
            time.sleep(0.5)
        
        # è·å–ç»“æœ
        time.sleep(2)
        result = streaming_asr.get_result(timeout=1.0)
        if result:
            print(f"âœ… æµå¼è¯†åˆ«å®Œæˆ")
            print(f"   - æ–‡æœ¬: {result.text}")
        
        streaming_asr.stop()
        
        print("\n" + "="*70)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("="*70)
        
    except ImportError as e:
        print(f"\nâŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("\nå®‰è£…å‘½ä»¤:")
        print("  pip install openai-whisper")
        print("  pip install faster-whisper  # å¯é€‰,ä½†æ¨è")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
