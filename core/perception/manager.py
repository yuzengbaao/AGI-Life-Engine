#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®æ—¶æ„ŸçŸ¥ç³»ç»Ÿ - Real-Time Perception System
ä¸ºAGIç³»ç»Ÿæä¾›å®æ—¶æ‘„åƒå¤´å’Œéº¦å…‹é£è¾“å…¥

ä½œè€…: AGI System Team
æ—¥æœŸ: 2025-11-21
ç‰ˆæœ¬: 1.0.0
"""

import cv2
import numpy as np
import sounddevice as sd
import threading
import queue
import time
import logging
try:
    import webrtcvad
except ImportError:
    import pip
    pip.main(['install', 'webrtcvad-wheels'])
    import webrtcvad

from typing import Optional, Dict, Any, Callable, List
from dataclasses import dataclass
from enum import Enum
from datetime import datetime

logger = logging.getLogger(__name__)


class CaptureStatus(Enum):
    """æ•è·çŠ¶æ€æšä¸¾"""
    STOPPED = "stopped"
    RUNNING = "running"
    PAUSED = "paused"
    ERROR = "error"


@dataclass
class PerceptionConfig:
    """æ„ŸçŸ¥é…ç½®"""
    # æ‘„åƒå¤´é…ç½®
    camera_device_id: int = 0
    camera_width: int = 1280
    camera_height: int = 720
    camera_fps: int = 30
    frame_buffer_size: int = 60  # ä¿ç•™2ç§’çš„å¸§
    
    # éº¦å…‹é£é…ç½®
    mic_device_id: Optional[int] = None  # Noneè¡¨ç¤ºä½¿ç”¨é»˜è®¤è®¾å¤‡
    sample_rate: int = 16000
    channels: int = 1
    chunk_duration: float = 0.03  # WebRTC VAD requires 10, 20, or 30ms frames (0.03 = 30ms)
    audio_buffer_size: int = 1000  # Increased buffer size for smaller chunks
    
    # VADé…ç½®
    vad_aggressiveness: int = 3  # 0-3, 3 is most aggressive in filtering non-speech
    speech_padding_ms: int = 300  # Silence padding around speech
    min_speech_duration_ms: int = 200  # Minimum duration to be considered speech
    
    # å¤„ç†é…ç½®
    process_interval: int = 5  # æ¯Nå¸§å¤„ç†ä¸€æ¬¡(èŠ‚çœCPU)
    motion_threshold: float = 0.1  # è¿åŠ¨æ£€æµ‹é˜ˆå€¼
    enable_motion_detection: bool = True


class CameraCapture:
    """æ‘„åƒå¤´æ•è·ç±»"""
    
    def __init__(self, config: PerceptionConfig):
        """åˆå§‹åŒ–æ‘„åƒå¤´æ•è·
        
        Args:
            config: æ„ŸçŸ¥é…ç½®å¯¹è±¡
        """
        self.config = config
        self.cap: Optional[cv2.VideoCapture] = None
        self.status = CaptureStatus.STOPPED
        self.frame_buffer = queue.Queue(maxsize=config.frame_buffer_size)
        self.capture_thread: Optional[threading.Thread] = None
        self.stop_event = threading.Event()
        self.frame_count = 0
        self.last_frame: Optional[np.ndarray] = None
        
        logger.info(f"ğŸ“· æ‘„åƒå¤´æ•è·åˆå§‹åŒ–: device={config.camera_device_id}, "
                   f"resolution={config.camera_width}x{config.camera_height}, "
                   f"fps={config.camera_fps}")
    
    @staticmethod
    def list_devices() -> List[Dict[str, Any]]:
        """åˆ—å‡ºå¯ç”¨çš„æ‘„åƒå¤´è®¾å¤‡
        
        Returns:
            è®¾å¤‡åˆ—è¡¨,æ¯ä¸ªè®¾å¤‡åŒ…å«idå’Œname
        """
        devices = []
        for i in range(10):  # æ£€æŸ¥å‰10ä¸ªè®¾å¤‡
            # ä½¿ç”¨DSHOWåç«¯æ£€æµ‹è®¾å¤‡(Windows)
            cap = cv2.VideoCapture(i, cv2.CAP_DSHOW)
            if cap.isOpened():
                devices.append({
                    'id': i,
                    'name': f'Camera {i}',
                    'backend': cap.getBackendName()
                })
                cap.release()
        return devices
    
    def start(self) -> bool:
        """å¯åŠ¨æ‘„åƒå¤´æ•è·
        
        Returns:
            å¯åŠ¨æˆåŠŸè¿”å›True,å¦åˆ™False
        """
        if self.status == CaptureStatus.RUNNING:
            logger.warning("âš ï¸ æ‘„åƒå¤´å·²åœ¨è¿è¡Œä¸­")
            return True
        
        try:
            # æ‰“å¼€æ‘„åƒå¤´ - ä½¿ç”¨DirectShowåç«¯(Windows)ä»¥ç¡®ä¿ç¨³å®šè®¿é—®
            # ä¼˜å…ˆå°è¯•DSHOWåç«¯ï¼Œå¤±è´¥åˆ™å›é€€åˆ°é»˜è®¤åç«¯
            self.cap = cv2.VideoCapture(self.config.camera_device_id, cv2.CAP_DSHOW)
            
            if not self.cap.isOpened():
                logger.warning(f"âš ï¸ DSHOWåç«¯å¤±è´¥ï¼Œå°è¯•é»˜è®¤åç«¯...")
                self.cap = cv2.VideoCapture(self.config.camera_device_id)
            
            if not self.cap.isOpened():
                logger.error(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´è®¾å¤‡ {self.config.camera_device_id}")
                self.status = CaptureStatus.ERROR
                return False
            
            # è®¾ç½®åˆ†è¾¨ç‡
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.config.camera_width)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.config.camera_height)
            self.cap.set(cv2.CAP_PROP_FPS, self.config.camera_fps)
            
            # è·å–å®é™…è®¾ç½®çš„å‚æ•°
            actual_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            actual_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            actual_fps = int(self.cap.get(cv2.CAP_PROP_FPS))
            
            logger.info(f"âœ… æ‘„åƒå¤´å·²å¯åŠ¨: {actual_width}x{actual_height} @ {actual_fps}fps")
            
            # å¯åŠ¨æ•è·çº¿ç¨‹
            self.stop_event.clear()
            self.capture_thread = threading.Thread(target=self._capture_loop, daemon=True)
            self.capture_thread.start()
            
            self.status = CaptureStatus.RUNNING
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ‘„åƒå¤´å¯åŠ¨å¤±è´¥: {e}")
            self.status = CaptureStatus.ERROR
            return False
    
    def stop(self):
        """åœæ­¢æ‘„åƒå¤´æ•è·"""
        if self.status == CaptureStatus.STOPPED:
            return
        
        logger.info("ğŸ›‘ åœæ­¢æ‘„åƒå¤´æ•è·...")
        self.stop_event.set()
        
        if self.capture_thread:
            self.capture_thread.join(timeout=2.0)
        
        if self.cap:
            self.cap.release()
            self.cap = None
        
        self.status = CaptureStatus.STOPPED
        logger.info("âœ… æ‘„åƒå¤´å·²åœæ­¢")
    
    def _capture_loop(self):
        """æ•è·å¾ªç¯(åœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œ)"""
        logger.info("ğŸ¬ æ‘„åƒå¤´æ•è·å¾ªç¯å·²å¯åŠ¨")
        
        while not self.stop_event.is_set():
            try:
                ret, frame = self.cap.read()
                
                if not ret:
                    logger.warning("âš ï¸ è¯»å–å¸§å¤±è´¥")
                    time.sleep(0.1)
                    continue
                
                self.frame_count += 1
                self.last_frame = frame.copy()
                
                # å°†å¸§åŠ å…¥ç¼“å†²åŒº
                if self.frame_buffer.full():
                    try:
                        self.frame_buffer.get_nowait()  # ä¸¢å¼ƒæœ€æ—§çš„å¸§
                    except queue.Empty:
                        pass
                
                self.frame_buffer.put({
                    'frame': frame,
                    'timestamp': datetime.now(),
                    'frame_number': self.frame_count
                })
                
                # æ§åˆ¶å¸§ç‡
                time.sleep(1.0 / self.config.camera_fps)
                
            except Exception as e:
                logger.error(f"âŒ æ•è·å¸§æ—¶å‡ºé”™: {e}")
                time.sleep(0.1)
        
        logger.info("ğŸ¬ æ‘„åƒå¤´æ•è·å¾ªç¯å·²ç»“æŸ")
    
    def get_frame(self, timeout: float = 1.0) -> Optional[Dict[str, Any]]:
        """è·å–ä¸€å¸§
        
        Args:
            timeout: è¶…æ—¶æ—¶é—´(ç§’)
            
        Returns:
            åŒ…å«frame, timestamp, frame_numberçš„å­—å…¸,è¶…æ—¶è¿”å›None
        """
        try:
            return self.frame_buffer.get(timeout=timeout)
        except queue.Empty:
            return None
    
    def get_latest_frame(self) -> Optional[np.ndarray]:
        """è·å–æœ€æ–°çš„å¸§(ä¸ä»ç¼“å†²åŒºç§»é™¤)
        
        Returns:
            æœ€æ–°çš„å¸§,å¦‚æœæ²¡æœ‰åˆ™è¿”å›None
        """
        return self.last_frame.copy() if self.last_frame is not None else None
    
    def detect_motion(self, threshold: float = None) -> bool:
        """æ£€æµ‹æ˜¯å¦æœ‰è¿åŠ¨
        
        Args:
            threshold: è¿åŠ¨æ£€æµ‹é˜ˆå€¼,Noneä½¿ç”¨é…ç½®å€¼
            
        Returns:
            æ£€æµ‹åˆ°è¿åŠ¨è¿”å›True
        """
        if threshold is None:
            threshold = self.config.motion_threshold
        
        # ç®€å•çš„å¸§å·®æ³•æ£€æµ‹è¿åŠ¨
        # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•
        if self.last_frame is None or self.frame_buffer.qsize() < 2:
            return False
        
        try:
            # è·å–å‰ä¸€å¸§
            prev_frame_data = self.frame_buffer.queue[-2]
            prev_frame = prev_frame_data['frame']
            
            # è®¡ç®—å¸§å·®
            gray1 = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(self.last_frame, cv2.COLOR_BGR2GRAY)
            diff = cv2.absdiff(gray1, gray2)
            
            # è®¡ç®—å·®å¼‚æ¯”ä¾‹
            motion_ratio = np.sum(diff > 30) / diff.size
            
            return motion_ratio > threshold
            
        except Exception as e:
            logger.error(f"âŒ è¿åŠ¨æ£€æµ‹å¤±è´¥: {e}")
            return False
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–æ•è·çŠ¶æ€
        
        Returns:
            çŠ¶æ€å­—å…¸
        """
        return {
            'status': self.status.value,
            'frame_count': self.frame_count,
            'buffer_size': self.frame_buffer.qsize(),
            'has_frame': self.last_frame is not None,
            'config': {
                'device_id': self.config.camera_device_id,
                'resolution': f"{self.config.camera_width}x{self.config.camera_height}",
                'fps': self.config.camera_fps
            }
        }


class MicrophoneCapture:
    """éº¦å…‹é£æ•è·ç±»"""
    
    def __init__(self, config: PerceptionConfig):
        """åˆå§‹åŒ–éº¦å…‹é£æ•è·
        
        Args:
            config: æ„ŸçŸ¥é…ç½®å¯¹è±¡
        """
        self.config = config
        self.status = CaptureStatus.STOPPED
        self.audio_buffer = queue.Queue(maxsize=config.audio_buffer_size)
        self.stream: Optional[sd.InputStream] = None
        self.chunk_size = int(config.sample_rate * config.chunk_duration)
        self.current_chunk = []
        self.chunk_count = 0
        
        # VAD Initialization
        self.vad = webrtcvad.Vad(config.vad_aggressiveness)
        self.is_speech_active = False
        self.silence_counter = 0
        self.speech_frames = []
        
        # Calculate frame counts for timing
        # chunk_duration is in seconds (e.g. 0.03), so *1000 gives ms
        frame_ms = config.chunk_duration * 1000
        self.min_speech_frames = int(config.min_speech_duration_ms / frame_ms)
        self.padding_frames = int(config.speech_padding_ms / frame_ms)
        
        logger.info(f"ğŸ¤ éº¦å…‹é£æ•è·åˆå§‹åŒ–: sample_rate={config.sample_rate}Hz, "
                   f"channels={config.channels}, chunk={config.chunk_duration}s")
    
    @staticmethod
    def list_devices() -> List[Dict[str, Any]]:
        """åˆ—å‡ºå¯ç”¨çš„éŸ³é¢‘è®¾å¤‡
        
        Returns:
            è®¾å¤‡åˆ—è¡¨
        """
        devices = []
        try:
            device_list = sd.query_devices()
            for i, device in enumerate(device_list):
                if device['max_input_channels'] > 0:
                    devices.append({
                        'id': i,
                        'name': device['name'],
                        'channels': device['max_input_channels'],
                        'sample_rate': device['default_samplerate']
                    })
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢éŸ³é¢‘è®¾å¤‡å¤±è´¥: {e}")
        
        return devices
    
    def start(self) -> bool:
        """å¯åŠ¨éº¦å…‹é£æ•è·
        
        Returns:
            å¯åŠ¨æˆåŠŸè¿”å›True,å¦åˆ™False
        """
        if self.status == CaptureStatus.RUNNING:
            logger.warning("âš ï¸ éº¦å…‹é£å·²åœ¨è¿è¡Œä¸­")
            return True
        
        try:
            # åˆ›å»ºéŸ³é¢‘æµ
            # å¢å¤§blocksizeå‡å°‘å›è°ƒé¢‘ç‡ï¼Œé¿å…input overflow
            # blocksize=8192ç›¸å½“äº512ms@16kHzï¼Œç»™ç³»ç»Ÿæ›´å¤šå¤„ç†æ—¶é—´
            self.stream = sd.InputStream(
                device=self.config.mic_device_id,
                channels=self.config.channels,
                samplerate=self.config.sample_rate,
                callback=self._audio_callback,
                blocksize=8192,  # ä»4096å¢åŠ åˆ°8192ï¼Œè¿›ä¸€æ­¥å‡å°‘overflowé£é™©
                latency='high'   # ä½¿ç”¨é«˜å»¶è¿Ÿæ¨¡å¼ï¼Œä¼˜å…ˆç¨³å®šæ€§
            )
            
            self.stream.start()
            self.status = CaptureStatus.RUNNING
            
            logger.info(f"âœ… éº¦å…‹é£å·²å¯åŠ¨: {self.config.sample_rate}Hz, "
                       f"{self.config.channels}å£°é“, blocksize=8192")
            return True
            
        except Exception as e:
            logger.error(f"âŒ éº¦å…‹é£å¯åŠ¨å¤±è´¥: {e}")
            self.status = CaptureStatus.ERROR
            return False
    
    def stop(self):
        """åœæ­¢éº¦å…‹é£æ•è·"""
        if self.status == CaptureStatus.STOPPED:
            return
        
        logger.info("ğŸ›‘ åœæ­¢éº¦å…‹é£æ•è·...")
        
        if self.stream:
            self.stream.stop()
            self.stream.close()
            self.stream = None
        
        self.status = CaptureStatus.STOPPED
        logger.info("âœ… éº¦å…‹é£å·²åœæ­¢")
    
    def _audio_callback(self, indata, frames, time_info, status):
        """éŸ³é¢‘å›è°ƒå‡½æ•°(ç”±sounddeviceè°ƒç”¨)
        
        Args:
            indata: è¾“å…¥éŸ³é¢‘æ•°æ®
            frames: å¸§æ•°
            time_info: æ—¶é—´ä¿¡æ¯
            status: çŠ¶æ€æ ‡å¿—ä½
        """
        if status:
            # logger.warning(f"âš ï¸ éŸ³é¢‘çŠ¶æ€: {status}")
            pass
            
        try:
            # 1. Convert to 16-bit PCM (required by WebRTC VAD)
            # indata is float32 [-1.0, 1.0], convert to int16 [-32768, 32767]
            audio_int16 = (indata * 32768).astype(np.int16)
            
            # 2. Check for speech using VAD
            is_speech = False
            try:
                # WebRTC VAD only supports 16000Hz (and 8k, 32k, 48k) mono 16-bit PCM
                # Ensure input is mono
                if self.config.channels > 1:
                    audio_mono = audio_int16.mean(axis=1).astype(np.int16)
                    raw_bytes = audio_mono.tobytes()
                else:
                    raw_bytes = audio_int16.tobytes()
                    
                is_speech = self.vad.is_speech(raw_bytes, self.config.sample_rate)
            except Exception as e:
                # Fallback if VAD fails
                pass
            
            # 3. Speech Logic with Padding
            if is_speech:
                self.is_speech_active = True
                self.silence_counter = 0
                self.speech_frames.append(indata.copy())
            elif self.is_speech_active:
                # Currently in speech mode, but silence detected
                self.silence_counter += 1
                self.speech_frames.append(indata.copy())
                
                # If silence exceeds padding, stop speech segment
                if self.silence_counter > self.padding_frames:
                    self.is_speech_active = False
                    
                    # Only process if duration is long enough
                    if len(self.speech_frames) >= self.min_speech_frames:
                        # Concatenate all speech frames
                        full_speech = np.concatenate(self.speech_frames, axis=0)
                        
                        # Add to buffer
                        if self.audio_buffer.full():
                            try:
                                self.audio_buffer.get_nowait()
                            except queue.Empty:
                                pass
                        
                        self.audio_buffer.put({
                            'audio': full_speech,
                            'timestamp': datetime.now(),
                            'chunk_number': self.chunk_count,
                            'sample_rate': self.config.sample_rate,
                            'channels': self.config.channels,
                            'is_speech': True
                        })
                        self.chunk_count += 1
                    
                    # Reset
                    self.speech_frames = []
                    self.silence_counter = 0
            
            # If not speech and not active, do nothing (filter out noise)
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘å›è°ƒé”™è¯¯: {e}")
    
    def get_audio_chunk(self, timeout: float = 2.0) -> Optional[Dict[str, Any]]:
        """è·å–ä¸€å—éŸ³é¢‘æ•°æ®
        
        Args:
            timeout: è¶…æ—¶æ—¶é—´(ç§’)
            
        Returns:
            åŒ…å«audio, timestamp, chunk_numberç­‰ä¿¡æ¯çš„å­—å…¸
        """
        try:
            return self.audio_buffer.get(timeout=timeout)
        except queue.Empty:
            return None
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–æ•è·çŠ¶æ€
        
        Returns:
            çŠ¶æ€å­—å…¸
        """
        return {
            'status': self.status.value,
            'chunk_count': self.chunk_count,
            'buffer_size': self.audio_buffer.qsize(),
            'config': {
                'device_id': self.config.mic_device_id,
                'sample_rate': self.config.sample_rate,
                'channels': self.config.channels,
                'chunk_duration': self.config.chunk_duration
            }
        }


class PerceptionManager:
    """æ„ŸçŸ¥ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†æ‘„åƒå¤´å’Œéº¦å…‹é£"""
    
    def __init__(self, config: Optional[PerceptionConfig] = None):
        """åˆå§‹åŒ–æ„ŸçŸ¥ç®¡ç†å™¨
        
        Args:
            config: æ„ŸçŸ¥é…ç½®,Noneä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or PerceptionConfig()
        self.camera = CameraCapture(self.config)
        self.microphone = MicrophoneCapture(self.config)
        
        # å¤„ç†å›è°ƒ
        self.video_processor: Optional[Callable] = None
        self.audio_processor: Optional[Callable] = None
        
        # å¤„ç†çº¿ç¨‹
        self.processing_thread: Optional[threading.Thread] = None
        self.stop_processing = threading.Event()
        
        logger.info("ğŸ¯ æ„ŸçŸ¥ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def set_video_processor(self, processor: Callable):
        """è®¾ç½®è§†é¢‘å¤„ç†å™¨å›è°ƒ
        
        Args:
            processor: å¤„ç†å‡½æ•°,æ¥æ”¶frameå‚æ•°
        """
        self.video_processor = processor
        logger.info("âœ… è§†é¢‘å¤„ç†å™¨å·²è®¾ç½®")
    
    def set_audio_processor(self, processor: Callable):
        """è®¾ç½®éŸ³é¢‘å¤„ç†å™¨å›è°ƒ
        
        Args:
            processor: å¤„ç†å‡½æ•°,æ¥æ”¶audio_dataå‚æ•°
        """
        self.audio_processor = processor
        logger.info("âœ… éŸ³é¢‘å¤„ç†å™¨å·²è®¾ç½®")
    
    def start_camera(self) -> bool:
        """å¯åŠ¨æ‘„åƒå¤´"""
        return self.camera.start()
    
    def stop_camera(self):
        """åœæ­¢æ‘„åƒå¤´"""
        self.camera.stop()
    
    def start_microphone(self) -> bool:
        """å¯åŠ¨éº¦å…‹é£"""
        return self.microphone.start()
    
    def stop_microphone(self):
        """åœæ­¢éº¦å…‹é£"""
        self.microphone.stop()
    
    def start_all(self) -> Dict[str, bool]:
        """å¯åŠ¨æ‰€æœ‰æ„ŸçŸ¥è®¾å¤‡
        
        Returns:
            å¯åŠ¨ç»“æœå­—å…¸
        """
        results = {
            'camera': self.start_camera(),
            'microphone': self.start_microphone()
        }
        
        # å¯åŠ¨å¤„ç†çº¿ç¨‹
        if any(results.values()):
            self.stop_processing.clear()
            self.processing_thread = threading.Thread(
                target=self._processing_loop, 
                daemon=True
            )
            self.processing_thread.start()
        
        return results
    
    def stop_all(self):
        """åœæ­¢æ‰€æœ‰æ„ŸçŸ¥è®¾å¤‡"""
        logger.info("ğŸ›‘ åœæ­¢æ‰€æœ‰æ„ŸçŸ¥è®¾å¤‡...")
        
        # åœæ­¢å¤„ç†çº¿ç¨‹
        self.stop_processing.set()
        if self.processing_thread:
            self.processing_thread.join(timeout=2.0)
        
        self.stop_camera()
        self.stop_microphone()
        
        logger.info("âœ… æ‰€æœ‰æ„ŸçŸ¥è®¾å¤‡å·²åœæ­¢")
    
    def _processing_loop(self):
        """å¤„ç†å¾ªç¯(åœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œ)"""
        logger.info("âš™ï¸ æ„ŸçŸ¥å¤„ç†å¾ªç¯å·²å¯åŠ¨")
        
        frame_counter = 0
        
        while not self.stop_processing.is_set():
            try:
                # å¤„ç†è§†é¢‘å¸§
                if (self.camera.status == CaptureStatus.RUNNING and 
                    self.video_processor):
                    
                    frame_counter += 1
                    
                    # æŒ‰é—´éš”å¤„ç†(èŠ‚çœCPU)
                    if frame_counter % self.config.process_interval == 0:
                        frame_data = self.camera.get_frame(timeout=0.1)
                        if frame_data:
                            try:
                                self.video_processor(frame_data)
                            except Exception as e:
                                logger.error(f"âŒ è§†é¢‘å¤„ç†å¤±è´¥: {e}")
                
                # å¤„ç†éŸ³é¢‘å—
                if (self.microphone.status == CaptureStatus.RUNNING and 
                    self.audio_processor):
                    
                    audio_data = self.microphone.get_audio_chunk(timeout=0.1)
                    if audio_data:
                        try:
                            self.audio_processor(audio_data)
                        except Exception as e:
                            logger.error(f"âŒ éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
                
                time.sleep(0.01)  # é¿å…CPUå ç”¨è¿‡é«˜
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†å¾ªç¯å‡ºé”™: {e}")
                time.sleep(0.1)
        
        logger.info("âš™ï¸ æ„ŸçŸ¥å¤„ç†å¾ªç¯å·²ç»“æŸ")
    
    def is_camera_running(self) -> bool:
        """æ£€æŸ¥æ‘„åƒå¤´æ˜¯å¦è¿è¡Œä¸­
        
        Returns:
            Trueè¡¨ç¤ºè¿è¡Œä¸­ï¼ŒFalseè¡¨ç¤ºåœæ­¢
        """
        return self.camera.status == CaptureStatus.RUNNING
    
    def is_microphone_running(self) -> bool:
        """æ£€æŸ¥éº¦å…‹é£æ˜¯å¦è¿è¡Œä¸­
        
        Returns:
            Trueè¡¨ç¤ºè¿è¡Œä¸­ï¼ŒFalseè¡¨ç¤ºåœæ­¢
        """
        return self.microphone.status == CaptureStatus.RUNNING
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–æ•´ä½“çŠ¶æ€
        
        Returns:
            çŠ¶æ€å­—å…¸
        """
        return {
            'camera': self.camera.get_status(),
            'microphone': self.microphone.get_status(),
            'processing_active': not self.stop_processing.is_set()
        }
    
    @staticmethod
    def list_devices() -> Dict[str, List[Dict[str, Any]]]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨è®¾å¤‡
        
        Returns:
            è®¾å¤‡åˆ—è¡¨å­—å…¸
        """
        return {
            'cameras': CameraCapture.list_devices(),
            'microphones': MicrophoneCapture.list_devices()
        }


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 60)
    print("å®æ—¶æ„ŸçŸ¥ç³»ç»Ÿæµ‹è¯•")
    print("=" * 60)
    
    # åˆ—å‡ºè®¾å¤‡
    print("\nğŸ“‹ å¯ç”¨è®¾å¤‡:")
    devices = PerceptionManager.list_devices()
    
    print("\nğŸ“· æ‘„åƒå¤´è®¾å¤‡:")
    for cam in devices['cameras']:
        print(f"  - ID {cam['id']}: {cam['name']} ({cam['backend']})")
    
    print("\nğŸ¤ éº¦å…‹é£è®¾å¤‡:")
    for mic in devices['microphones']:
        print(f"  - ID {mic['id']}: {mic['name']} "
              f"({mic['channels']}ch @ {mic['sample_rate']}Hz)")
    
    # åˆ›å»ºç®¡ç†å™¨
    print("\n" + "=" * 60)
    print("åˆ›å»ºæ„ŸçŸ¥ç®¡ç†å™¨...")
    manager = PerceptionManager()
    
    # è®¾ç½®ç®€å•çš„å¤„ç†å™¨
    def simple_video_processor(frame_data):
        print(f"ğŸ“¹ å¤„ç†å¸§ #{frame_data['frame_number']}, "
              f"æ—¶é—´: {frame_data['timestamp']}")
    
    def simple_audio_processor(audio_data):
        print(f"ğŸµ å¤„ç†éŸ³é¢‘å— #{audio_data['chunk_number']}, "
              f"æ—¶é—´: {audio_data['timestamp']}, "
              f"å¤§å°: {audio_data['audio'].shape}")
    
    manager.set_video_processor(simple_video_processor)
    manager.set_audio_processor(simple_audio_processor)
    
    # å¯åŠ¨è®¾å¤‡
    print("\nå¯åŠ¨æ„ŸçŸ¥è®¾å¤‡...")
    results = manager.start_all()
    print(f"å¯åŠ¨ç»“æœ: {results}")
    
    # è¿è¡Œ10ç§’
    print("\nè¿è¡Œ10ç§’...")
    try:
        time.sleep(10)
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­")
    
    # è·å–çŠ¶æ€
    print("\nğŸ“Š å½“å‰çŠ¶æ€:")
    status = manager.get_status()
    print(f"æ‘„åƒå¤´: {status['camera']}")
    print(f"éº¦å…‹é£: {status['microphone']}")
    
    # åœæ­¢
    print("\nåœæ­¢æ‰€æœ‰è®¾å¤‡...")
    manager.stop_all()
    
    print("\nâœ… æµ‹è¯•å®Œæˆ")
