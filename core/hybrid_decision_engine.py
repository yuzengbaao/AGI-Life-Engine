#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ··åˆå†³ç­–å¼•æ“ (Hybrid Decision Engine)
èåˆç³»ç»ŸAï¼ˆç»„ä»¶ç»„è£…ï¼‰å’Œç³»ç»ŸBï¼ˆåˆ†å½¢æ‹“æ‰‘ï¼‰çš„å†³ç­–èƒ½åŠ›

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä¸‰è·¯å†³ç­–ï¼šFractalï¼ˆå¿«ï¼‰â†’ TheSeedï¼ˆä¸­ï¼‰â†’ LLMï¼ˆæ…¢ï¼‰
2. è‡ªé€‚åº”é˜ˆå€¼ï¼šåŠ¨æ€è°ƒæ•´å†³ç­–è·¯å¾„
3. ç½®ä¿¡åº¦å­¦ä¹ ï¼šä»å†³ç­–ç»“æœä¸­å­¦ä¹ 
4. å…ƒå­¦ä¹ ï¼šMetaLearnerä¼˜åŒ–å†³ç­–ç­–ç•¥

ä½œè€…ï¼šClaude Code (Sonnet 4.5)
åˆ›å»ºæ—¥æœŸï¼š2026-01-13
ç‰ˆæœ¬ï¼šv1.0
"""

import numpy as np
import torch
import logging
import time
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from enum import Enum

# å¯¼å…¥ç³»ç»ŸAå’ŒB
try:
    from core.seed import TheSeed, Experience
except ImportError:
    TheSeed = None
    Experience = None

try:
    from core.fractal_intelligence import create_fractal_intelligence, FractalOutput
except ImportError:
    create_fractal_intelligence = None

try:
    from core.llm_client import LLMService
except ImportError:
    LLMService = None

# å¯¼å…¥åŒèºæ—‹å¼•æ“ (v2.1çº åç‰ˆ)
try:
    from core.double_helix_engine_v2 import DoubleHelixEngineV2
except ImportError:
    DoubleHelixEngineV2 = None

# ğŸ†• [P0çº§ä¼˜åŒ–] å¯¼å…¥å†³ç­–ç¼“å­˜
try:
    from core.decision_cache import DecisionCache
except ImportError:
    DecisionCache = None

logger = logging.getLogger(__name__)


class DecisionPath(Enum):
    """å†³ç­–è·¯å¾„"""
    FRACTAL = "fractal"      # ç³»ç»ŸBï¼šæœ€å¿«ï¼Œ10-15ms
    SEED = "seed"            # ç³»ç»ŸAï¼šä¸­ç­‰ï¼Œ50-100ms
    LLM = "llm"              # å¤–éƒ¨LLMï¼šæœ€æ…¢ï¼Œ200-2000ms


@dataclass
class DecisionResult:
    """å†³ç­–ç»“æœ"""
    action: int
    confidence: float
    path: DecisionPath
    response_time_ms: float
    explanation: str
    entropy: float = 0.0
    needs_validation: bool = False
    metadata: Dict[str, Any] = None


class HybridDecisionEngine:
    """
    æ··åˆå†³ç­–å¼•æ“

    ä¸‰è·¯å†³ç­–ç­–ç•¥ï¼š
    1. Fractalï¼ˆç³»ç»ŸBï¼‰- æé€Ÿæœ¬åœ°å†³ç­–ï¼ˆ10-15msï¼‰
    2. TheSeedï¼ˆç³»ç»ŸAï¼‰- DQNå¢å¼ºå†³ç­–ï¼ˆ50-100msï¼‰
    3. LLMï¼ˆå¤–éƒ¨ï¼‰- å¤æ‚æ¨ç†å†³ç­–ï¼ˆ200-2000msï¼‰
    """

    def __init__(
        self,
        state_dim: int = 64,
        action_dim: int = 4,
        device: str = 'cpu',
        enable_fractal: bool = True,
        enable_llm: bool = False,  # é»˜è®¤ç¦ç”¨LLMä»¥é™ä½æˆæœ¬
        decision_mode: str = 'round_robin'  # æ–°å¢ï¼šå†³ç­–æ¨¡å¼
    ):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = device
        self.enable_fractal = enable_fractal
        self.enable_llm = enable_llm
        self.decision_mode = decision_mode  # 'adaptive', 'round_robin', 'confidence_based'

        # 1. åˆå§‹åŒ–ç³»ç»ŸBï¼šåˆ†å½¢æ™ºèƒ½ï¼ˆæœ€å¿«ï¼‰
        self.fractal = None
        if enable_fractal and create_fractal_intelligence:
            try:
                self.fractal = create_fractal_intelligence(
                    input_dim=state_dim,
                    state_dim=state_dim,
                    device=device
                )
                logger.info("[Hybrid] ç³»ç»ŸBï¼ˆåˆ†å½¢æ™ºèƒ½ï¼‰å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"[Hybrid] ç³»ç»ŸBåˆå§‹åŒ–å¤±è´¥: {e}")
                self.enable_fractal = False

        # 2. åˆå§‹åŒ–ç³»ç»ŸAï¼šTheSeedï¼ˆä¸­ç­‰é€Ÿåº¦ï¼‰
        self.seed = None
        if TheSeed:
            try:
                self.seed = TheSeed(state_dim=state_dim, action_dim=action_dim)
                logger.info("[Hybrid] ç³»ç»ŸAï¼ˆTheSeedï¼‰å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"[Hybrid] ç³»ç»ŸAåˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            logger.warning("[Hybrid] TheSeedä¸å¯ç”¨")

        # 3. LLMæœåŠ¡ï¼ˆå¯é€‰ï¼‰
        self.llm_service = None
        if enable_llm and LLMService:
            try:
                self.llm_service = LLMService()
                logger.info("[Hybrid] LLMæœåŠ¡å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"[Hybrid] LLMåˆå§‹åŒ–å¤±è´¥: {e}")
                self.enable_llm = False

        # 4. è‡ªé€‚åº”é˜ˆå€¼ç®¡ç†ï¼ˆæé«˜åˆå§‹é˜ˆå€¼ï¼‰
        self.confidence_history: List[float] = []
        self.adaptive_threshold = 0.55  # ğŸ”§ ä¿®å¤ï¼šæé«˜åˆ°0.55ï¼Œç»™ç³»ç»ŸAæœºä¼š
        self.threshold_window = 100
        # ğŸ†• [P0ä¼˜åŒ–] åŠ¨æ€é˜ˆå€¼èŒƒå›´
        self.threshold_range = (0.4, 0.7)  # åŠ¨æ€èŒƒå›´ï¼š0.4-0.7
        self.min_threshold = 0.4
        self.max_threshold = 0.7
        # ğŸ†• [P0ä¼˜åŒ–] å¥–åŠ±å†å²ï¼ˆç”¨äºåŠ¨æ€é˜ˆå€¼è°ƒæ•´ï¼‰
        self.reward_history: List[float] = []  # ä¿å­˜æœ€è¿‘100æ¬¡å¥–åŠ±

        # 5. å†³ç­–ç»Ÿè®¡
        self.stats = {
            'total_decisions': 0,
            'fractal_decisions': 0,
            'seed_decisions': 0,
            'llm_decisions': 0,
            'cache_decisions': 0,  # ğŸ†• æ–°å¢ï¼šç¼“å­˜å†³ç­–ç»Ÿè®¡
            'avg_confidence': 0.0,
            'avg_response_time': 0.0
        }

        # 6. è½®è¯¢è®¡æ•°å™¨ï¼ˆç”¨äºround_robinæ¨¡å¼ï¼‰
        self.round_robin_counter = 0

        # ğŸ†• [P0çº§ä¼˜åŒ–] å†³ç­–ç¼“å­˜å±‚
        self.decision_cache = None
        if DecisionCache:
            self.decision_cache = DecisionCache(
                max_size=1000,
                similarity_threshold=0.85,
                ttl_seconds=3600
            )
            logger.info("[Hybrid] [P0ä¼˜åŒ–] å†³ç­–ç¼“å­˜å·²å¯ç”¨ (0mså»¶è¿Ÿ)")

        # ğŸ†• [P0çº§ä¼˜åŒ–] å¼ºåˆ¶æœ¬åœ°æ¨¡å¼é…ç½®
        self.force_local_mode = True  # é…ç½®é¡¹ï¼šå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°å†³ç­–
        self.max_llm_latency = 500.0  # msï¼šLLMæœ€å¤§å¯æ¥å—å»¶è¿Ÿ
        self.cache_fallback = True  # é…ç½®é¡¹ï¼šç¼“å­˜å›é€€

        # 7. åŒèºæ—‹å¼•æ“v2.1ï¼ˆç”¨äºdouble_helixæ¨¡å¼ï¼‰- åŒ…å«åˆ›é€ æ€§èåˆ
        self.helix_engine = None
        if decision_mode == 'double_helix' and DoubleHelixEngineV2:
            try:
                self.helix_engine = DoubleHelixEngineV2(
                    state_dim=state_dim,
                    action_dim=action_dim,
                    device=device,
                    spiral_radius=0.3,
                    phase_shift=np.pi,
                    phase_speed=0.1,
                    cycle_length=10,
                    ascent_rate=0.01,
                    enable_nonlinear=True,      # å¯ç”¨éçº¿æ€§èåˆ
                    enable_meta_learning=True,  # å¯ç”¨å…ƒå­¦ä¹ 
                    enable_dialogue=False       # æš‚ä¸å¯ç”¨å¯¹è¯å¼•æ“
                )
                logger.info("[Hybrid] ğŸš€ åŒèºæ—‹å¼•æ“v2.1å·²å¯ç”¨ - åŒ…å«åˆ›é€ æ€§èåˆå’Œäº’è¡¥ååŒ")
            except Exception as e:
                logger.warning(f"[Hybrid] åŒèºæ—‹å¼•æ“v2.1åˆå§‹åŒ–å¤±è´¥: {e}")
                logger.info("[Hybrid] å›é€€åˆ°round_robinæ¨¡å¼")
                self.decision_mode = 'round_robin'

        logger.info(f"[Hybrid] æ··åˆå†³ç­–å¼•æ“åˆå§‹åŒ–å®Œæˆ (å†³ç­–æ¨¡å¼={decision_mode})")

    def decide(
        self,
        state: np.ndarray,
        context: Optional[Dict[str, Any]] = None,
        force_path: Optional[DecisionPath] = None
    ) -> DecisionResult:
        """
        æ··åˆå†³ç­–ï¼ˆä¿®å¤ç‰ˆï¼‰

        å†³ç­–æ¨¡å¼ï¼š
        1. double_helixï¼šåŒèºæ—‹æ¨¡å¼ï¼Œç³»ç»ŸAå’ŒBç›¸äº’ç¼ ç»•ï¼ˆæ–°å¢ï¼‰
        2. round_robinï¼ˆé»˜è®¤ï¼‰ï¼šå¼ºåˆ¶è½®è¯¢Aå’ŒBï¼Œç¡®ä¿éƒ½ä½¿ç”¨
        3. adaptiveï¼šåŸºäºç½®ä¿¡åº¦è‡ªé€‚åº”é€‰æ‹©
        4. confidence_basedï¼šä¼ ç»Ÿé˜ˆå€¼æ¨¡å¼
        """
        self.stats['total_decisions'] += 1
        context = context or {}
        self.round_robin_counter += 1

        # ğŸ†• [P0ä¼˜åŒ–] å¿«é€Ÿè·¯å¾„0ï¼šå†³ç­–ç¼“å­˜æ£€æŸ¥ï¼ˆ0mså»¶è¿Ÿï¼‰
        if self.decision_cache and not force_path:
            # ç”ŸæˆçŠ¶æ€hashä½œä¸ºç¼“å­˜key
            state_hash = hash(state.tobytes())

            # å°è¯•ä»ç¼“å­˜è·å–å†³ç­–ç»“æœ
            cached_result = self.decision_cache.get(state.flatten() if hasattr(state, 'flatten') else state)
            if cached_result and cached_result[1] > 0.85:  # (intent, confidence) ä¸”ç½®ä¿¡åº¦ > 0.85
                intent, confidence, metadata = cached_result
                self.stats['cache_decisions'] += 1

                logger.debug(
                    f"[Hybrid] [ç¼“å­˜å‘½ä¸­] "
                    f"intent={intent}, "
                    f"confidence={confidence:.3f}, "
                    f"cache_decisions={self.stats['cache_decisions']}"
                )

                # æ„é€ DecisionResult
                return DecisionResult(
                    action=int(self._intent_to_action(intent, state)),
                    confidence=confidence,
                    path=DecisionPath.SEED,  # ç¼“å­˜ç»“æœæ ‡è®°ä¸ºSEEDè·¯å¾„
                    response_time_ms=0.001,  # ~0msï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
                    explanation=f"[ç¼“å­˜] {intent}",
                    metadata={'cached': True, **metadata}
                )

        # ğŸ”§ ä¿®å¤ï¼šå®ç°çœŸæ­£çš„æ··åˆå†³ç­–
        if force_path is not None:
            # å¼ºåˆ¶æŒ‡å®šè·¯å¾„
            return self._decide_by_path(force_path, state, context)

        elif self.decision_mode == 'double_helix' and self.helix_engine:
            # ğŸ§¬ åŒèºæ—‹æ¨¡å¼ï¼šç³»ç»ŸAå’ŒBç›¸äº’ç¼ ç»•ï¼Œæ¿€å‘æ™ºæ…§æ¶Œç°
            helix_result = self.helix_engine.decide(state, context)

            # è½¬æ¢ä¸ºDecisionResultæ ¼å¼
            result = DecisionResult(
                action=helix_result.action,
                confidence=helix_result.confidence,
                path=DecisionPath.SEED,  # åŒèºæ—‹èåˆï¼Œæ ‡è®°ä¸ºSEED
                response_time_ms=helix_result.response_time_ms,
                explanation=helix_result.explanation,
                entropy=helix_result.entropy,
                needs_validation=helix_result.confidence < 0.5,
                metadata={
                    'double_helix': True,
                    'weight_A': helix_result.weight_A,
                    'weight_B': helix_result.weight_B,
                    'phase': helix_result.phase,
                    'emergence': helix_result.emergence_score,
                    'cycle': helix_result.cycle_number,
                    'ascent': helix_result.ascent_level,
                    'fusion_method': helix_result.fusion_method,  # ğŸ†• èåˆæ–¹æ³•
                    'complementary_preference': self._extract_system_preference(helix_result.fusion_method)  # ğŸ†• ç³»ç»Ÿåå¥½
                }
            )

            # æ›´æ–°ç»Ÿè®¡
            if helix_result.individual_A:
                self.stats['seed_decisions'] += 1
            if helix_result.individual_B:
                self.stats['fractal_decisions'] += 1

            # ğŸ†• [P0ä¼˜åŒ–] å°è¯•ç¼“å­˜å¹¶è¿”å›
            return self._maybe_cache_and_return(state, result, intent_override="double_helix")

        elif self.decision_mode == 'round_robin':
            # ğŸ”§ è½®è¯¢æ¨¡å¼ï¼šå¼ºåˆ¶äº¤æ›¿ä½¿ç”¨Aå’ŒB
            if self.round_robin_counter % 2 == 0 and self.enable_fractal and self.fractal:
                result = self._decide_fractal(state, context)
                self.stats['fractal_decisions'] += 1
                result.explanation = f"ç³»ç»ŸBï¼ˆè½®è¯¢{self.round_robin_counter}ï¼‰- {result.explanation}"
                # ğŸ†• [P0ä¼˜åŒ–] å°è¯•ç¼“å­˜å¹¶è¿”å›
                return self._maybe_cache_and_return(state, result, intent_override="round_robin_fractal")
            elif self.seed:
                result = self._decide_seed(state, context)
                self.stats['seed_decisions'] += 1
                result.explanation = f"ç³»ç»ŸAï¼ˆè½®è¯¢{self.round_robin_counter}ï¼‰- {result.explanation}"
                # ğŸ†• [P0ä¼˜åŒ–] å°è¯•ç¼“å­˜å¹¶è¿”å›
                return self._maybe_cache_and_return(state, result, intent_override="round_robin_seed")
            else:
                # å…œåº•
                return self._get_fallback_result(state, context)

        elif self.decision_mode == 'adaptive':
            # ğŸ”§ è‡ªé€‚åº”æ¨¡å¼ï¼šåŸºäºç½®ä¿¡åº¦é€‰æ‹©ï¼Œä½†ç»™ç³»ç»ŸAæœºä¼š
            result_fractal = None
            if self.enable_fractal and self.fractal:
                result_fractal = self._decide_fractal(state, context)

            result_seed = None
            if self.seed:
                result_seed = self._decide_seed(state, context)

            # é€‰æ‹©ç½®ä¿¡åº¦æ›´é«˜çš„
            if result_fractal and result_seed:
                if result_fractal.confidence >= result_seed.confidence:
                    self.stats['fractal_decisions'] += 1
                    result_fractal.explanation = f"ç³»ç»ŸBï¼ˆè‡ªé€‚åº”é€‰æ‹©ï¼‰- {result_fractal.explanation}"
                    # ğŸ†• [P0ä¼˜åŒ–] å°è¯•ç¼“å­˜å¹¶è¿”å›
                    return self._maybe_cache_and_return(state, result_fractal, intent_override="adaptive_fractal")
                else:
                    self.stats['seed_decisions'] += 1
                    result_seed.explanation = f"ç³»ç»ŸAï¼ˆè‡ªé€‚åº”é€‰æ‹©ï¼‰- {result_seed.explanation}"
                    # ğŸ†• [P0ä¼˜åŒ–] å°è¯•ç¼“å­˜å¹¶è¿”å›
                    return self._maybe_cache_and_return(state, result_seed, intent_override="adaptive_seed")
            elif result_fractal:
                self.stats['fractal_decisions'] += 1
                # ğŸ†• [P0ä¼˜åŒ–] å°è¯•ç¼“å­˜å¹¶è¿”å›
                return self._maybe_cache_and_return(state, result_fractal, intent_override="adaptive_fractal_only")
            elif result_seed:
                self.stats['seed_decisions'] += 1
                # ğŸ†• [P0ä¼˜åŒ–] å°è¯•ç¼“å­˜å¹¶è¿”å›
                return self._maybe_cache_and_return(state, result_seed, intent_override="adaptive_seed_only")
            else:
                return self._get_fallback_result(state, context)

        else:  # confidence_based (ä¼ ç»Ÿæ¨¡å¼)
            # åŸºäºé˜ˆå€¼é€‰æ‹©
            if self.enable_fractal and self.fractal and force_path in [None, DecisionPath.FRACTAL]:
                result = self._decide_fractal(state, context)
                if result.confidence >= self.adaptive_threshold:
                    self.stats['fractal_decisions'] += 1
                    # ğŸ†• [P0ä¼˜åŒ–] å°è¯•ç¼“å­˜å¹¶è¿”å›
                    return self._maybe_cache_and_return(state, result, intent_override="confidence_fractal")

            if self.seed and force_path in [None, DecisionPath.SEED]:
                result = self._decide_seed(state, context)
                if result.confidence >= self.adaptive_threshold:
                    self.stats['seed_decisions'] += 1
                    # ğŸ†• [P0ä¼˜åŒ–] å°è¯•ç¼“å­˜å¹¶è¿”å›
                    return self._maybe_cache_and_return(state, result, intent_override="confidence_seed")

            # å…œåº•
            return self._get_fallback_result(state, context)

    def _decide_by_path(self, path: DecisionPath, state: np.ndarray, context: Dict[str, Any]) -> DecisionResult:
        """æŒ‰æŒ‡å®šè·¯å¾„å†³ç­–"""
        if path == DecisionPath.FRACTAL and self.enable_fractal and self.fractal:
            result = self._decide_fractal(state, context)
            self.stats['fractal_decisions'] += 1
            # ğŸ†• [P0ä¼˜åŒ–] å°è¯•ç¼“å­˜å¹¶è¿”å›
            return self._maybe_cache_and_return(state, result, intent_override="force_fractal")
        elif path == DecisionPath.SEED and self.seed:
            result = self._decide_seed(state, context)
            self.stats['seed_decisions'] += 1
            # ğŸ†• [P0ä¼˜åŒ–] å°è¯•ç¼“å­˜å¹¶è¿”å›
            return self._maybe_cache_and_return(state, result, intent_override="force_seed")
        elif path == DecisionPath.LLM and self.enable_llm and self.llm_service:
            result = self._decide_llm(state, context)
            self.stats['llm_decisions'] += 1
            # ğŸ†• [P0ä¼˜åŒ–] å°è¯•ç¼“å­˜å¹¶è¿”å›
            return self._maybe_cache_and_return(state, result, intent_override="llm")
        else:
            return self._get_fallback_result(state, context)

    def _get_fallback_result(self, state: np.ndarray, context: Dict[str, Any]) -> DecisionResult:
        """è·å–å…œåº•ç»“æœ"""
        # ä¼˜å…ˆä½¿ç”¨ç³»ç»ŸB
        if self.enable_fractal and self.fractal:
            result = self._decide_fractal(state, context)
            self.stats['fractal_decisions'] += 1
            return result
        # å…¶æ¬¡ä½¿ç”¨ç³»ç»ŸA
        if self.seed:
            result = self._decide_seed(state, context)
            self.stats['seed_decisions'] += 1
            return result
        # æœ€åå…œåº•ï¼šéšæœºå†³ç­–
        return DecisionResult(
            action=np.random.randint(0, self.action_dim),
            confidence=0.3,
            path=DecisionPath.FRACTAL,
            response_time_ms=0.1,
            explanation="éšæœºå†³ç­–ï¼ˆæ‰€æœ‰ç³»ç»Ÿä¸å¯ç”¨ï¼‰",
            needs_validation=True
        )

    def _decide_fractal(
        self,
        state: np.ndarray,
        context: Dict[str, Any]
    ) -> DecisionResult:
        """ç³»ç»ŸBå†³ç­–ï¼šåˆ†å½¢æ‹“æ‰‘æ™ºèƒ½"""
        start_time = time.time()

        # è½¬æ¢ä¸ºTensor
        state_tensor = torch.from_numpy(state).float().to(self.device)

        # Fractalå†³ç­–
        with torch.no_grad():
            output, meta = self.fractal.core(state_tensor, return_meta=True)

        response_time = (time.time() - start_time) * 1000

        # æå–å†³ç­–ä¿¡æ¯
        confidence = meta.self_awareness.mean().item()
        entropy = meta.entropy.item()

        # ä»è¾“å‡ºæ¨æ–­åŠ¨ä½œï¼ˆç®€åŒ–å¤„ç†ï¼‰
        if output.dim() > 1:
            action = torch.argmax(output, dim=-1).item() % self.action_dim
        else:
            action = int(output.item() % self.action_dim)

        return DecisionResult(
            action=int(action),
            confidence=confidence,
            path=DecisionPath.FRACTAL,
            response_time_ms=response_time,
            explanation=f"ç³»ç»ŸBï¼ˆåˆ†å½¢æ‹“æ‰‘ï¼‰- ç½®ä¿¡åº¦{confidence:.4f}",
            entropy=entropy,
            needs_validation=confidence < self.adaptive_threshold,
            metadata={
                'goal_score': meta.goal_score,
                'metaparams': meta.metaparams
            }
        )

    def _decide_seed(
        self,
        state: np.ndarray,
        context: Dict[str, Any]
    ) -> DecisionResult:
        """ç³»ç»ŸAå†³ç­–ï¼šTheSeed DQN"""
        start_time = time.time()

        # TheSeedå†³ç­–
        action = self.seed.act(state)
        value = self.seed.evaluate(state, state, 0.0)
        confidence = min(1.0, max(0.0, value))

        response_time = (time.time() - start_time) * 1000

        return DecisionResult(
            action=int(action),
            confidence=confidence,
            path=DecisionPath.SEED,
            response_time_ms=response_time,
            explanation=f"ç³»ç»ŸAï¼ˆTheSeedï¼‰- ä»·å€¼{value:.4f}",
            entropy=0.5,
            needs_validation=confidence < self.adaptive_threshold
        )

    def _decide_llm(
        self,
        state: np.ndarray,
        context: Dict[str, Any]
    ) -> DecisionResult:
        """å¤–éƒ¨LLMå†³ç­–"""
        start_time = time.time()

        # ç®€åŒ–çš„LLMå†³ç­–ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦å®Œæ•´å®ç°ï¼‰
        # è¿™é‡Œè¿”å›åŸºäºçŠ¶æ€åˆ†æçš„ä¼ªå†³ç­–

        response_time = (time.time() - start_time) * 1000

        # ç®€åŒ–ï¼šåŸºäºçŠ¶æ€çš„hashæ¥å†³å®šåŠ¨ä½œ
        action = int(hash(state.tobytes()) % self.action_dim)

        return DecisionResult(
            action=action,
            confidence=0.7,  # LLMé€šå¸¸æœ‰è¾ƒé«˜ç½®ä¿¡åº¦
            path=DecisionPath.LLM,
            response_time_ms=response_time,
            explanation=f"å¤–éƒ¨LLMå†³ç­–ï¼ˆç®€åŒ–ç‰ˆï¼‰",
            entropy=0.3,
            needs_validation=False
        )

    def learn(
        self,
        state: np.ndarray,
        action: int,
        reward: float,
        next_state: np.ndarray
    ):
        """
        ä»ç»éªŒä¸­å­¦ä¹ ï¼ˆä¸¤è·¯å­¦ä¹ ï¼‰

        1. TheSeedï¼šDQNå­¦ä¹ 
        2. Fractalï¼šç›®æ ‡ä¿®æ”¹
        """
        # 1. TheSeedå­¦ä¹ 
        if self.seed and Experience:
            experience = Experience(
                state=state,
                action=action,
                reward=reward,
                next_state=next_state
            )
            self.seed.learn(experience)

        # 2. Fractalå­¦ä¹ 
        if self.enable_fractal and self.fractal:
            try:
                exp_dict = {'state': torch.from_numpy(state).float().to(self.device)}
                self.fractal.learn(exp_dict, reward)
            except Exception as e:
                logger.debug(f"[Hybrid] Fractalå­¦ä¹ å¤±è´¥ï¼ˆæ­£å¸¸ï¼‰: {e}")

        # 3. æ›´æ–°è‡ªé€‚åº”é˜ˆå€¼
        self._update_adaptive_threshold(reward)

    def _update_adaptive_threshold(self, reward: float):
        """ğŸ†• [P0ä¼˜åŒ–] æ›´æ–°è‡ªé€‚åº”ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆå¢å¼ºç‰ˆï¼‰

        åŸºäºå†å²æ€§èƒ½åŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼š
        1. è®°å½•å¥–åŠ±å†å²
        2. è®¡ç®—æœ€è¿‘20æ¬¡å¥–åŠ±å¹³å‡å€¼
        3. åŸºäºå¹³å‡å€¼åŠ¨æ€è°ƒæ•´é˜ˆå€¼
        """
        # 1. è®°å½•å¥–åŠ±å†å²
        self.reward_history.append(reward)
        if len(self.reward_history) > 100:
            self.reward_history.pop(0)

        # 2. åŸºäºå¥–åŠ±è°ƒæ•´é˜ˆå€¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        if reward > 0:
            # æ­£å¥–åŠ±ï¼šé™ä½é˜ˆå€¼ï¼Œæ›´å¤šä½¿ç”¨æœ¬åœ°å†³ç­–
            self.adaptive_threshold = max(self.min_threshold, self.adaptive_threshold - 0.001)
        else:
            # è´Ÿå¥–åŠ±ï¼šæé«˜é˜ˆå€¼ï¼Œæ›´åŠ è°¨æ…
            self.adaptive_threshold = min(self.max_threshold, self.adaptive_threshold + 0.001)

        # 3. ğŸ†• åŸºäºå†å²æ€§èƒ½åŠ¨æ€è°ƒæ•´ï¼ˆæ–°å¢é€»è¾‘ï¼‰
        if len(self.reward_history) >= 20:
            # è®¡ç®—æœ€è¿‘20æ¬¡çš„å¹³å‡å¥–åŠ±
            recent_avg = np.mean(self.reward_history[-20:])

            # åŸºäºå†å²å¹³å‡å€¼è¿›è¡ŒäºŒæ¬¡è°ƒæ•´
            if recent_avg > 0.7:
                # é«˜æ€§èƒ½ï¼šé™ä½é˜ˆå€¼ï¼Œæ›´æ¿€è¿›ä½¿ç”¨æœ¬åœ°å†³ç­–
                adjustment = -0.005
                self.adaptive_threshold = max(
                    self.min_threshold,
                    self.adaptive_threshold + adjustment
                )
            elif recent_avg < 0.4:
                # ä½æ€§èƒ½ï¼šæé«˜é˜ˆå€¼ï¼Œæ›´è°¨æ…å†³ç­–
                adjustment = 0.005
                self.adaptive_threshold = min(
                    self.max_threshold,
                    self.adaptive_threshold + adjustment
                )

            logger.debug(
                f"[Hybrid] [åŠ¨æ€é˜ˆå€¼] "
                f"threshold={self.adaptive_threshold:.4f}, "
                f"recent_avg_reward={recent_avg:.3f}, "
                f"history_size={len(self.reward_history)}"
            )

        logger.debug(f"[Hybrid] é˜ˆå€¼æ›´æ–°: {self.adaptive_threshold:.4f} (reward={reward:.2f})")

    def _extract_system_preference(self, fusion_method: str) -> str:
        """ä»èåˆæ–¹æ³•ä¸­æå–ç³»ç»Ÿåå¥½"""
        if 'complementary_selection_A' in fusion_method:
            return 'A'
        elif 'complementary_selection_B' in fusion_method:
            return 'B'
        elif 'creative_fusion' in fusion_method:
            return 'creative'
        else:
            return 'neutral'

    def _store_to_cache(
        self,
        state: np.ndarray,
        intent: str,
        confidence: float,
        metadata: Dict[str, Any]
    ):
        """ğŸ†• [P0ä¼˜åŒ–] å­˜å‚¨å†³ç­–ç»“æœåˆ°ç¼“å­˜"""
        if not self.decision_cache:
            return

        # åªç¼“å­˜é«˜ç½®ä¿¡åº¦ç»“æœï¼ˆ> 0.7ï¼‰
        if confidence > 0.7:
            state_embedding = state.flatten() if hasattr(state, 'flatten') else state
            self.decision_cache.put(
                text_embedding=state_embedding,
                intent=intent,
                confidence=confidence,
                metadata=metadata
            )
            logger.debug(
                f"[Hybrid] [ç¼“å­˜å­˜å‚¨] "
                f"intent={intent}, "
                f"confidence={confidence:.3f}"
            )

    def _maybe_cache_and_return(
        self,
        state: np.ndarray,
        result: DecisionResult,
        intent_override: Optional[str] = None
    ) -> DecisionResult:
        """ğŸ†• [P0ä¼˜åŒ–] å°è¯•ç¼“å­˜ç»“æœå¹¶è¿”å›

        Args:
            state: åŸå§‹çŠ¶æ€
            result: å†³ç­–ç»“æœ
            intent_override: å¯é€‰çš„æ„å›¾è¦†ç›–ï¼ˆç”¨äºä»explanationæå–æ„å›¾ï¼‰

        Returns:
            DecisionResult: åŸå§‹ç»“æœï¼ˆå¯èƒ½è¢«ç¼“å­˜ï¼‰
        """
        # åªç¼“å­˜é«˜ç½®ä¿¡åº¦ç»“æœ
        if result.confidence > 0.7 and self.decision_cache:
            # ç”Ÿæˆintentå­—ç¬¦ä¸²
            if intent_override:
                intent = intent_override
            else:
                # ä»explanationæå–ç®€å•çš„intentæ ‡è¯†
                intent = result.explanation.split('-')[0].strip() if '-' in result.explanation else result.explanation[:20]

            # æå–è·¯å¾„æ ‡è¯†ä½œä¸ºintentçš„ä¸€éƒ¨åˆ†
            path_name = result.path.name if hasattr(result.path, 'name') else str(result.path)

            # æ„é€ å®Œæ•´intent
            full_intent = f"{path_name}:{intent}"

            # å­˜å‚¨åˆ°ç¼“å­˜
            self._store_to_cache(
                state=state,
                intent=full_intent,
                confidence=result.confidence,
                metadata={
                    'path': path_name,
                    'explanation': result.explanation,
                    'needs_validation': result.needs_validation
                }
            )

        return result

    def _intent_to_action(self, intent: str, state: np.ndarray) -> int:
        """ğŸ†• [P0ä¼˜åŒ–] å°†æ„å›¾è½¬æ¢ä¸ºåŠ¨ä½œ

        Args:
            intent: æ„å›¾å­—ç¬¦ä¸²ï¼ˆå¦‚ 'file_read', 'system_status'ï¼‰
            state: å½“å‰çŠ¶æ€ï¼ˆç”¨äºä¸Šä¸‹æ–‡ï¼‰

        Returns:
            int: åŠ¨ä½œID
        """
        # ç®€åŒ–ç‰ˆæ˜ å°„ï¼šåŸºäºintentçš„hashæ˜ å°„åˆ°actionç©ºé—´
        # ç¡®ä¿åŒä¸€ä¸ªintentæ€»æ˜¯æ˜ å°„åˆ°åŒä¸€ä¸ªaction
        action = hash(intent) % self.action_dim

        # å¯é€‰ï¼šåŸºäºçŠ¶æ€è¿›è¡Œå¾®è°ƒï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰
        state_factor = int(np.sum(state[:10]) * 100) % self.action_dim
        action = (action + state_factor) % self.action_dim

        return action

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–å†³ç­–ç»Ÿè®¡"""
        stats = self.stats.copy()

        if stats['total_decisions'] > 0:
            stats['fractal_ratio'] = stats['fractal_decisions'] / stats['total_decisions']
            stats['seed_ratio'] = stats['seed_decisions'] / stats['total_decisions']
            stats['llm_ratio'] = stats['llm_decisions'] / stats['total_decisions']
            stats['external_dependency'] = stats.get('llm_ratio', 0.0)

        stats['adaptive_threshold'] = self.adaptive_threshold

        # ğŸ†• [P0ä¼˜åŒ–] æ·»åŠ ç¼“å­˜ç»Ÿè®¡
        if self.decision_cache:
            cache_stats = self.decision_cache.get_statistics()
            stats['cache'] = {
                'enabled': True,
                'hit_rate': cache_stats['hit_rate'],
                'hits': cache_stats['hits'],
                'misses': cache_stats['misses'],
                'size': cache_stats['cache_size'],
                'max_size': cache_stats['max_size']
            }
            # è®¡ç®—æœ¬åœ°å†³ç­–å‘½ä¸­ç‡ï¼ˆç¼“å­˜ + Fractal + Seedï¼‰
            total_local = stats.get('cache_decisions', 0) + stats['fractal_decisions'] + stats['seed_decisions']
            stats['local_hit_rate'] = total_local / max(stats['total_decisions'], 1)
        else:
            stats['cache'] = {'enabled': False}

        # æ·»åŠ åŒèºæ—‹ç»Ÿè®¡
        if self.helix_engine:
            helix_stats = self.helix_engine.get_statistics()
            stats['double_helix'] = {
                'enabled': True,
                'current_phase': helix_stats['current_phase'],
                'current_weight_A': helix_stats['current_weight_A'],
                'current_weight_B': helix_stats['current_weight_B'],
                'cycle_number': helix_stats['cycle_number'],
                'ascent_level': helix_stats['ascent_level'],
                'avg_emergence': helix_stats['avg_emergence'],
                'cycles_completed': helix_stats['cycles_completed']
            }
        else:
            stats['double_helix'] = {'enabled': False}

        return stats


# ä¾¿æ·å‡½æ•°
def create_hybrid_decision_engine(
    state_dim: int = 64,
    action_dim: int = 4,
    device: str = 'cpu',
    enable_fractal: bool = True,
    enable_llm: bool = False
) -> HybridDecisionEngine:
    """åˆ›å»ºæ··åˆå†³ç­–å¼•æ“"""
    return HybridDecisionEngine(
        state_dim=state_dim,
        action_dim=action_dim,
        device=device,
        enable_fractal=enable_fractal,
        enable_llm=enable_llm
    )
