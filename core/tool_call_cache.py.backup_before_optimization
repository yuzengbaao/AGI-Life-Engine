#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å·¥å…·è°ƒç”¨ç¼“å­˜ä¼˜åŒ–å™¨
Tool Call Cache Optimizer

P2-2 ä¿®å¤: è§£å†³å·¥å…·ä¼˜å…ˆæ¨¡å¼ä¸‹ Token æ¶ˆè€—å¢åŠ  3 å€çš„é—®é¢˜

é—®é¢˜åˆ†æ:
- å·¥å…·ä¼˜å…ˆæ¨¡å¼ä¼šå¤šæ¬¡è°ƒç”¨ç›¸åŒå·¥å…·/å‚æ•°
- LLM æ¯æ¬¡éƒ½éœ€è¦é‡æ–°ç”Ÿæˆç›¸åŒçš„å“åº”
- Token æ¶ˆè€—éšè°ƒç”¨æ¬¡æ•°çº¿æ€§å¢é•¿

è§£å†³æ–¹æ¡ˆ:
1. åŸºäºå‚æ•°å“ˆå¸Œçš„ç¼“å­˜é”®
2. è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…
3. TTL è‡ªåŠ¨è¿‡æœŸ
4. ç¼“å­˜å‘½ä¸­ç»Ÿè®¡

ä½œè€…: AGI System
æ—¥æœŸ: 2026-02-04
"""

import hashlib
import json
import time
import logging
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, field
from collections import OrderedDict
import numpy as np

logger = logging.getLogger(__name__)


@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    cache_key: str
    tool_name: str
    params: Dict[str, Any]
    result: Dict[str, Any]
    timestamp: float
    last_accessed: float
    access_count: int
    ttl: float = 3600.0  # é»˜è®¤ 1 å°æ—¶

    def age(self) -> float:
        """ç¼“å­˜å¹´é¾„ï¼ˆç§’ï¼‰"""
        return time.time() - self.timestamp

    def access_age(self) -> float:
        """è·ç¦»ä¸Šæ¬¡è®¿é—®æ—¶é—´ï¼ˆç§’ï¼‰"""
        return time.time() - self.last_accessed

    def is_expired(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
        return self.age() > self.ttl

    def touch(self):
        """æ›´æ–°è®¿é—®æ—¶é—´å’Œè®¡æ•°"""
        self.last_accessed = time.time()
        self.access_count += 1

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "cache_key": self.cache_key,
            "tool_name": self.tool_name,
            "params": self.params,
            "result": self.result,
            "timestamp": self.timestamp,
            "last_accessed": self.last_accessed,
            "access_count": self.access_count,
            "ttl": self.ttl,
            "age_seconds": self.age(),
            "is_expired": self.is_expired(),
        }


class ToolCallCache:
    """
    å·¥å…·è°ƒç”¨ç¼“å­˜å™¨

    æ ¸å¿ƒåŠŸèƒ½:
    1. åŸºäºå‚æ•°å“ˆå¸Œçš„å¿«é€Ÿç¼“å­˜é”®ç”Ÿæˆ
    2. LRU æ·˜æ±°ç­–ç•¥
    3. TTL è‡ªåŠ¨è¿‡æœŸ
    4. ç¼“å­˜å‘½ä¸­ç‡ç»Ÿè®¡
    """

    def __init__(
        self,
        max_size: int = 1000,  # æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
        default_ttl: float = 3600.0,  # é»˜è®¤ TTL (ç§’)
        enable_semantic_match: bool = False,  # å¯ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…
        semantic_threshold: float = 0.85,  # è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
    ):
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.enable_semantic_match = enable_semantic_match
        self.semantic_threshold = semantic_threshold

        # æœ‰åºå­—å…¸: {cache_key: CacheEntry}
        self.cache: OrderedDict[str, CacheEntry] = OrderedDict()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0,
            "expirations": 0,
            "total_calls": 0,
        }

        logger.info(
            f"ğŸ’¾ ToolCallCache åˆå§‹åŒ–: "
            f"max_size={max_size}, "
            f"default_ttl={default_ttl}s, "
            f"semantic_match={enable_semantic_match}"
        )

    def generate_cache_key(self, tool_name: str, params: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®

        ä½¿ç”¨ SHA256 å“ˆå¸Œå·¥å…·åå’Œåºåˆ—åŒ–å‚æ•°

        Args:
            tool_name: å·¥å…·åç§°
            params: å·¥å…·å‚æ•°

        Returns:
            ç¼“å­˜é”® (SHA256 å“ˆå¸Œçš„å‰ 16 å­—ç¬¦)
        """
        # è§„èŒƒåŒ–å‚æ•°ï¼šæ’åºé”®ã€ç§»é™¤ None å€¼
        normalized_params = self._normalize_params(params)

        # åºåˆ—åŒ–ä¸º JSON
        cache_input = {
            "tool": tool_name,
            "params": normalized_params,
        }
        json_str = json.dumps(cache_input, sort_keys=True, ensure_ascii=False)

        # è®¡ç®— SHA256 å“ˆå¸Œ
        hash_obj = hashlib.sha256(json_str.encode("utf-8"))
        return f"{tool_name}_{hash_obj.hexdigest()[:16]}"

    def _normalize_params(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§„èŒƒåŒ–å‚æ•°

        å¤„ç†:
        - æ’åºé”®
        - ç§»é™¤ None å€¼
        - è½¬æ¢é›†åˆä¸ºåˆ—è¡¨
        """
        normalized = {}

        for key in sorted(params.keys()):
            value = params[key]

            if value is None:
                continue

            # è½¬æ¢é›†åˆä¸ºåˆ—è¡¨ï¼ˆå¯å“ˆå¸Œï¼‰
            if isinstance(value, (set, frozenset)):
                value = list(value)

            # é€’å½’è§„èŒƒåµŒå¥—å­—å…¸
            if isinstance(value, dict):
                value = self._normalize_params(value)

            normalized[key] = value

        return normalized

    def get(self, tool_name: str, params: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        è·å–ç¼“å­˜ç»“æœ

        Args:
            tool_name: å·¥å…·åç§°
            params: å·¥å…·å‚æ•°

        Returns:
            ç¼“å­˜çš„ç»“æœï¼Œå¦‚æœæœªå‘½ä¸­åˆ™è¿”å› None
        """
        self.stats["total_calls"] += 1
        cache_key = self.generate_cache_key(tool_name, params)

        # ç²¾ç¡®åŒ¹é…
        if cache_key in self.cache:
            entry = self.cache[cache_key]

            if entry.is_expired():
                # è¿‡æœŸï¼Œåˆ é™¤å¹¶è¿”å› None
                del self.cache[cache_key]
                self.stats["expirations"] += 1
                self.stats["misses"] += 1
                return None

            # å‘½ä¸­ï¼Œæ›´æ–°è®¿é—®æ—¶é—´
            entry.touch()
            # ç§»åˆ°æœ«å°¾ï¼ˆLRUï¼‰
            self.cache.move_to_end(cache_key)

            self.stats["hits"] += 1
            logger.debug(f"âœ… ç¼“å­˜å‘½ä¸­: {tool_name} (key: {cache_key})")
            return entry.result

        # è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_semantic_match:
            similar = self._find_similar(tool_name, params)
            if similar:
                entry = self.cache[similar]
                entry.touch()
                self.cache.move_to_end(similar)

                self.stats["hits"] += 1
                logger.debug(
                    f"âœ… è¯­ä¹‰ç¼“å­˜å‘½ä¸­: {tool_name} (key: {similar}, similarity: >{self.semantic_threshold})"
                )
                return entry.result

        self.stats["misses"] += 1
        logger.debug(f"âŒ ç¼“å­˜æœªå‘½ä¸­: {tool_name} (key: {cache_key})")
        return None

    def _find_similar(
        self, tool_name: str, params: Dict[str, Any]
    ) -> Optional[str]:
        """
        æŸ¥æ‰¾è¯­ä¹‰ç›¸ä¼¼çš„ç¼“å­˜æ¡ç›®

        ç®€åŒ–å®ç°ï¼šåŸºäºå‚æ•°é”®çš„ç›¸ä¼¼åº¦
        ï¼ˆå®Œæ•´å®ç°éœ€è¦åµŒå…¥æ¨¡å‹å’Œå‘é‡ç›¸ä¼¼åº¦è®¡ç®—ï¼‰

        Args:
            tool_name: å·¥å…·åç§°
            params: å·¥å…·å‚æ•°

        Returns:
            ç›¸ä¼¼ç¼“å­˜é”®ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
        """
        target_keys = set(params.keys())
        best_key = None
        best_similarity = 0.0

        for cache_key, entry in self.cache.items():
            if entry.tool_name != tool_name:
                continue

            cached_keys = set(entry.params.keys())

            # Jaccard ç›¸ä¼¼åº¦
            intersection = len(target_keys & cached_keys)
            union = len(target_keys | cached_keys)
            similarity = intersection / union if union > 0 else 0.0

            if similarity > best_similarity and similarity >= self.semantic_threshold:
                best_similarity = similarity
                best_key = cache_key

        return best_key

    def put(
        self,
        tool_name: str,
        params: Dict[str, Any],
        result: Dict[str, Any],
        ttl: Optional[float] = None,
    ) -> str:
        """
        å­˜å‚¨ç¼“å­˜ç»“æœ

        Args:
            tool_name: å·¥å…·åç§°
            params: å·¥å…·å‚æ•°
            result: æ‰§è¡Œç»“æœ
            ttl: è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNone åˆ™ä½¿ç”¨é»˜è®¤å€¼

        Returns:
            ç¼“å­˜é”®
        """
        cache_key = self.generate_cache_key(tool_name, params)

        entry = CacheEntry(
            cache_key=cache_key,
            tool_name=tool_name,
            params=params.copy(),
            result=result,
            timestamp=time.time(),
            last_accessed=time.time(),
            access_count=1,
            ttl=ttl or self.default_ttl,
        )

        # æ£€æŸ¥ç¼“å­˜å¤§å°ï¼Œå¿…è¦æ—¶æ·˜æ±°
        if len(self.cache) >= self.max_size and cache_key not in self.cache:
            self._evict_lru()

        self.cache[cache_key] = entry
        self.cache.move_to_end(cache_key)

        logger.debug(f"ğŸ’¾ ç¼“å­˜å­˜å‚¨: {tool_name} (key: {cache_key})")
        return cache_key

    def _evict_lru(self):
        """æ·˜æ±°æœ€è¿‘æœ€å°‘ä½¿ç”¨çš„æ¡ç›®"""
        if not self.cache:
            return

        # ç§»é™¤ç¬¬ä¸€ä¸ªï¼ˆæœ€æ—§çš„ï¼‰
        lru_key, lru_entry = self.cache.popitem(last=False)
        self.stats["evictions"] += 1

        logger.debug(f"ğŸ—‘ï¸ LRU æ·˜æ±°: {lru_entry.tool_name} (key: {lru_key})")

    def invalidate(self, tool_name: Optional[str] = None):
        """
        ä½¿ç¼“å­˜å¤±æ•ˆ

        Args:
            tool_name: å·¥å…·åç§°ï¼ŒNone åˆ™æ¸…ç©ºå…¨éƒ¨ç¼“å­˜
        """
        if tool_name is None:
            # æ¸…ç©ºå…¨éƒ¨
            count = len(self.cache)
            self.cache.clear()
            logger.info(f"ğŸ§¹ ç¼“å­˜å·²æ¸…ç©º: {count} æ¡")
        else:
            # æŒ‰å·¥å…·åå¤±æ•ˆ
            to_remove = [
                key
                for key, entry in self.cache.items()
                if entry.tool_name == tool_name
            ]
            for key in to_remove:
                del self.cache[key]
            logger.info(f"ğŸ§¹ {tool_name} ç¼“å­˜å·²å¤±æ•ˆ: {len(to_remove)} æ¡")

    def cleanup_expired(self) -> int:
        """
        æ¸…ç†è¿‡æœŸæ¡ç›®

        Returns:
            æ¸…ç†çš„æ¡ç›®æ•°
        """
        expired_keys = [
            key
            for key, entry in self.cache.items()
            if entry.is_expired()
        ]

        for key in expired_keys:
            del self.cache[key]
            self.stats["expirations"] += 1

        if expired_keys:
            logger.info(f"ğŸ§¹ æ¸…ç†è¿‡æœŸç¼“å­˜: {len(expired_keys)} æ¡")

        return len(expired_keys)

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total = self.stats["total_calls"]
        hits = self.stats["hits"]
        misses = self.stats["misses"]

        hit_rate = hits / total if total > 0 else 0.0
        miss_rate = misses / total if total > 0 else 0.0

        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "hits": hits,
            "misses": misses,
            "total_calls": total,
            "hit_rate": f"{hit_rate * 100:.1f}%",
            "miss_rate": f"{miss_rate * 100:.1f}%",
            "evictions": self.stats["evictions"],
            "expirations": self.stats["expirations"],
            "tokens_saved_estimate": hits * 100,  # å‡è®¾æ¯æ¬¡å‘½ä¸­èŠ‚çœ 100 tokens
        }

    def export_entries(self) -> List[Dict[str, Any]]:
        """å¯¼å‡ºæ‰€æœ‰ç¼“å­˜æ¡ç›®"""
        return [entry.to_dict() for entry in self.cache.values()]

    def save_state(self, filepath: str):
        """ä¿å­˜ç¼“å­˜çŠ¶æ€"""
        state = {
            "cache": {key: entry.to_dict() for key, entry in self.cache.items()},
            "stats": self.stats,
            "config": {
                "max_size": self.max_size,
                "default_ttl": self.default_ttl,
                "enable_semantic_match": self.enable_semantic_match,
                "semantic_threshold": self.semantic_threshold,
            },
        }

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(state, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ ç¼“å­˜çŠ¶æ€å·²ä¿å­˜: {filepath}")

    def load_state(self, filepath: str):
        """åŠ è½½ç¼“å­˜çŠ¶æ€"""
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                state = json.load(f)

            # æ¢å¤ç¼“å­˜æ¡ç›®
            self.cache.clear()
            for key, entry_dict in state.get("cache", {}).items():
                entry = CacheEntry(
                    cache_key=entry_dict["cache_key"],
                    tool_name=entry_dict["tool_name"],
                    params=entry_dict["params"],
                    result=entry_dict["result"],
                    timestamp=entry_dict["timestamp"],
                    last_accessed=entry_dict["last_accessed"],
                    access_count=entry_dict["access_count"],
                    ttl=entry_dict.get("ttl", self.default_ttl),
                )
                self.cache[key] = entry

            # æ¢å¤ç»Ÿè®¡
            self.stats = state.get("stats", self.stats.copy())

            logger.info(f"ğŸ“‚ ç¼“å­˜çŠ¶æ€å·²åŠ è½½: {len(self.cache)} æ¡")

        except FileNotFoundError:
            logger.warning(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")


# å…¨å±€å•ä¾‹å®ä¾‹
_global_cache: Optional[ToolCallCache] = None


def get_tool_call_cache() -> ToolCallCache:
    """è·å–å…¨å±€å·¥å…·è°ƒç”¨ç¼“å­˜å®ä¾‹"""
    global _global_cache

    if _global_cache is None:
        _global_cache = ToolCallCache(
            max_size=1000,
            default_ttl=3600.0,
            enable_semantic_match=True,
        )

    return _global_cache


def reset_tool_call_cache():
    """é‡ç½®å…¨å±€å·¥å…·è°ƒç”¨ç¼“å­˜"""
    global _global_cache
    _global_cache = None
    logger.info("ğŸ”„ å…¨å±€å·¥å…·è°ƒç”¨ç¼“å­˜å·²é‡ç½®")
