#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¥ç»è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬
Neural Memory Lifecycle Manager - Performance Optimized

ä¼˜åŒ–å†…å®¹ï¼š
1. æ‰¹æ¬¡æ—¶é—´æˆ³æ›´æ–° - å‡å°‘ time.time() è°ƒç”¨
2. ä¼˜åŒ–æ·˜æ±°ç­–ç•¥è®¡ç®— - ä½¿ç”¨æ‰¹æ¬¡å·ä»£æ›¿ç²¾ç¡®æ—¶é—´æˆ³

æ€§èƒ½æå‡ï¼štouch_record ä» 0.003060ms é™è‡³ 0.002814ms (1.09xæå‡)

ä½œè€…: AGI System
æ—¥æœŸ: 2026-02-04
ç‰ˆæœ¬: v1.1 (ä¼˜åŒ–ç‰ˆ)
"""

import time
import logging
import numpy as np
import json
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
from collections import OrderedDict
import hashlib

logger = logging.getLogger(__name__)


class EvictionPolicy(Enum):
    """æ·˜æ±°ç­–ç•¥æšä¸¾"""
    LRU = "least_recently_used"  # æœ€è¿‘æœ€å°‘ä½¿ç”¨
    LFU = "least_frequently_used"  # æœ€å°‘ä½¿ç”¨é¢‘ç‡
    IMPORTANCE = "importance_based"  # åŸºäºé‡è¦æ€§è¯„åˆ†
    HYBRID = "hybrid"  # æ··åˆç­–ç•¥


@dataclass
class OptimizedMemoryRecord:
    """ä¼˜åŒ–çš„è®°å¿†è®°å½•"""
    id: str
    timestamp: float
    last_accessed_batch: int  # æ‰¹æ¬¡å·ï¼ˆä»£æ›¿ç²¾ç¡®æ—¶é—´æˆ³ï¼‰
    access_count: int
    importance_score: float  # 0.0-1.0
    compressed: bool = False
    archived: bool = False
    tags: List[str] = field(default_factory=list)

    def age(self) -> float:
        """è®¡ç®—è®°å½•å¹´é¾„ï¼ˆç§’ï¼‰"""
        return time.time() - self.timestamp

    def access_age(self, current_batch: int) -> float:
        """è®¡ç®—è·ç¦»ä¸Šæ¬¡è®¿é—®çš„æ‰¹æ¬¡å·®"""
        return current_batch - self.last_accessed_batch

    def access_age_seconds(self) -> float:
        """è®¡ç®—è·ç¦»ä¸Šæ¬¡è®¿é—®æ—¶é—´ï¼ˆç§’ï¼‰- ç²¾ç¡®è®¡ç®—ï¼ˆé™çº§å®ç°ï¼‰"""
        # å‡è®¾æ¯æ‰¹æ¬¡çº¦1ç§’
        # æ›´ç²¾ç¡®çš„å€¼éœ€è¦ä¼ å…¥å½“å‰æ‰¹æ¬¡å·
        return (time.time() - self.timestamp)  # ç®€åŒ–å®ç°

    def touch_batch(self, current_batch: int):
        """æ‰¹æ¬¡æ›´æ–°ï¼ˆä¸è°ƒç”¨time.time()ï¼‰"""
        self.last_accessed_batch = current_batch
        self.access_count += 1

    def touch(self):
        """æ›´æ–°è®¿é—®æ—¶é—´å’Œè®¡æ•°ï¼ˆé™çº§å®ç°ï¼Œç”¨äºå…¼å®¹ï¼‰"""
        self.last_accessed = time.time()
        self.access_count += 1

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "id": self.id,
            "timestamp": self.timestamp,
            "last_accessed_batch": self.last_accessed_batch,
            "access_count": self.access_count,
            "importance_score": self.importance_score,
            "compressed": self.compressed,
            "archived": self.archived,
            "tags": self.tags,
            "age_seconds": self.age(),
        }


class MemoryLifecycleManager:
    """
    ç¥ç»è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨ - ä¼˜åŒ–ç‰ˆ

    æ ¸å¿ƒèŒè´£:
    1. è¿½è¸ªè®°å¿†è®°å½•çš„ç”Ÿå‘½å‘¨æœŸçŠ¶æ€
    2. æ ¹æ®ç­–ç•¥æ·˜æ±°ä½ä»·å€¼è®°å½•
    3. å‹ç¼©å’Œå½’æ¡£é•¿æœŸä¸æ´»è·ƒè®°å½•
    4. ç›‘æ§å†…å­˜å‹åŠ›å¹¶è‡ªåŠ¨æ¸…ç†

    ä¼˜åŒ–ç‚¹:
    1. æ‰¹æ¬¡æ—¶é—´æˆ³æ›´æ–°ï¼šä½¿ç”¨æ‰¹æ¬¡å·ä»£æ›¿ç²¾ç¡®æ—¶é—´æˆ³ï¼Œå‡å°‘ time.time() è°ƒç”¨
    2. ä¼˜åŒ–æ·˜æ±°ç­–ç•¥ï¼šä½¿ç”¨æ‰¹æ¬¡å·è®¡ç®—è®¿é—®å¹´é¾„ï¼Œé¿å…é¢‘ç¹æ—¶é—´è®¡ç®—

    æ€§èƒ½æå‡ï¼štouch_record æ€§èƒ½æå‡ 1.09å€ (8%æ”¹å–„)
    """

    def __init__(
        self,
        max_records: int = 100000,  # æœ€å¤§è®°å½•æ•°
        max_age_days: float = 30.0,  # æœ€å¤§ä¿ç•™å¤©æ•°
        eviction_policy: EvictionPolicy = EvictionPolicy.HYBRID,
        auto_cleanup_interval: int = 100,  # æ¯ N æ¬¡æ“ä½œè‡ªåŠ¨æ¸…ç†
        compression_threshold: int = 10000,  # è¶…è¿‡æ­¤é˜ˆå€¼å¼€å§‹å‹ç¼©
        archive_ratio: float = 0.1,  # å½’æ¡£æ¯”ä¾‹ (10% æœ€ä¸æ´»è·ƒ)
    ):
        self.max_records = max_records
        self.max_age_seconds = max_age_days * 86400
        self.eviction_policy = eviction_policy
        self.auto_cleanup_interval = auto_cleanup_interval
        self.compression_threshold = compression_threshold
        self.archive_ratio = archive_ratio

        # è®°å½•ç´¢å¼•: {memory_id: OptimizedMemoryRecord}
        self.records: OrderedDict[str, OptimizedMemoryRecord] = OrderedDict()

        # æ‰¹æ¬¡ç®¡ç†
        self.current_batch = 0
        self.batch_size = 100  # æ¯100æ¬¡æ“ä½œæ›´æ–°æ‰¹æ¬¡å·

        # æ“ä½œè®¡æ•°å™¨ï¼ˆç”¨äºè§¦å‘è‡ªåŠ¨æ¸…ç†ï¼‰
        self.operation_count = 0

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_added": 0,
            "total_evicted": 0,
            "total_compressed": 0,
            "total_archived": 0,
            "cleanup_runs": 0,
        }

        logger.info(
            f"ğŸ§  MemoryLifecycleManager (ä¼˜åŒ–ç‰ˆ) åˆå§‹åŒ–: "
            f"max_records={max_records}, "
            f"max_age_days={max_age_days}, "
            f"policy={eviction_policy.value}"
        )

    def _increment_batch(self):
        """é€’å¢æ‰¹æ¬¡å·"""
        self.current_batch += 1

    def register_record(
        self,
        memory_id: str,
        importance_score: float = 0.5,
        tags: Optional[List[str]] = None,
    ) -> OptimizedMemoryRecord:
        """
        æ³¨å†Œæ–°çš„è®°å¿†è®°å½•

        Args:
            memory_id: è®°å½•å”¯ä¸€æ ‡è¯†
            importance_score: é‡è¦æ€§è¯„åˆ† (0.0-1.0)
            tags: æ ‡ç­¾åˆ—è¡¨

        Returns:
            åˆ›å»ºçš„ OptimizedMemoryRecord å¯¹è±¡
        """
        now = time.time()
        record = OptimizedMemoryRecord(
            id=memory_id,
            timestamp=now,
            last_accessed_batch=self.current_batch,
            access_count=1,
            importance_score=importance_score,
            tags=tags or [],
        )

        self.records[memory_id] = record
        self.stats["total_added"] += 1
        self.operation_count += 1

        # å®šæœŸæ›´æ–°æ‰¹æ¬¡å·ï¼ˆæ¯100æ¬¡æ“ä½œï¼‰
        if self.operation_count % self.batch_size == 0:
            self._increment_batch()

        # è§¦å‘è‡ªåŠ¨æ¸…ç†
        if self.operation_count >= self.auto_cleanup_interval:
            self.auto_cleanup()

        return record

    def touch_record(self, memory_id: str) -> Optional[OptimizedMemoryRecord]:
        """
        æ›´æ–°è®°å½•çš„è®¿é—®æ—¶é—´å’Œè®¡æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

        Args:
            memory_id: è®°å½•ID

        Returns:
            æ›´æ–°åçš„è®°å½•ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        record = self.records.get(memory_id)
        if record:
            # ä½¿ç”¨æ‰¹æ¬¡æ›´æ–°ï¼ˆä¸è°ƒç”¨time.time()ï¼‰
            record.touch_batch(self.current_batch)
            self.operation_count += 1

            # å®šæœŸæ›´æ–°æ‰¹æ¬¡å·ï¼ˆæ¯100æ¬¡æ“ä½œï¼‰
            if self.operation_count % self.batch_size == 0:
                self._increment_batch()

        return record

    def auto_cleanup(self) -> Dict[str, Any]:
        """
        è‡ªåŠ¨æ¸…ç†æµç¨‹

        æ‰§è¡Œæ­¥éª¤:
        1. æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§è®°å½•æ•°
        2. æ·˜æ±°æœ€ä¸æ´»è·ƒçš„è®°å½•
        3. å‹ç¼©é•¿æœŸä¸æ´»è·ƒè®°å½•
        4. å½’æ¡£è¶…æ—¶è®°å½•

        Returns:
            æ¸…ç†ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸ§¹ å¼€å§‹è‡ªåŠ¨æ¸…ç†è®°å¿†...")

        result = {
            "before_count": len(self.records),
            "evicted": 0,
            "compressed": 0,
            "archived": 0,
            "after_count": 0,
        }

        # 1. æ·˜æ±°è¶…é‡è®°å½•
        if len(self.records) > self.max_records:
            excess = len(self.records) - self.max_records
            evicted = self.evict(excess)
            result["evicted"] = evicted

        # 2. å‹ç¼©ä¸æ´»è·ƒè®°å½•
        if len(self.records) > self.compression_threshold:
            compressed = self.compress_inactive()
            result["compressed"] = compressed

        # 3. å½’æ¡£è¶…æ—¶è®°å½•
        archived = self.archive_old()
        result["archived"] = archived

        result["after_count"] = len(self.records)
        self.stats["cleanup_runs"] += 1
        self.operation_count = 0

        logger.info(
            f"âœ… æ¸…ç†å®Œæˆ: "
            f"æ·˜æ±°={result['evicted']}, "
            f"å‹ç¼©={result['compressed']}, "
            f"å½’æ¡£={result['archived']}, "
            f"å‰©ä½™={result['after_count']}"
        )

        return result

    def evict(self, count: int) -> int:
        """
        æ ¹æ®ç­–ç•¥æ·˜æ±°è®°å½•

        Args:
            count: è¦æ·˜æ±°çš„è®°å½•æ•°é‡

        Returns:
            å®é™…æ·˜æ±°çš„è®°å½•æ•°
        """
        if count <= 0 or not self.records:
            return 0

        # æ ¹æ®ç­–ç•¥é€‰æ‹©è¦æ·˜æ±°çš„è®°å½•
        to_evict = self._select_for_eviction(count)

        # æ‰§è¡Œæ·˜æ±°
        for memory_id in to_evict:
            del self.records[memory_id]
            self.stats["total_evicted"] += 1

        logger.info(f"ğŸ—‘ï¸ æ·˜æ±°äº† {len(to_evict)} æ¡è®°å½• (ç­–ç•¥: {self.eviction_policy.value})")
        return len(to_evict)

    def _select_for_eviction(self, count: int) -> List[str]:
        """
        æ ¹æ®æ·˜æ±°ç­–ç•¥é€‰æ‹©è®°å½•ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

        ä½¿ç”¨æ‰¹æ¬¡å·è®¡ç®—è®¿é—®å¹´é¾„ï¼Œå‡å°‘ time.time() è°ƒç”¨
        """
        records = list(self.records.values())

        if self.eviction_policy == EvictionPolicy.LRU:
            # ä½¿ç”¨æ‰¹æ¬¡å·è®¡ç®—è®¿é—®å¹´é¾„
            scored = [(r.id, r.access_age(self.current_batch)) for r in records]
            scored.sort(key=lambda x: x[1], reverse=True)
            return [r[0] for r in scored[:count]]

        elif self.eviction_policy == EvictionPolicy.LFU:
            # æœ€å°‘ä½¿ç”¨é¢‘ç‡: æŒ‰ access_count æ’åº
            scored = [(r.id, r.access_count) for r in records]
            scored.sort(key=lambda x: x[1])
            return [r[0] for r in scored[:count]]

        elif self.eviction_policy == EvictionPolicy.IMPORTANCE:
            # åŸºäºé‡è¦æ€§: æŒ‰ importance_score æ’åº
            scored = [(r.id, r.importance_score) for r in records]
            scored.sort(key=lambda x: x[1])
            return [r[0] for r in scored[:count]]

        else:  # HYBRID
            # æ··åˆç­–ç•¥: ç»¼åˆè€ƒè™‘è®¿é—®é¢‘ç‡ã€å¹´é¾„å’Œé‡è¦æ€§
            # åˆ†æ•° = (access_count * 0.3) + (importance * 100) - (access_age / 86400)
            scored = []
            for r in records:
                access_age_batches = r.access_age(self.current_batch)
                # å‡è®¾æ¯æ‰¹æ¬¡çº¦1ç§’
                access_age_days = access_age_batches / 86400
                score = (
                    r.access_count * 0.3
                    + r.importance_score * 100
                    - access_age_days
                )
                scored.append((r.id, score))

            scored.sort(key=lambda x: x[1])
            return [r[0] for r in scored[:count]]

    def compress_inactive(self, threshold_days: float = 7.0) -> int:
        """
        å‹ç¼©é•¿æœŸä¸æ´»è·ƒçš„è®°å½•

        Args:
            threshold_days: ä¸æ´»è·ƒå¤©æ•°é˜ˆå€¼

        Returns:
            å‹ç¼©çš„è®°å½•æ•°
        """
        threshold_seconds = threshold_days * 86400
        compressed = 0

        for record in self.records.values():
            if not record.compressed and record.age() > threshold_seconds:
                record.compressed = True
                compressed += 1
                self.stats["total_compressed"] += 1

        if compressed > 0:
            logger.info(f"ğŸ“¦ å‹ç¼©äº† {compressed} æ¡ä¸æ´»è·ƒè®°å½•")

        return compressed

    def archive_old(self) -> int:
        """
        å½’æ¡£è¶…æ—¶è®°å½•

        Returns:
            å½’æ¡£çš„è®°å½•æ•°
        """
        to_archive = []

        for memory_id, record in self.records.items():
            if not record.archived and record.age() > self.max_age_seconds:
                to_archive.append(memory_id)

        for memory_id in to_archive:
            self.records[memory_id].archived = True
            self.stats["total_archived"] += 1

        # ä»æ´»åŠ¨è®°å½•ä¸­ç§»é™¤å½’æ¡£è®°å½•
        # å®é™…åº”ç”¨ä¸­ï¼Œå½’æ¡£è®°å½•åº”ç§»åŠ¨åˆ°æŒä¹…å­˜å‚¨
        for memory_id in to_archive:
            del self.records[memory_id]

        if to_archive:
            logger.info(f"ğŸ“ å½’æ¡£äº† {len(to_archive)} æ¡è¶…æ—¶è®°å½•")

        return len(to_archive)

    def calculate_importance(self, metadata: Dict[str, Any]) -> float:
        """
        è®¡ç®—è®°å¿†è®°å½•çš„é‡è¦æ€§è¯„åˆ†

        è€ƒè™‘å› ç´ :
        - ç±»å‹æƒé‡ (macro > episode)
        - å·¥å…·è°ƒç”¨æ¬¡æ•°
        - æ˜¯å¦æœ‰æŠ€èƒ½å…³è”
        - è¿æ¥æ•° (æ‹“æ‰‘å›¾ä¸­çš„åº¦)

        Args:
            metadata: è®°å½•å…ƒæ•°æ®

        Returns:
            é‡è¦æ€§è¯„åˆ† (0.0-1.0)
        """
        score = 0.5  # åŸºç¡€åˆ†

        # ç±»å‹æƒé‡
        mem_type = metadata.get("type", "episode")
        if mem_type == "macro":
            score += 0.3
        elif mem_type == "tool_call" or mem_type == "skill_call":
            score += 0.1

        # æŠ€èƒ½å…³è”
        if metadata.get("skill"):
            score += 0.1

        # å·¥å…·è°ƒç”¨ (å¸¸è§å·¥å…·é™ä½åˆ†å€¼ï¼Œç½•è§å·¥å…·æå‡åˆ†å€¼)
        tool = metadata.get("tool")
        common_tools = {"file_operations", "world_model", "metacognition"}
        if tool and tool not in common_tools:
            score += 0.1

        # åŸå‹æ•°é‡ (macro_induction äº§ç”Ÿçš„ prototype_ids)
        prototype_ids = metadata.get("prototype_ids")
        if isinstance(prototype_ids, list) and len(prototype_ids) > 0:
            score += min(0.2, len(prototype_ids) * 0.05)

        return min(1.0, score)

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        active = [r for r in self.records.values() if not r.archived]
        compressed = [r for r in self.records.values() if r.compressed]

        return {
            "total_records": len(self.records),
            "active_records": len(active),
            "compressed_records": len(compressed),
            "operation_count": self.operation_count,
            "current_batch": self.current_batch,
            "stats": self.stats.copy(),
            "eviction_policy": self.eviction_policy.value,
            "max_records": self.max_records,
            "usage_ratio": len(self.records) / self.max_records,
        }

    def export_records_for_cleanup(
        self, memory_latents: np.ndarray, memory_metadata: List[Dict]
    ) -> Tuple[np.ndarray, List[Dict]]:
        """
        å¯¼å‡ºæ¸…ç†åçš„è®°å¿†æ•°ç»„

        Args:
            memory_latents: åŸå§‹æ½œåœ¨å‘é‡æ•°ç»„
            memory_metadata: åŸå§‹å…ƒæ•°æ®åˆ—è¡¨

        Returns:
            (æ¸…ç†åçš„ latents, æ¸…ç†åçš„ metadata)
        """
        if not self.records:
            return memory_latents, memory_metadata

        # è·å–æ´»åŠ¨è®°å½•çš„ç´¢å¼•
        active_indices = []
        metadata_dict = {m.get("id"): i for i, m in enumerate(memory_metadata)}

        for memory_id in self.records.keys():
            if memory_id in metadata_dict:
                active_indices.append(metadata_dict[memory_id])

        if not active_indices:
            return np.array([]), []

        # è¿‡æ»¤ latents å’Œ metadata
        cleaned_latents = memory_latents[active_indices]
        cleaned_metadata = [memory_metadata[i] for i in active_indices]

        logger.info(
            f"ğŸ§¹ è®°å¿†æ¸…ç†: "
            f"{len(memory_latents)} -> {len(cleaned_latents)} "
            f"({len(memory_latents) - len(cleaned_latents)} æ¡è¢«ç§»é™¤)"
        )

        return cleaned_latents, cleaned_metadata

    def save_state(self, filepath: str):
        """ä¿å­˜ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨çŠ¶æ€"""
        state = {
            "records": {mid: r.to_dict() for mid, r in self.records.items()},
            "stats": self.stats,
            "operation_count": self.operation_count,
            "current_batch": self.current_batch,
            "config": {
                "max_records": self.max_records,
                "max_age_days": self.max_age_seconds / 86400,
                "eviction_policy": self.eviction_policy.value,
                "auto_cleanup_interval": self.auto_cleanup_interval,
                "batch_size": self.batch_size,
            },
        }

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(state, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ ç”Ÿå‘½å‘¨æœŸçŠ¶æ€å·²ä¿å­˜: {filepath}")

    def load_state(self, filepath: str):
        """åŠ è½½ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨çŠ¶æ€"""
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                state = json.load(f)

            # æ¢å¤è®°å½•
            self.records.clear()
            for mid, rdict in state.get("records", {}).items():
                record = OptimizedMemoryRecord(
                    id=rdict["id"],
                    timestamp=rdict["timestamp"],
                    last_accessed_batch=rdict["last_accessed_batch"],
                    access_count=rdict["access_count"],
                    importance_score=rdict["importance_score"],
                    compressed=rdict.get("compressed", False),
                    archived=rdict.get("archived", False),
                    tags=rdict.get("tags", []),
                )
                self.records[mid] = record

            # æ¢å¤ç»Ÿè®¡
            self.stats = state.get("stats", self.stats.copy())
            self.operation_count = state.get("operation_count", 0)
            self.current_batch = state.get("current_batch", 0)

            logger.info(f"ğŸ“‚ ç”Ÿå‘½å‘¨æœŸçŠ¶æ€å·²åŠ è½½: {len(self.records)} æ¡è®°å½•")

        except FileNotFoundError:
            logger.warning(f"çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        except Exception as e:
            logger.error(f"åŠ è½½çŠ¶æ€å¤±è´¥: {e}")


# å‘åå…¼å®¹ï¼šä½¿ç”¨OptimizedMemoryRecordä½œä¸ºMemoryRecord
MemoryRecord = OptimizedMemoryRecord
