#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æœ¬åœ°æ–‡æ¡£è¯»å–å™¨
Local Document Reader for AGI

å®‰å…¨åœ°è¯»å–æœ¬åœ°é¡¹ç›®åŒºæ–‡æ¡£ï¼Œæ”¯æŒå¤šç§æ ¼å¼
"""

import os
import re
from pathlib import Path
from typing import List, Dict, Optional, Set
import logging
from datetime import datetime

logger = logging.getLogger(__name__)


class LocalDocumentReader:
    """
    æœ¬åœ°æ–‡æ¡£è¯»å–å™¨

    å®‰å…¨ç‰¹æ€§:
    1. è·¯å¾„ç™½åå• - åªå…è®¸è¯»å–é¡¹ç›®ç›®å½•
    2. æ–‡ä»¶ç±»å‹é™åˆ¶ - åªå…è®¸å®‰å…¨æ ¼å¼
    3. å¤§å°é™åˆ¶ - é˜²æ­¢è¯»å–è¿‡å¤§æ–‡ä»¶
    4. æ•æ„Ÿæ–‡ä»¶è¿‡æ»¤ - æ’é™¤å¯†é’¥ã€é…ç½®ç­‰
    """

    # å…è®¸è¯»å–çš„æ–‡ä»¶ç±»å‹
    ALLOWED_EXTENSIONS = {
        '.md', '.txt', '.rst', '.py', '.js', '.ts', '.json',
        '.yaml', '.yml', '.xml', '.html', '.css'
    }

    # æ’é™¤çš„æ•æ„Ÿæ–‡ä»¶æ¨¡å¼
    SENSITIVE_PATTERNS = [
        r'\.env$',
        r'\.key$',
        r'\.pem$',
        r'password',
        r'secret',
        r'credential',
        r'token',
        r'__pycache__',
        r'\.git',
        r'node_modules',
        r'\.venv'
    ]

    # æ–‡ä»¶å¤§å°é™åˆ¶ (MB)
    # ğŸ†• [2026-01-22] æé«˜é™åˆ¶ä»¥æ”¯æŒå¤§å‹æ–‡ä»¶ï¼ˆå¦‚ vector_memory.jsonï¼‰
    MAX_FILE_SIZE_MB = 1000  # è®¾ç½®ä¸º1GBï¼Œæ”¯æŒè¶…å¤§å‹æ•°æ®æ–‡ä»¶

    def __init__(self, project_root: str = None):
        """
        åˆå§‹åŒ–æ–‡æ¡£è¯»å–å™¨

        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        """
        if project_root is None:
            project_root = Path(__file__).parent.parent

        self.project_root = Path(project_root).resolve()
        self.read_history = []

        logger.info(f"ğŸ“– æœ¬åœ°æ–‡æ¡£è¯»å–å™¨å·²åˆå§‹åŒ–")
        logger.info(f"   é¡¹ç›®æ ¹ç›®å½•: {self.project_root}")
        logger.info(f"   å…è®¸çš„æ–‡ä»¶ç±»å‹: {len(self.ALLOWED_EXTENSIONS)} ç§")
        logger.info(f"   å®‰å…¨é™åˆ¶: {self.MAX_FILE_SIZE_MB}MB")

    def is_safe_path(self, file_path: Path) -> bool:
        """æ£€æŸ¥è·¯å¾„æ˜¯å¦å®‰å…¨ï¼ˆåœ¨é¡¹ç›®ç›®å½•å†…ï¼‰"""
        try:
            resolved_path = file_path.resolve()
            # æ£€æŸ¥æ˜¯å¦åœ¨é¡¹ç›®æ ¹ç›®å½•å†…
            resolved_path.relative_to(self.project_root)
            return True
        except ValueError:
            return False

    def is_safe_file(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å®‰å…¨è¯»å–"""
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åï¼ˆå…è®¸æ— æ‰©å±•åçš„å¯æ‰§è¡Œæ–‡ä»¶ï¼‰
        if file_path.suffix:
            if file_path.suffix.lower() not in self.ALLOWED_EXTENSIONS:
                return False
        
        # æ£€æŸ¥æ•æ„Ÿæ–‡ä»¶æ¨¡å¼
        for pattern in self.SENSITIVE_PATTERNS:
            if re.search(pattern, str(file_path), re.IGNORECASE):
                return False

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        try:
            size_mb = file_path.stat().st_size / (1024 * 1024)
            if size_mb > self.MAX_FILE_SIZE_MB:
                logger.warning(f"æ–‡ä»¶è¿‡å¤§: {file_path.name} ({size_mb:.1f}MB)")
                return False
        except Exception as e:
            logger.warning(f"æ— æ³•è·å–æ–‡ä»¶å¤§å°: {e}")
            return False

        return True

    def read_file(self, file_path: str) -> Dict[str, any]:
        """
        è¯»å–å•ä¸ªæ–‡ä»¶

        Args:
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹æˆ–ç»å¯¹ï¼‰

        Returns:
            åŒ…å«æ–‡ä»¶å†…å®¹å’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        try:
            # è§£æè·¯å¾„
            path = Path(file_path)

            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºé¡¹ç›®æ ¹ç›®å½•
            if not path.is_absolute():
                path = self.project_root / path

            # å®‰å…¨æ£€æŸ¥
            if not self.is_safe_path(path):
                return {
                    'success': False,
                    'error': 'è·¯å¾„è¶…å‡ºé¡¹ç›®ç›®å½•èŒƒå›´',
                    'path': str(path)
                }

            if not path.exists():
                return {
                    'success': False,
                    'error': 'æ–‡ä»¶ä¸å­˜åœ¨',
                    'path': str(path)
                }

            if not self.is_safe_file(path):
                return {
                    'success': False,
                    'error': 'æ–‡ä»¶ç±»å‹ä¸å®‰å…¨æˆ–åŒ…å«æ•æ„Ÿä¿¡æ¯',
                    'path': str(path)
                }

            # è¯»å–æ–‡ä»¶
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()

            # è®°å½•è¯»å–å†å²
            self.read_history.append({
                'path': str(path),
                'timestamp': datetime.now().isoformat(),
                'size': len(content)
            })

            return {
                'success': True,
                'path': str(path),
                'relative_path': str(path.relative_to(self.project_root)),
                'content': content,
                'size': len(content),
                'lines': len(content.split('\n')),
                'extension': path.suffix,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'path': file_path
            }

    def list_documents(self, directory: str = ".", pattern: str = "*", recursive: bool = True, max_results: int = 100) -> List[Dict]:
        """
        åˆ—å‡ºç›®å½•ä¸­çš„æ–‡æ¡£

        Args:
            directory: ç›®å½•è·¯å¾„
            pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            recursive: æ˜¯å¦é€’å½’æŸ¥æ‰¾
            max_results: ğŸ†• [2026-01-24] æœ€å¤§è¿”å›æ•°é‡ï¼Œé˜²æ­¢å…¨é‡æ‰«æï¼ˆé»˜è®¤100ï¼‰

        Returns:
            æ–‡æ¡£ä¿¡æ¯åˆ—è¡¨
        """
        try:
            dir_path = Path(directory)

            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºé¡¹ç›®æ ¹ç›®å½•
            if not dir_path.is_absolute():
                dir_path = self.project_root / dir_path

            # å®‰å…¨æ£€æŸ¥
            if not self.is_safe_path(dir_path):
                return []

            if not dir_path.exists() or not dir_path.is_dir():
                return []

            documents = []

            # ğŸ†• [2026-01-24] æ’é™¤å¤§å‹ç›®å½•ï¼Œé¿å…å…¨é‡æ‰«æ
            EXCLUDED_DIRS = {
                '.git', '__pycache__', 'node_modules', '.venv', 'venv',
                'backups', 'backbag', '.backups', 'pyvista-0.46.4',
                'data', 'logs', 'memory_db', '.mypy_cache', '.pytest_cache',
                'workspace'  # ğŸ†• [2026-01-28] æ’é™¤ workspace ç›®å½•ï¼Œé¿å…æ‰«ææ—§æµ‹è¯•è®°å½•
            }

            # é€’å½’æˆ–éé€’å½’éå†
            if recursive:
                files = dir_path.rglob(pattern)
            else:
                files = dir_path.glob(pattern)

            scanned_count = 0
            for file_path in files:
                try:
                    # ğŸ†• [2026-01-24] æ—©æœŸç»ˆæ­¢ï¼šè¾¾åˆ°æœ€å¤§æ•°é‡ååœæ­¢æ‰«æ
                    if len(documents) >= max_results:
                        logger.info(f"ğŸ“‚ å·²è¾¾æœ€å¤§è¿”å›æ•°é‡ {max_results}ï¼Œåœæ­¢æ‰«æï¼ˆå·²æ‰«æ {scanned_count} ä¸ªæ–‡ä»¶ï¼‰")
                        break
                    
                    scanned_count += 1
                    
                    # è·³è¿‡ç¬¦å·é“¾æ¥ï¼Œé¿å…æƒé™é—®é¢˜
                    if file_path.is_symlink():
                        continue
                    
                    # ğŸ†• [2026-01-24] è·³è¿‡æ’é™¤ç›®å½•ä¸­çš„æ–‡ä»¶
                    if any(excluded in file_path.parts for excluded in EXCLUDED_DIRS):
                        continue
                    
                    if file_path.is_file() and self.is_safe_file(file_path):
                        stat = file_path.stat()
                        documents.append({
                            'path': str(file_path),
                            'relative_path': str(file_path.relative_to(self.project_root)),
                            'name': file_path.name,
                            'extension': file_path.suffix,
                            'size': stat.st_size,
                            'modified': datetime.fromtimestamp(stat.st_mtime).isoformat()
                        })
                except (OSError, PermissionError) as e:
                    # è·³è¿‡æ— æ³•è®¿é—®çš„æ–‡ä»¶ï¼ˆå¦‚ç¬¦å·é“¾æ¥ï¼‰
                    logger.debug(f"è·³è¿‡æ— æ³•è®¿é—®çš„æ–‡ä»¶: {file_path.name} ({e})")
                    continue

            logger.info(f"ğŸ“‚ è¿”å› {len(documents)} ä¸ªæ–‡æ¡£ï¼ˆæ‰«æäº† {scanned_count} ä¸ªæ–‡ä»¶ï¼‰")
            return documents

        except Exception as e:
            logger.error(f"åˆ—å‡ºæ–‡æ¡£å¤±è´¥: {e}")
            return []

    def search_in_documents(self, query: str, directory: str = ".", max_results: int = 20) -> List[Dict]:
        """
        åœ¨æ–‡æ¡£ä¸­æœç´¢å…³é”®è¯

        Args:
            query: æœç´¢å…³é”®è¯
            directory: æœç´¢ç›®å½•
            max_results: æœ€å¤§ç»“æœæ•°

        Returns:
            åŒ¹é…ç»“æœåˆ—è¡¨
        """
        try:
            documents = self.list_documents(directory, recursive=True)
            results = []

            for doc in documents[:max_results * 2]:  # å¤šæ£€æŸ¥ä¸€äº›æ–‡ä»¶
                result = self.read_file(doc['relative_path'])

                if result['success']:
                    content = result['content']
                    lines = content.split('\n')

                    # æœç´¢åŒ¹é…è¡Œ
                    matches = []
                    for i, line in enumerate(lines):
                        if query.lower() in line.lower():
                            matches.append({
                                'line_number': i + 1,
                                'content': line.strip(),
                                'preview': line.strip()[:100]
                            })

                    if matches:
                        results.append({
                            'file': doc['relative_path'],
                            'matches': matches[:5],  # æœ€å¤šæ˜¾ç¤º5ä¸ªåŒ¹é…
                            'total_matches': len(matches)
                        })

                if len(results) >= max_results:
                    break

            logger.info(f"ğŸ” æœç´¢ '{query}' åœ¨ {len(results)} ä¸ªæ–‡ä»¶ä¸­æ‰¾åˆ°åŒ¹é…")
            return results

        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []

    def get_document_summary(self, file_path: str) -> Dict:
        """
        è·å–æ–‡æ¡£æ‘˜è¦

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            æ–‡æ¡£æ‘˜è¦ä¿¡æ¯
        """
        result = self.read_file(file_path)

        if not result['success']:
            return result

        content = result['content']
        lines = content.split('\n')

        # æå–æ ‡é¢˜ï¼ˆMarkdownï¼‰
        titles = []
        for line in lines:
            if line.startswith('#'):
                titles.append(line.strip())

        # ç»Ÿè®¡ä¿¡æ¯
        summary = {
            'success': True,
            'path': result['path'],
            'relative_path': result['relative_path'],
            'titles': titles[:10],  # æœ€å¤š10ä¸ªæ ‡é¢˜
            'total_lines': result['lines'],
            'total_chars': result['size'],
            'extension': result['extension'],
            'encoding': 'utf-8',
            'preview': content[:500]  # å‰500å­—ç¬¦é¢„è§ˆ
        }

        return summary

    def index_project_docs(self, exclude_dirs: List[str] = None, force_rebuild: bool = False) -> Dict:
        """
        ğŸ†• [2026-01-24] å¸¦æŒä¹…åŒ–ç¼“å­˜çš„é¡¹ç›®æ–‡æ¡£ç´¢å¼•

        å·¥ä½œæµç¨‹ï¼š
        1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²ä¿å­˜çš„ç´¢å¼•æ–‡ä»¶
        2. å¦‚æœå­˜åœ¨ä¸”æœªè¿‡æœŸï¼ˆ24å°æ—¶å†…ï¼‰ï¼Œç›´æ¥è¿”å›ç¼“å­˜
        3. å¦‚æœä¸å­˜åœ¨æˆ–è¿‡æœŸï¼Œæ‰§è¡Œå…¨é‡ç´¢å¼•å¹¶ä¿å­˜

        Args:
            exclude_dirs: æ’é™¤çš„ç›®å½•åˆ—è¡¨
            force_rebuild: å¼ºåˆ¶é‡å»ºç´¢å¼•ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰

        Returns:
            ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        """
        import json
        
        # ç´¢å¼•æ–‡ä»¶è·¯å¾„
        index_file = self.project_root / "data" / "document_index.json"
        index_file.parent.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆ24å°æ—¶å†…ï¼‰
        cache_valid = False
        cached_index = None
        
        if not force_rebuild and index_file.exists():
            try:
                with open(index_file, 'r', encoding='utf-8') as f:
                    cached_index = json.load(f)
                
                # æ£€æŸ¥æ—¶é—´æˆ³
                cached_time = datetime.fromisoformat(cached_index.get('timestamp', '2000-01-01'))
                age_hours = (datetime.now() - cached_time).total_seconds() / 3600
                
                if age_hours < 24:
                    cache_valid = True
                    logger.info(f"ğŸ“‚ ä½¿ç”¨ç¼“å­˜ç´¢å¼•ï¼ˆ{age_hours:.1f}å°æ—¶å‰åˆ›å»ºï¼ŒåŒ…å« {cached_index.get('total_documents', 0)} ä¸ªæ–‡æ¡£ï¼‰")
                else:
                    logger.info(f"ğŸ“‚ ç´¢å¼•ç¼“å­˜å·²è¿‡æœŸï¼ˆ{age_hours:.1f}å°æ—¶ï¼‰ï¼Œé‡æ–°æ„å»º...")
                    
            except Exception as e:
                logger.warning(f"è¯»å–ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}ï¼Œé‡æ–°æ„å»º...")
        
        # å¦‚æœç¼“å­˜æœ‰æ•ˆï¼Œç›´æ¥è¿”å›
        if cache_valid and cached_index:
            return cached_index
        
        # æ‰§è¡Œå…¨é‡ç´¢å¼•
        logger.info("ğŸ“‚ å¼€å§‹å…¨é‡æ–‡æ¡£ç´¢å¼•...")
        
        try:
            if exclude_dirs is None:
                exclude_dirs = ['.git', '__pycache__', 'node_modules', '.venv', '.conda', 'venv', 
                               'backups', 'backbag', '.backups', 'pyvista-0.46.4', '.mypy_cache',
                               'workspace']  # ğŸ†• [2026-01-28] æ’é™¤ workspace ç›®å½•ï¼Œé¿å…æ‰«ææ—§æµ‹è¯•è®°å½•

            # å…¨é‡æ‰«æï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
            all_docs = self._full_scan_documents(".", exclude_dirs)

            # æŒ‰ç±»å‹åˆ†ç»„
            by_extension = {}
            for doc in all_docs:
                ext = doc['extension']
                if ext not in by_extension:
                    by_extension[ext] = []
                by_extension[ext].append(doc)

            # è¯»å–é‡è¦æ–‡æ¡£çš„æ‘˜è¦
            important_docs = [doc for doc in all_docs
                            if doc['extension'] in ['.md', '.txt', '.rst']
                            and doc['size'] < 1024 * 1024 * 50]  # å°äº50MB

            summaries = []
            for doc in important_docs[:50]:  # æœ€å¤šç´¢å¼•50ä¸ªé‡è¦æ–‡æ¡£
                try:
                    summary = self.get_document_summary(doc['relative_path'])
                    if summary['success']:
                        summaries.append({
                            'path': summary['relative_path'],
                            'titles': summary.get('titles', []),
                            'lines': summary.get('total_lines', 0),
                            'chars': summary.get('total_chars', 0)
                        })
                except Exception:
                    continue

            index_data = {
                'total_documents': len(all_docs),
                'by_extension': {k: len(v) for k, v in by_extension.items()},
                'indexed_summaries': len(summaries),
                'summaries': summaries,
                'documents': all_docs,  # å®Œæ•´æ–‡æ¡£åˆ—è¡¨
                'timestamp': datetime.now().isoformat(),
                'exclude_dirs': exclude_dirs
            }
            
            # ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶
            try:
                with open(index_file, 'w', encoding='utf-8') as f:
                    json.dump(index_data, f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… ç´¢å¼•å·²ä¿å­˜åˆ° {index_file}ï¼ˆ{len(all_docs)} ä¸ªæ–‡æ¡£ï¼‰")
            except Exception as e:
                logger.warning(f"ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")

            return index_data

        except Exception as e:
            logger.error(f"ç´¢å¼•å¤±è´¥: {e}")
            return {
                'total_documents': 0,
                'error': str(e)
            }

    def _full_scan_documents(self, directory: str, exclude_dirs: List[str]) -> List[Dict]:
        """
        ğŸ†• [2026-01-24] å…¨é‡æ‰«ææ–‡æ¡£ï¼ˆä»…ä¾›ç´¢å¼•ä½¿ç”¨ï¼‰
        
        ä¸ list_documents ä¸åŒï¼Œè¿™ä¸ªæ–¹æ³•ä¼šæ‰«ææ‰€æœ‰æ–‡æ¡£ç”¨äºæ„å»ºç´¢å¼•
        """
        try:
            dir_path = Path(directory)
            if not dir_path.is_absolute():
                dir_path = self.project_root / dir_path

            if not self.is_safe_path(dir_path):
                return []

            if not dir_path.exists() or not dir_path.is_dir():
                return []

            documents = []
            exclude_set = set(exclude_dirs)

            for file_path in dir_path.rglob('*'):
                try:
                    if file_path.is_symlink():
                        continue
                    
                    # è·³è¿‡æ’é™¤ç›®å½•
                    if any(excluded in file_path.parts for excluded in exclude_set):
                        continue
                    
                    if file_path.is_file() and self.is_safe_file(file_path):
                        stat = file_path.stat()
                        documents.append({
                            'path': str(file_path),
                            'relative_path': str(file_path.relative_to(self.project_root)),
                            'name': file_path.name,
                            'extension': file_path.suffix,
                            'size': stat.st_size,
                            'modified': datetime.fromtimestamp(stat.st_mtime).isoformat()
                        })
                except (OSError, PermissionError):
                    continue

            logger.info(f"ğŸ“‚ å…¨é‡æ‰«æå®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£")
            return documents

        except Exception as e:
            logger.error(f"å…¨é‡æ‰«æå¤±è´¥: {e}")
            return []

    def get_cached_index(self) -> Optional[Dict]:
        """
        ğŸ†• [2026-01-24] è·å–ç¼“å­˜çš„ç´¢å¼•ï¼ˆä¸è§¦å‘é‡å»ºï¼‰
        
        ç”¨äºå¿«é€ŸæŸ¥è¯¢å·²çŸ¥æ–‡æ¡£ï¼Œä¸æ‰§è¡Œæ‰«æ
        """
        import json
        index_file = self.project_root / "data" / "document_index.json"
        
        if not index_file.exists():
            return None
            
        try:
            with open(index_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return None

    def search_in_index(self, query: str, max_results: int = 20) -> List[Dict]:
        """
        ğŸ†• [2026-01-24] åœ¨å·²ç¼“å­˜çš„ç´¢å¼•ä¸­æœç´¢ï¼ˆå¿«é€Ÿï¼‰
        
        ä¸è¯»å–æ–‡ä»¶å†…å®¹ï¼Œåªæœç´¢æ–‡ä»¶åå’Œè·¯å¾„
        """
        cached = self.get_cached_index()
        if not cached:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°æ–‡æ¡£ç´¢å¼•ï¼Œè¯·å…ˆæ‰§è¡Œ index æ“ä½œ")
            return []
        
        query_lower = query.lower()
        results = []
        
        for doc in cached.get('documents', []):
            # æœç´¢æ–‡ä»¶åå’Œè·¯å¾„
            if query_lower in doc['name'].lower() or query_lower in doc['relative_path'].lower():
                results.append(doc)
                if len(results) >= max_results:
                    break
        
        logger.info(f"ğŸ” åœ¨ç´¢å¼•ä¸­æœç´¢ '{query}': æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…")
        return results

    def get_statistics(self) -> Dict:
        """è·å–è¯»å–å™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'project_root': str(self.project_root),
            'total_read': len(self.read_history),
            'allowed_extensions': list(self.ALLOWED_EXTENSIONS),
            'max_size_mb': self.MAX_FILE_SIZE_MB,
            'recent_reads': self.read_history[-10:]  # æœ€è¿‘10æ¬¡è¯»å–
        }

    def write_file(self, file_path: str, content: str, **kwargs) -> Dict:
        """
        å†™å…¥æ–‡ä»¶ï¼ˆä»£ç†åˆ° LocalDocumentWriterï¼‰

        æ­¤æ–¹æ³•æä¾›å‘åå…¼å®¹æ€§ï¼Œå°†å†™å…¥è¯·æ±‚è½¬å‘åˆ° LocalDocumentWriter

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            content: æ–‡ä»¶å†…å®¹
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆcreate_dirs, backup, encodingç­‰ï¼‰

        Returns:
            æ“ä½œç»“æœå­—å…¸
        """
        try:
            from core.local_document_writer import get_document_writer
            writer = get_document_writer(str(self.project_root))
            return writer.write_file(file_path, content, **kwargs)
        except ImportError as e:
            logger.error(f"æ— æ³•å¯¼å…¥ LocalDocumentWriter: {e}")
            return {
                'success': False,
                'error': f'LocalDocumentWriter ä¸å¯ç”¨: {e}'
            }

    def append_file(self, file_path: str, content: str, **kwargs) -> Dict:
        """
        è¿½åŠ å†…å®¹åˆ°æ–‡ä»¶ï¼ˆä»£ç†åˆ° LocalDocumentWriterï¼‰

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            content: è¦è¿½åŠ çš„å†…å®¹
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            æ“ä½œç»“æœå­—å…¸
        """
        try:
            from core.local_document_writer import get_document_writer
            writer = get_document_writer(str(self.project_root))
            return writer.append_file(file_path, content, **kwargs)
        except ImportError as e:
            logger.error(f"æ— æ³•å¯¼å…¥ LocalDocumentWriter: {e}")
            return {
                'success': False,
                'error': f'LocalDocumentWriter ä¸å¯ç”¨: {e}'
            }

    def edit_file(self, file_path: str, old_content: str, new_content: str, **kwargs) -> Dict:
        """
        ç¼–è¾‘æ–‡ä»¶ï¼ˆä»£ç†åˆ° LocalDocumentWriterï¼‰

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            old_content: è¦æ›¿æ¢çš„æ—§å†…å®¹
            new_content: æ–°å†…å®¹
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            æ“ä½œç»“æœå­—å…¸
        """
        try:
            from core.local_document_writer import get_document_writer
            writer = get_document_writer(str(self.project_root))
            return writer.edit_file(file_path, old_content, new_content, **kwargs)
        except ImportError as e:
            logger.error(f"æ— æ³•å¯¼å…¥ LocalDocumentWriter: {e}")
            return {
                'success': False,
                'error': f'LocalDocumentWriter ä¸å¯ç”¨: {e}'
            }


# ==================== å•ä¾‹å®ä¾‹ ====================

_reader_instance = None

def get_document_reader(project_root: str = None) -> LocalDocumentReader:
    """è·å–æ–‡æ¡£è¯»å–å™¨å•ä¾‹"""
    global _reader_instance
    if _reader_instance is None:
        _reader_instance = LocalDocumentReader(project_root)
    return _reader_instance


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == "__main__":
    import asyncio

    async def test_document_reader():
        """æµ‹è¯•æ–‡æ¡£è¯»å–å™¨"""
        print("=" * 80)
        print("æœ¬åœ°æ–‡æ¡£è¯»å–å™¨æµ‹è¯•")
        print("=" * 80)

        reader = get_document_reader()

        # 1. ç´¢å¼•é¡¹ç›®æ–‡æ¡£
        print("\n[1] ç´¢å¼•é¡¹ç›®æ–‡æ¡£")
        index = reader.index_project_docs()
        print(f"âœ… æ€»æ–‡æ¡£æ•°: {index.get('total_documents', 0)}")
        print(f"âœ… å·²ç´¢å¼•æ‘˜è¦: {index.get('indexed_summaries', 0)}")

        if 'by_extension' in index:
            print("\næ–‡ä»¶ç±»å‹åˆ†å¸ƒ:")
            for ext, count in sorted(index['by_extension'].items()):
                print(f"  {ext}: {count} ä¸ªæ–‡ä»¶")

        # 2. è¯»å–ç¤ºä¾‹æ–‡ä»¶
        print("\n[2] è¯»å–READMEæ–‡ä»¶")
        readme_result = reader.read_file("README.md")
        if readme_result['success']:
            print(f"âœ… æ–‡ä»¶: {readme_result['relative_path']}")
            print(f"   å¤§å°: {readme_result['size']} å­—ç¬¦")
            print(f"   è¡Œæ•°: {readme_result['lines']}")
            print(f"   é¢„è§ˆ: {readme_result['content'][:200]}...")
        else:
            print(f"âŒ {readme_result['error']}")

        # 3. æœç´¢æ–‡æ¡£
        print("\n[3] æœç´¢åŒ…å«'LLM'çš„æ–‡æ¡£")
        search_results = reader.search_in_documents("LLM", max_results=5)
        print(f"âœ… æ‰¾åˆ° {len(search_results)} ä¸ªåŒ¹é…æ–‡ä»¶")
        for result in search_results:
            print(f"  - {result['file']}: {result['total_matches']} å¤„åŒ¹é…")

        # 4. ç»Ÿè®¡ä¿¡æ¯
        print("\n[4] è¯»å–å™¨ç»Ÿè®¡")
        stats = reader.get_statistics()
        print(f"âœ… é¡¹ç›®æ ¹ç›®å½•: {stats['project_root']}")
        print(f"âœ… æ€»è¯»å–æ¬¡æ•°: {stats['total_read']}")

        print("\n" + "=" * 80)
        print("æµ‹è¯•å®Œæˆ")
        print("=" * 80)

    asyncio.run(test_document_reader())
