import time
import json
import logging
from typing import List, Dict, Any, Optional, Deque
from collections import deque
from core.llm_client import LLMService

# ğŸ†• å¯¼å…¥æ¨¡å¼åŒ¹é…å™¨å’Œå†³ç­–ç¼“å­˜
try:
    from core.pattern_matcher import PatternMatcher, get_pattern_matcher
    from core.decision_cache import DecisionCache, get_decision_cache
except ImportError:
    PatternMatcher = None
    DecisionCache = None

# Configure logging
logger = logging.getLogger("IntentTracker")

class IntentTracker:
    """
    Tracks user actions and infers high-level intent using LLM analysis.
    Acts as the 'Subconscious Inference Engine' for the AGI.
    """
    def __init__(self, history_size: int = 20):
        if history_size <= 0:
            raise ValueError("history_size must be a positive integer")

        self.action_history: Deque[Dict[str, Any]] = deque(maxlen=history_size)
        self.llm: LLMService = LLMService()
        self.current_hypothesis: Optional[Dict[str, Any]] = None
        self.last_inference_time: float = 0.0
        self.inference_interval: float = 30.0  # Analyze every 30 seconds or when buffer fills
        self.min_actions_for_inference: int = 3

        # Context State
        self.active_application: str = "Unknown"
        self.visual_context: str = "None"

        # ğŸ†• [P0çº§ä¼˜åŒ–] é›†æˆæ¨¡å¼åŒ¹é…å™¨å’Œå†³ç­–ç¼“å­˜
        self.enable_fast_intent = True  # é…ç½®å¼€å…³ï¼Œå¯ç¦ç”¨å¿«é€Ÿè·¯å¾„
        self.pattern_matcher: Optional[PatternMatcher] = get_pattern_matcher() if PatternMatcher else None
        self.intent_cache: Optional[DecisionCache] = get_decision_cache(max_size=1000) if DecisionCache else None

        if self.pattern_matcher:
            logger.info("[IntentTracker] âœ… æ¨¡å¼åŒ¹é…å™¨å·²å¯ç”¨ (å»¶è¿Ÿ<5ms)")
        if self.intent_cache:
            logger.info("[IntentTracker] âœ… æ„å›¾ç¼“å­˜å·²å¯ç”¨ (å‘½ä¸­ç‡ç›®æ ‡>60%)")

        # ç»Ÿè®¡ä¿¡æ¯
        self.fast_path_hits = 0
        self.cache_hits = 0
        self.llm_calls = 0
        
    def add_observation(self, observation: Dict[str, Any]) -> None:
        """
        Ingest a new observation from any observer (CAD, Global, etc.).
        Validates input and enriches with metadata before storing.
        """
        if not isinstance(observation, dict):
            logger.warning("Observation must be a dictionary.")
            return

        timestamp: float = observation.get("timestamp", time.time())
        raw_text: Optional[str] = observation.get("text")
        summary: Optional[str] = observation.get("summary")
        action_text: str = raw_text or summary or "Unknown Action"
        
        # Enriched entry
        try:
            entry: Dict[str, Any] = {
                "timestamp": timestamp,
                "source": observation.get("type", "general"),
                "content": action_text,
                "details": observation.get("vlm_context", "")
            }
            self.action_history.append(entry)
        except Exception as e:
            logger.error(f"Failed to record observation: {e}")

    def update_context(self, app_name: Optional[str], visual_summary: Optional[str] = None) -> None:
        """Update global context info with optional fields."""
        if app_name is not None:
            if not isinstance(app_name, str):
                logger.warning("Application name must be a string.")
                return
            self.active_application = app_name
        if visual_summary is not None:
            if not isinstance(visual_summary, str):
                logger.warning("Visual summary must be a string.")
                return
            self.visual_context = visual_summary

    async def infer_intent(self) -> Optional[Dict[str, Any]]:
        """
        Analyze recent history to infer user intent.
        Returns the intent dictionary if a new insight is found.
        Uses rate limiting and structured error handling.

        ğŸ†• [P0çº§ä¼˜åŒ–] å¿«é€Ÿè·¯å¾„ï¼š
        1. æ¨¡å¼åŒ¹é…ï¼ˆ< 5msï¼‰- 50-100ä¸ªå¸¸è§æ„å›¾
        2. ç¼“å­˜æ£€ç´¢ï¼ˆ< 10msï¼‰- åŸºäºå‘é‡ç›¸ä¼¼åº¦
        3. LLMè°ƒç”¨ï¼ˆ< 2000msï¼‰- ä»…å½“å¿«é€Ÿè·¯å¾„å¤±è´¥
        """
        # 1. Check constraints
        current_time = time.time()
        if len(self.action_history) < self.min_actions_for_inference:
            return None

        time_since_last = current_time - self.last_inference_time
        # Only infer if enough time passed OR buffer is reasonably full
        if time_since_last < self.inference_interval and len(self.action_history) < self.action_history.maxlen * 0.7:
            return None

        self.last_inference_time = current_time

        # ğŸ†• [P0ä¼˜åŒ–] å¿«é€Ÿè·¯å¾„1ï¼šæ¨¡å¼åŒ¹é…ï¼ˆ< 5msï¼‰
        if self.enable_fast_intent and self.pattern_matcher:
            # æå–æœ€è¿‘çš„æ–‡æœ¬å†…å®¹
            recent_text = self._extract_recent_text()
            if recent_text:
                match_result = self.pattern_matcher.match(recent_text)
                if match_result and match_result.confidence >= 0.9:
                    self.fast_path_hits += 1
                    logger.info(
                        f"[IntentTracker] ğŸ¯ æ¨¡å¼åŒ¹é…å‘½ä¸­ "
                        f"(intent={match_result.intent}, "
                        f"confidence={match_result.confidence:.2f}, "
                        f"fast_path_hits={self.fast_path_hits})"
                    )
                    # æ„é€ æ ‡å‡†æ ¼å¼çš„è¿”å›ç»“æœ
                    return {
                        "intent": match_result.intent,
                        "confidence": match_result.confidence,
                        "next_prediction": f"Based on pattern: {match_result.matched_pattern}",
                        "suggestion": f"Tool: {match_result.metadata.get('tool', 'unknown')}",
                        "source": "pattern_matcher",
                        "matched_pattern": match_result.matched_pattern
                    }

        # ğŸ†• [P0ä¼˜åŒ–] å¿«é€Ÿè·¯å¾„2ï¼šç¼“å­˜æ£€ç´¢ï¼ˆ< 10msï¼‰
        if self.enable_fast_intent and self.intent_cache:
            # ç”Ÿæˆå†å²æ–‡æœ¬çš„embeddingï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨æ–‡æœ¬hashï¼‰
            history_key = self._generate_history_key()
            if history_key:
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šä½¿ç”¨å†å²æ–‡æœ¬çš„ç¬¬ä¸€ä¸ªè§‚å¯Ÿä½œä¸ºç¼“å­˜key
                # å®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨çœŸå®çš„embedding
                cached_result = self._check_cache(history_key)
                if cached_result:
                    self.cache_hits += 1
                    logger.info(
                        f"[IntentTracker] ğŸ’¾ ç¼“å­˜å‘½ä¸­ "
                        f"(intent={cached_result.get('intent')}, "
                        f"cache_hits={self.cache_hits})"
                    )
                    return cached_result

        # ğŸ†• [P0ä¼˜åŒ–] è®°å½•LLMè°ƒç”¨
        self.llm_calls += 1
        
        # 2. Prepare Prompt
        try:
            history_str = "\n".join([
                f"- [{time.strftime('%H:%M:%S', time.localtime(e['timestamp']))}] ({e['source']}) {e['content']} {e['details']}"
                for e in self.action_history
            ])
        except Exception as e:
            logger.error(f"Failed to format action history: {e}")
            return None

        prompt = f"""
You are the 'Subconscious Intent Analyst' of an AGI system.
The user is currently working in: {self.active_application}.
Visual Context: {self.visual_context}

Recent User Actions:
{history_str}

Analyze this stream of behavior.
1. What is the user trying to achieve? (The high-level goal)
2. What pattern are they following?
3. Predict their next likely step.

Output a concise JSON object:
{{
  "intent": "High level goal description",
  "confidence": 0.0-1.0,
  "next_prediction": "Prediction of next action",
  "suggestion": "How can the system help? (Optional)"
}}
"""

        # 3. Call LLM
        try:
            response: str = self.llm.chat_completion(
                system_prompt="You are the 'Subconscious Intent Analyst' of an AGI system.",
                user_prompt=prompt,
                model=None  # Use default
            )
            
            if not response or not response.strip():
                logger.warning("Empty response from LLM during intent inference.")
                return None

            # 4. Parse Response
            clean_resp: str = response.strip()
            if clean_resp.startswith("{"):
                parsed: Dict[str, Any] = json.loads(clean_resp)
                self.current_hypothesis = parsed
                return parsed
            else:
                logger.warning(f"LLM response does not start with '{{'. Got: {clean_resp[:200]}")
                return None

        except json.JSONDecodeError as je:
            logger.error(f"Failed to parse LLM response as JSON: {je}")
            return None
        except Exception as e:
            logger.error(f"Unexpected error during intent inference: {e}")
            return None

    # ğŸ†• [P0çº§ä¼˜åŒ–] å¿«é€Ÿè·¯å¾„è¾…åŠ©æ–¹æ³•

    def _extract_recent_text(self) -> Optional[str]:
        """æå–æœ€è¿‘çš„æ–‡æœ¬å†…å®¹ç”¨äºæ¨¡å¼åŒ¹é…"""
        if not self.action_history:
            return None

        # è·å–æœ€è¿‘çš„è§‚å¯Ÿ
        recent = list(self.action_history)[-1]
        return recent.get('content', '')

    def _generate_history_key(self) -> Optional[str]:
        """ç”Ÿæˆå†å²è®°å½•çš„å”¯ä¸€keyï¼ˆç”¨äºç¼“å­˜ï¼‰"""
        if not self.action_history:
            return None

        # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨æœ€è¿‘3ä¸ªè§‚å¯Ÿçš„å†…å®¹hash
        recent_texts = [e.get('content', '') for e in list(self.action_history)[-3:]]
        combined = '|'.join(recent_texts)
        return combined[:200]  # é™åˆ¶é•¿åº¦

    def _check_cache(self, key: str) -> Optional[Dict[str, Any]]:
        """æ£€æŸ¥ç¼“å­˜ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šå®é™…åº”è¯¥ä½¿ç”¨embeddingå‘é‡ç›¸ä¼¼åº¦
        # å½“å‰å®ç°ï¼šä»…åœ¨å®Œå…¨åŒ¹é…æ—¶è¿”å›ç¼“å­˜
        if hasattr(self, '_cache_store') and key in self._cache_store:
            return self._cache_store[key]
        return None

    def _store_cache(self, key: str, result: Dict[str, Any]) -> None:
        """å­˜å‚¨åˆ°ç¼“å­˜"""
        if not hasattr(self, '_cache_store'):
            self._cache_store = {}

        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self._cache_store) >= 1000:
            # åˆ é™¤æœ€æ—§çš„æ¡ç›®
            oldest_key = next(iter(self._cache_store))
            del self._cache_store[oldest_key]

        self._cache_store[key] = result

    def get_fast_path_statistics(self) -> Dict[str, Any]:
        """è·å–å¿«é€Ÿè·¯å¾„ç»Ÿè®¡ä¿¡æ¯"""
        total = self.fast_path_hits + self.cache_hits + self.llm_calls
        return {
            'fast_path_hits': self.fast_path_hits,
            'cache_hits': self.cache_hits,
            'llm_calls': self.llm_calls,
            'total_inferences': total,
            'fast_path_rate': self.fast_path_hits / total if total > 0 else 0.0,
            'llm_call_rate': self.llm_calls / total if total > 0 else 0.0
        }