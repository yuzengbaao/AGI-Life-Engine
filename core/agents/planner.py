import json
import platform
import hashlib
from typing import List, Dict, Any, Optional, Set
from .base_agent import BaseAgent

# ğŸ†• [P0çº§ä¼˜åŒ–] å¯¼å…¥è§„åˆ’ç¼“å­˜
try:
    from core.decision_cache import get_decision_cache
except ImportError:
    get_decision_cache = None

class PlannerAgent(BaseAgent):
    """
    Role: Strategist
    Responsibility: Break down high-level user goals into atomic, executable steps.
    Output: A list of JSON-formatted tasks or simple text steps.
    
    [2026-01-17] è§£é™¤ç¡¬ç¼–ç æ­¥æ•°é™åˆ¶ï¼Œæ”¯æŒè‡ªé€‚åº”è§„åˆ’æ·±åº¦
    [2026-01-17] ç§»é™¤ç¡¬æ€§ä¸Šé™ï¼Œè§„åˆ’å™¨è‡ªä¸»å†³å®šæ¨ç†å±‚çº§æ•°é‡
    [2026-01-18] æ·»åŠ åŠ¨æ€å·¥å…·æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¯æ„ŸçŸ¥è¿è¡Œæ—¶åˆ›å»ºçš„æ–°å·¥å…·
    """
    # è‡ªé€‚åº”è§„åˆ’æ­¥æ•°é…ç½® - æ— ç¡¬æ€§ä¸Šé™ï¼Œè§„åˆ’å™¨è‡ªä¸»å†³ç­–
    MIN_PLAN_STEPS = 3        # æœ€å°æ­¥æ•°ï¼ˆç®€å•ä»»åŠ¡ï¼‰
    DEFAULT_PLAN_STEPS = 10   # é»˜è®¤æ­¥æ•°ï¼ˆå¸¸è§„ä»»åŠ¡ï¼‰
    DEEP_PLAN_STEPS = 25      # æ·±åº¦è§„åˆ’æ­¥æ•°ï¼ˆå¤æ‚ä»»åŠ¡ï¼‰
    ULTRA_DEEP_STEPS = 50     # è¶…æ·±åº¦è§„åˆ’ï¼ˆé«˜å¤æ‚åº¦ä»»åŠ¡ï¼‰
    MAX_PLAN_STEPS = 999      # ç†è®ºä¸Šé™ï¼ˆå®é™…ç”±è§„åˆ’å™¨è‡ªä¸»å†³å®šï¼Œæ— ç¡¬æ€§é™åˆ¶ï¼‰
    
    def __init__(self, llm_service, biological_memory=None, event_bus=None, tool_registry=None):
        super().__init__("Planner", llm_service)
        self.biological_memory = biological_memory
        self._adaptive_max_steps = self.DEFAULT_PLAN_STEPS  # å½“å‰è‡ªé€‚åº”æ­¥æ•°

        # ğŸ†• [2026-01-18] åŠ¨æ€å·¥å…·æ„ŸçŸ¥èƒ½åŠ›
        self._tool_registry = tool_registry
        self._dynamic_tools: Set[str] = set()  # è¿è¡Œæ—¶åˆ›å»ºçš„å·¥å…·åç§°
        self._event_bus = event_bus

        # ğŸ†• [P0çº§ä¼˜åŒ–] è§„åˆ’ç»“æœç¼“å­˜
        self.planning_cache = {}  # {task_hash: steps}
        self.cache_hits = 0
        self.cache_misses = 0
        self.enable_planning_cache = True  # é…ç½®å¼€å…³

        # è®¢é˜…å·¥å…·åˆ›å»ºäº‹ä»¶
        if event_bus:
            self._subscribe_to_tool_events(event_bus)
    
    def _subscribe_to_tool_events(self, event_bus):
        """è®¢é˜…å·¥å…·åˆ›å»ºäº‹ä»¶ï¼Œæ„ŸçŸ¥æ–°å·¥å…·"""
        try:
            if hasattr(event_bus, 'subscribe'):
                event_bus.subscribe('tool.created', self._on_tool_created)
                event_bus.subscribe('autonomy.tool_created', self._on_tool_created)
                print("   [Planner] ğŸ”” Subscribed to tool creation events")
        except Exception as e:
            print(f"   [Planner] âš ï¸ Failed to subscribe to events: {e}")
    
    def _on_tool_created(self, event):
        """å¤„ç†å·¥å…·åˆ›å»ºäº‹ä»¶"""
        try:
            data = event.data if hasattr(event, 'data') else event
            tool_name = data.get('tool_name', '')
            if tool_name:
                self._dynamic_tools.add(tool_name)
                print(f"   [Planner] ğŸ”§ New tool available: {tool_name}")
        except Exception as e:
            print(f"   [Planner] âš ï¸ Error handling tool event: {e}")
    
    def get_available_tools(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨å·¥å…·ï¼ˆåŒ…æ‹¬åŠ¨æ€åˆ›å»ºçš„ï¼‰"""
        base_tools = [
            'read_file', 'write_file', 'run_python', 'run_command',
            'execute_macro', 'wait', 'log'
        ]
        
        # æ·»åŠ ä» registry è·å–çš„å·¥å…·
        if self._tool_registry and hasattr(self._tool_registry, 'list_tools'):
            registry_tools = self._tool_registry.list_tools()
            base_tools.extend(registry_tools)
        
        # æ·»åŠ åŠ¨æ€åˆ›å»ºçš„å·¥å…·
        base_tools.extend(list(self._dynamic_tools))
        
        return list(set(base_tools))  # å»é‡

    def _estimate_task_complexity(self, text: str) -> float:
        """
        ä¼°ç®—ä»»åŠ¡å¤æ‚åº¦ (0.0 - 1.0)
        ç”¨äºåŠ¨æ€å†³å®šè§„åˆ’æ­¥æ•°ä¸Šé™
        """
        if not text:
            return 0.2
        
        lower = text.lower()
        complexity = 0.3  # åŸºç¡€å¤æ‚åº¦
        
        # å¤æ‚åº¦æŒ‡æ ‡
        complexity_indicators = {
            # é«˜å¤æ‚åº¦å…³é”®è¯
            "high_complexity": [
                "research", "analyze", "investigate", "design", "architect",
                "è°ƒæŸ¥", "ç ”ç©¶", "åˆ†æ", "è®¾è®¡", "æ¶æ„", "å®ç°", "implement",
                "multi-step", "å¤šæ­¥éª¤", "å¤æ‚", "complex", "comprehensive",
                "deep", "æ·±åº¦", "å…¨é¢", "ç³»ç»Ÿæ€§", "systematic"
            ],
            # ä¸­ç­‰å¤æ‚åº¦
            "medium_complexity": [
                "create", "build", "develop", "test", "debug",
                "åˆ›å»º", "æ„å»º", "å¼€å‘", "æµ‹è¯•", "è°ƒè¯•", "ä¿®å¤", "fix"
            ],
            # ä½å¤æ‚åº¦
            "low_complexity": [
                "read", "list", "check", "log", "observe",
                "è¯»å–", "åˆ—å‡º", "æ£€æŸ¥", "è®°å½•", "è§‚å¯Ÿ"
            ]
        }
        
        for keyword in complexity_indicators["high_complexity"]:
            if keyword in lower:
                complexity += 0.15
        
        for keyword in complexity_indicators["medium_complexity"]:
            if keyword in lower:
                complexity += 0.08
        
        for keyword in complexity_indicators["low_complexity"]:
            if keyword in lower:
                complexity -= 0.05
        
        # æ–‡æœ¬é•¿åº¦å½±å“
        if len(text) > 500:
            complexity += 0.2
        elif len(text) > 200:
            complexity += 0.1
        
        # å¤šå­ä»»åŠ¡æ£€æµ‹ï¼ˆé€—å·ã€åˆ†å·ã€"å’Œ"ã€"ç„¶å"ç­‰ï¼‰
        multi_task_markers = ["ï¼Œ", ",", ";", "ï¼›", "ç„¶å", "æ¥ç€", "and then", "finally"]
        for marker in multi_task_markers:
            if marker in text:
                complexity += 0.1
                break
        
        return max(0.0, min(1.0, complexity))

    def _get_adaptive_max_steps(self, text: str) -> int:
        """
        æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€è¿”å›æœ€å¤§æ­¥æ•°
        è§„åˆ’å™¨è‡ªä¸»å†³å®šæ¨ç†æ·±åº¦ï¼Œæ— ç¡¬æ€§ä¸Šé™
        """
        complexity = self._estimate_task_complexity(text)
        
        if complexity < 0.3:
            max_steps = self.MIN_PLAN_STEPS
        elif complexity < 0.5:
            max_steps = self.DEFAULT_PLAN_STEPS
        elif complexity < 0.7:
            max_steps = self.DEEP_PLAN_STEPS
        elif complexity < 0.85:
            max_steps = self.ULTRA_DEEP_STEPS
        else:
            # æç«¯å¤æ‚ä»»åŠ¡ï¼šè§„åˆ’å™¨è‡ªä¸»å†³å®šï¼Œè¿”å›ç†è®ºä¸Šé™
            # å®é™…æ­¥æ•°ç”± LLM æ ¹æ®ä»»åŠ¡éœ€æ±‚è‡ªè¡Œç”Ÿæˆ
            max_steps = self.MAX_PLAN_STEPS
        
        self._adaptive_max_steps = max_steps
        self.log_thought(f"ğŸ“Š Task complexity: {complexity:.2f} â†’ Max plan steps: {max_steps} (è‡ªä¸»å†³ç­–)")
        return max_steps

    async def decompose_task(self, text: str, failed_steps: List[str] = None, error_diagnosis: str = None, memory_context: List[Dict] = None) -> List[str]:
        """
        Decompose a high-level goal into executable steps, considering past failures and memory.

        ğŸ†• [P0çº§ä¼˜åŒ–] å¿«é€Ÿè·¯å¾„ï¼š
        1. è§„åˆ’ç¼“å­˜æ£€æŸ¥ï¼ˆåŸºäºä»»åŠ¡æ–‡æœ¬hashï¼‰
        2. ç¡®å®šæ€§è§„åˆ™åŒ¹é…ï¼ˆæœªæ¥æ‰©å±•ï¼‰
        3. LLMè§„åˆ’ï¼ˆä»…å½“ç¼“å­˜æœªå‘½ä¸­ï¼‰
        """
        self.log_thought(f"Analyzing task complexity: {text}")

        # ğŸ†• [P0ä¼˜åŒ–] å¿«é€Ÿè·¯å¾„1ï¼šè§„åˆ’ç¼“å­˜æ£€æŸ¥
        if self.enable_planning_cache and not failed_steps and not error_diagnosis:
            cache_key = self._generate_plan_cache_key(text)
            if cache_key and cache_key in self.planning_cache:
                cached_steps = self.planning_cache[cache_key]
                self.cache_hits += 1
                self.log_thought(
                    f"ğŸ’¾ è§„åˆ’ç¼“å­˜å‘½ä¸­ "
                    f"(steps={len(cached_steps)}, "
                    f"cache_hits={self.cache_hits})"
                )
                return cached_steps
            else:
                self.cache_misses += 1

        # Windows command adaptation
        if platform.system() == "Windows":
            text = text.replace("ls -lt", "dir /O-D").replace("ls", "dir").replace("grep", "findstr").replace("cat", "type").replace("rm", "del").replace("cp", "copy").replace("mv", "move")

        lower_text = (text or "").lower()
        if "[meta]" in lower_text and ("investigate" in lower_text or "high entropy" in lower_text or "è°ƒæŸ¥" in lower_text):
            import time as _time

            def j(tool: str, args: Dict[str, Any]) -> str:
                return json.dumps({"tool": tool, "args": args}, ensure_ascii=False)

            output_file = None
            marker = "data/entropy_investigation_"
            start = (text or "").find(marker)
            if start != -1:
                end = (text or "").find(".json", start)
                if end != -1:
                    output_file = (text or "")[start:end + 5]
            if not output_file:
                output_file = f"data/entropy_investigation_{int(_time.time())}.json"

            steps: List[str] = []
            steps.append(j("analyze_entropy_sources", {"output_file": output_file}))
            steps.append(j("check_memory_drift", {"threshold": 0.3}))
            steps.append(j("evaluate_uncertainty_distribution", {}))
            steps.append(j("synthesize_investigation_report", {"output_file": output_file}))
            steps.append(j("log", {"message": f"Meta-cognitive investigation complete. Report: {output_file}"}))
            self.log_thought(f"Meta-cognitive investigation plan created with {len(steps)} steps.")
            max_steps = self._get_adaptive_max_steps(text)
            return steps[:max_steps]

        avoid_instruction = ""
        if failed_steps and len(failed_steps) > 0:
            avoid_instruction = f"\n        IMPORTANT: The following steps have ALREADY FAILED. DO NOT generate them again. Find a different approach:\n        {json.dumps(failed_steps)}\n"

        error_instruction = ""
        if error_diagnosis:
            error_instruction = f"\n        CRITICAL: The previous attempt failed. DIAGNOSIS:\n        {error_diagnosis}\n        ADJUST your plan to fix this error.\n"
            
        # Build Memory Context String
        memory_instruction = ""
        if memory_context:
            memory_str = ""
            for m in memory_context:
                source = m.get('source', 'unknown')
                content = m.get('content', '')
                # Highlight failures
                prefix = "[PAST FAILURE]" if "failure" in source or "failure" in m.get('tags', []) else "[MEMORY]"
                memory_str += f"- {prefix} ({source}): {content[:200]}...\n"
            
            memory_instruction = f"""
        MEMORY CONTEXT (Related past experiences):
        {memory_str}
        PAY SPECIAL ATTENTION to [PAST FAILURE] items to avoid repeating mistakes.
        """

        macro_instruction = ""
        if self.biological_memory is not None:
            try:
                macros = self.biological_memory.suggest_macros_for_goal(text, top_k=3)
            except Exception:
                macros = []
            if macros:
                lines = []
                for m in macros:
                    mid = m.get("macro_id") or m.get("id")
                    pat = m.get("content_preview") or ""
                    lines.append(f"- Macro ID: {mid} | Pattern: {pat}")
                macro_text = "\n".join(lines)
                macro_instruction = f"""
        LEARNED MACRO PATTERNS (Reusable high-level behaviors):
{macro_text}
        You may treat an entire macro pattern as ONE high-level step in your plan,
        instead of enumerating all of its primitive operations separately.
        If a macro is applicable, you can call it directly:
        {{"tool": "execute_macro", "args": {{"macro_id": "<macro_id>", "bindings": {{}} }} }}
        """

        prompt = f"""
        You are the PLANNER Agent.
        Your job is to break down this complex user request into a sequence of atomic steps.
        
        CURRENT SYSTEM: {platform.system()}
        IMPORTANT: Generate commands compatible with this operating system (e.g., use 'dir' instead of 'ls' on Windows).
        
        The EXECUTOR Agent has these tools:
        - "Google Search X"
        - "Read file X" (JSON: {{"tool": "read_file", "args": {{"path": "..."}}}})
        - "Write file X" (JSON: {{"tool": "write_file", "args": {{"path": "...", "content": "..."}}}})
        - "Run Python script X" (JSON: {{"tool": "run_python", "args": {{"script_name": "..."}}}})
        - "Run Command X" (JSON: {{"tool": "run_command", "args": {{"command": "..."}}}})
        - "Execute Macro X" (JSON: {{"tool": "execute_macro", "args": {{"macro_id": "...", "bindings": {{}} }} }})
        - "Open App X" (ONLY use if app is NOT open)
        - "Type Text X"
        - "Wait X" (JSON: {{"tool": "wait", "args": {{"seconds": 5}}}})
        - "Log Observation" (JSON: {{"tool": "log", "args": {{"message": "..."}}}})
        
        USER REQUEST: {text}
        {memory_instruction}
        {macro_instruction}
        {avoid_instruction}
        {error_instruction}
        CRITICAL RULES:
        1. If the request is about "Observing", "Monitoring", or "Learning", DO NOT open new apps. Use "Wait" and "Log Observation".
        2. Output a PURE JSON list of strings.
        3. If the task involves file operations, PREFER using the JSON tool format in the step string.
        4. YOU decide the step count based on task complexity. Simple tasks: 3-10 steps. Complex tasks: 20-50+ steps. No artificial limit.
        5. For research/investigation/design tasks, use as many steps as needed for thoroughness. You have full autonomy.
        
        Example Output for Observation:
        [
            "{{\\"tool\\": \\"log\\", \\"args\\": {{\\"message\\": \\"Starting observation cycle...\\"}} }}",
            "{{\\"tool\\": \\"wait\\", \\"args\\": {{\\"seconds\\": 5}} }}",
            "{{\\"tool\\": \\"log\\", \\"args\\": {{\\"message\\": \\"Observation complete.\\"}} }}"
        ]
        """
        
        try:
            resp = self.llm.chat_completion(system_prompt="You are a Senior Project Manager.", user_prompt=prompt)

            if isinstance(resp, str) and (resp.startswith("[MOCK") or resp.startswith("[LLM ERROR]")):
                steps = self._heuristic_plan(text)
                # ğŸ†• [P0ä¼˜åŒ–] å¯å‘å¼è§„åˆ’ä¹Ÿç¼“å­˜ï¼ˆä½ç½®ä¿¡åº¦ï¼‰
                if not failed_steps and not error_diagnosis:
                    self._store_plan_cache(text, steps)
                self.log_thought(f"Planner LLM unavailable. Heuristic plan created with {len(steps)} steps.")
                return steps

            json_str = resp
            if "```json" in resp:
                json_str = resp.split("```json")[1].split("```")[0].strip()
            elif "```" in resp:
                json_str = resp.split("```")[1].split("```")[0].strip()

            steps = json.loads(json_str)
            if isinstance(steps, list) and steps:
                max_steps = self._get_adaptive_max_steps(text)
                final_steps = steps[:max_steps] if len(steps) > max_steps else steps

                # ğŸ†• [P0ä¼˜åŒ–] å­˜å‚¨åˆ°ç¼“å­˜ï¼ˆä»…åœ¨æ— é”™è¯¯ä¸”æ— å¤±è´¥å†å²æ—¶ï¼‰
                if not failed_steps and not error_diagnosis:
                    self._store_plan_cache(text, final_steps)

                self.log_thought(f"Plan created with {len(steps)} steps (max allowed: {max_steps}).")
                return final_steps

            steps = self._heuristic_plan(text)
            # ğŸ†• [P0ä¼˜åŒ–] å¯å‘å¼è§„åˆ’ä¹Ÿç¼“å­˜ï¼ˆä½ç½®ä¿¡åº¦ï¼‰
            if not failed_steps and not error_diagnosis:
                self._store_plan_cache(text, steps)
            self.log_thought(f"Planner returned non-list plan. Heuristic plan created with {len(steps)} steps.")
            return steps

        except Exception as e:
            self.log_thought(f"Planning failed: {e}. Using heuristic plan.")
            return self._heuristic_plan(text)

    def _heuristic_plan(self, text: str) -> List[str]:
        t = (text or "").strip()
        lower = t.lower()

        def j(tool: str, args: Dict[str, Any]) -> str:
            return json.dumps({"tool": tool, "args": args}, ensure_ascii=False)

        steps: List[str] = []

        if "analyze generated insight" in lower and " in " in lower:
            try:
                path = t.split(" in ", 1)[1].strip()
                steps.append(j("read_file", {"path": path}))
                steps.append(j("log", {"message": f"Loaded insight for analysis: {path}"}))
                return steps[:6]
            except Exception:
                pass

        # ğŸ”§ [2026-01-11] å…ƒè®¤çŸ¥è°ƒæŸ¥ä»»åŠ¡çš„ä¸“ç”¨è®¡åˆ’æ¨¡æ¿ - ä¿®å¤ç©ºè½¬å¾ªç¯
        # ğŸ”§ [2026-01-17] è§£é™¤ç¡¬ç¼–ç æ­¥æ•°é™åˆ¶ï¼Œä½¿ç”¨è‡ªé€‚åº”æ­¥æ•°
        if "[meta]" in lower and ("investigate" in lower or "high entropy" in lower or "è°ƒæŸ¥" in lower):
            import time as _time
            output_file = f"data/entropy_investigation_{int(_time.time())}.json"
            steps.append(j("analyze_entropy_sources", {"output_file": output_file}))
            steps.append(j("check_memory_drift", {"threshold": 0.3}))
            steps.append(j("evaluate_uncertainty_distribution", {}))
            steps.append(j("synthesize_investigation_report", {"output_file": output_file}))
            steps.append(j("log", {"message": f"Meta-cognitive investigation complete. Report: {output_file}"}))
            max_steps = self._get_adaptive_max_steps(t)
            return steps[:max_steps]

        if any(k in lower for k in ["self-diagnostics", "diagnostics", "self diagnostics", "è‡ªæ£€", "è¯Šæ–­", "ç»“æ„"]):
            steps.append(j("list_files", {"path": "."}))
            steps.append(j("list_files", {"path": "core"}))
            steps.append(j("inspect_code", {"path": "AGI_Life_Engine.py", "mode": "summary"}))
            steps.append(j("inspect_code", {"path": "core/agents/executor.py", "mode": "summary"}))
            steps.append(j("log", {"message": "Heuristic diagnostics complete."}))
            max_steps = self._get_adaptive_max_steps(t)
            return steps[:max_steps]

        if any(k in lower for k in ["list files", "dir ", "ç›®å½•", "æ–‡ä»¶åˆ—è¡¨"]):
            steps.append(j("list_files", {"path": "."}))
            max_steps = self._get_adaptive_max_steps(t)
            return steps[:max_steps]

        if "log" in lower or "observe" in lower or "monitor" in lower or "è§‚å¯Ÿ" in lower or "ç›‘æ§" in lower:
            steps.append(j("log", {"message": f"Observation: {t}"}))
            steps.append(j("wait", {"seconds": 2}))
            steps.append(j("log", {"message": "Observation tick complete."}))
            max_steps = self._get_adaptive_max_steps(t)
            return steps[:max_steps]

        steps.append(j("log", {"message": f"Heuristic fallback: {t}"}))
        max_steps = self._get_adaptive_max_steps(t)
        return steps[:max_steps]

    async def is_complex_task(self, text: str) -> bool:
        if len(text) < 10: return False
        prompt = f"Is this task complex enough to require breakdown? Answer YES/NO.\nTask: {text}"
        try:
            resp = self.llm.chat_completion(system_prompt="Analyzer", user_prompt=prompt)
            return "YES" in resp.upper()
        except:
            return False

    # ğŸ†• [P0çº§ä¼˜åŒ–] è§„åˆ’ç¼“å­˜è¾…åŠ©æ–¹æ³•

    def _generate_plan_cache_key(self, text: str) -> Optional[str]:
        """
        ç”Ÿæˆè§„åˆ’ç¼“å­˜çš„å”¯ä¸€key

        Args:
            text: ä»»åŠ¡æ–‡æœ¬

        Returns:
            MD5å“ˆå¸Œå€¼
        """
        if not text:
            return None

        # æ ‡å‡†åŒ–æ–‡æœ¬ï¼ˆå»é™¤ç©ºæ ¼ã€è½¬å°å†™ï¼‰
        normalized = ' '.join(text.lower().split())

        # ç”Ÿæˆhash
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()

    def _store_plan_cache(self, text: str, steps: List[str]) -> None:
        """
        å­˜å‚¨è§„åˆ’ç»“æœåˆ°ç¼“å­˜

        Args:
            text: ä»»åŠ¡æ–‡æœ¬
            steps: è§„åˆ’æ­¥éª¤åˆ—è¡¨
        """
        if not self.enable_planning_cache:
            return

        cache_key = self._generate_plan_cache_key(text)
        if not cache_key:
            return

        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self.planning_cache) >= 500:
            # åˆ é™¤æœ€æ—§çš„æ¡ç›®ï¼ˆç®€å•FIFOï¼‰
            oldest_key = next(iter(self.planning_cache))
            del self.planning_cache[oldest_key]
            self.log_thought(f"ğŸ—‘ï¸ è§„åˆ’ç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§æ¡ç›®")

        # å­˜å‚¨åˆ°ç¼“å­˜
        self.planning_cache[cache_key] = steps
        self.log_thought(f"ğŸ’¾ è§„åˆ’ç»“æœå·²ç¼“å­˜ (key={cache_key[:8]}..., steps={len(steps)})")

    def get_cache_statistics(self) -> Dict[str, Any]:
        """
        è·å–è§„åˆ’ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        total = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total if total > 0 else 0.0

        return {
            'cache_size': len(self.planning_cache),
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate': hit_rate,
            'enable_cache': self.enable_planning_cache
        }

    def clear_plan_cache(self) -> None:
        """æ¸…ç©ºè§„åˆ’ç¼“å­˜"""
        size_before = len(self.planning_cache)
        self.planning_cache.clear()
        self.log_thought(f"ğŸ—‘ï¸ è§„åˆ’ç¼“å­˜å·²æ¸…ç©º (åˆ é™¤{size_before}æ¡ç›®)")
