"""
æ´å¯Ÿè¯„ä¼°å™¨ (Insight Evaluator)
æŒç»­è·Ÿè¸ªå·²é›†æˆæ´å¯Ÿçš„å®é™…æ•ˆæœ,æä¾›åé¦ˆå¾ªç¯ã€‚

è¯„ä¼°ç»´åº¦:
1. ä½¿ç”¨é¢‘ç‡ - æ´å¯Ÿè¢«è°ƒç”¨çš„æ¬¡æ•°
2. æ€§èƒ½å½±å“ - å¯¹ç³»ç»Ÿæ•´ä½“æ€§èƒ½çš„è´¡çŒ®
3. é”™è¯¯ç‡ - æ´å¯Ÿæ‰§è¡Œå¤±è´¥çš„æ¯”ä¾‹
4. ä»·å€¼è¡°å‡ - æ´å¯Ÿæ•ˆæœéšæ—¶é—´çš„å˜åŒ–
5. ä¾èµ–å¥åº·åº¦ - å…¶ä»–ç»„ä»¶å¯¹è¯¥æ´å¯Ÿçš„ä¾èµ–ç¨‹åº¦
"""

import time
import json
from typing import Dict, Any, List
from pathlib import Path
from collections import defaultdict
from datetime import datetime, timedelta

class InsightEvaluator:
    """æ´å¯Ÿè¯„ä¼°å™¨ - æŒç»­ç›‘æ§æ´å¯Ÿçš„å®é™…ä»·å€¼"""

    def __init__(self, metrics_file: str = "data/skills/metrics.json", event_callback=None):
        """
        åˆå§‹åŒ–æ´å¯Ÿè¯„ä¼°å™¨

        Args:
            metrics_file: æŒ‡æ ‡å­˜å‚¨æ–‡ä»¶è·¯å¾„
            event_callback: äº‹ä»¶å›è°ƒå‡½æ•°(å¯é€‰)ï¼Œç­¾å callback(event_type: str, data: dict)ï¼Œå¯åŒæ­¥æˆ–å¼‚æ­¥
        """
        self.metrics_file = Path(metrics_file)
        self.metrics_file.parent.mkdir(parents=True, exist_ok=True)

        # ğŸ†• [2026-01-10] äº‹ä»¶å‘å¸ƒå›è°ƒï¼ˆç”¨äºæ‹“æ‰‘å›¾ä¸­ InsightEvaluator â†’ Engine çš„äº‹ä»¶å›æµï¼‰
        self._event_callback = event_callback

        # åŠ è½½å†å²æŒ‡æ ‡
        self.metrics = self._load_metrics()

        # å®æ—¶æŒ‡æ ‡ç¼“å­˜
        self.session_metrics = defaultdict(lambda: {
            'calls': 0,
            'successes': 0,
            'failures': 0,
            'total_time': 0.0,
            'errors': []
        })

    def set_event_callback(self, callback):
        """è®¾ç½®äº‹ä»¶å›è°ƒå‡½æ•°ï¼ˆç”¨äºè¿è¡Œæ—¶æ³¨å…¥ï¼‰"""
        self._event_callback = callback
    
    def _load_metrics(self) -> Dict:
        """åŠ è½½å†å²æŒ‡æ ‡"""
        if self.metrics_file.exists():
            try:
                with open(self.metrics_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {'insights': {}, 'sessions': []}
        return {'insights': {}, 'sessions': []}
    
    def _save_metrics(self):
        """ä¿å­˜æŒ‡æ ‡"""
        try:
            with open(self.metrics_file, 'w', encoding='utf-8') as f:
                json.dump(self.metrics, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"[Evaluator] âš ï¸ ä¿å­˜æŒ‡æ ‡å¤±è´¥: {e}")
    
    def record_call(self, 
                    skill_name: str, 
                    success: bool, 
                    execution_time: float,
                    error: str = None):
        """è®°å½•å•æ¬¡è°ƒç”¨"""
        # æ›´æ–°ä¼šè¯ç¼“å­˜
        session = self.session_metrics[skill_name]
        session['calls'] += 1
        session['total_time'] += execution_time
        
        if success:
            session['successes'] += 1
        else:
            session['failures'] += 1
            if error:
                session['errors'].append({
                    'error': error,
                    'timestamp': time.time()
                })
        
        # æ›´æ–°æŒä¹…åŒ–æŒ‡æ ‡
        if skill_name not in self.metrics['insights']:
            self.metrics['insights'][skill_name] = {
                'total_calls': 0,
                'total_successes': 0,
                'total_failures': 0,
                'total_time': 0.0,
                'first_used': time.time(),
                'last_used': time.time(),
                'daily_calls': [],
                'performance_history': []
            }
        
        insight_metrics = self.metrics['insights'][skill_name]
        insight_metrics['total_calls'] += 1
        insight_metrics['total_time'] += execution_time
        insight_metrics['last_used'] = time.time()
        
        if success:
            insight_metrics['total_successes'] += 1
        else:
            insight_metrics['total_failures'] += 1
        
        # è®°å½•æ¯æ—¥è°ƒç”¨(ç”¨äºæ£€æµ‹è¡°å‡)
        today = datetime.now().date().isoformat()
        daily_calls = insight_metrics.setdefault('daily_calls', [])
        if not daily_calls or daily_calls[-1]['date'] != today:
            daily_calls.append({'date': today, 'count': 1})
        else:
            daily_calls[-1]['count'] += 1
        
        # ä¿æŒæœ€è¿‘30å¤©æ•°æ®
        if len(daily_calls) > 30:
            insight_metrics['daily_calls'] = daily_calls[-30:]
    
    def evaluate(self, skill_name: str) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªæ´å¯Ÿ
        
        è¿”å›æ ¼å¼:
        {
            'score': float (0-1),
            'usage_frequency': float,
            'success_rate': float,
            'avg_execution_time': float,
            'value_decay': float,  # è´Ÿæ•°è¡¨ç¤ºè¡°å‡
            'recommendation': str,  # 'KEEP', 'IMPROVE', 'DEPRECATE'
            'health': str  # 'HEALTHY', 'WARNING', 'CRITICAL'
        }
        """
        if skill_name not in self.metrics['insights']:
            return {
                'score': 0.0,
                'usage_frequency': 0.0,
                'success_rate': 0.0,
                'recommendation': 'NEW',
                'health': 'UNKNOWN'
            }
        
        m = self.metrics['insights'][skill_name]
        
        # 1. ä½¿ç”¨é¢‘ç‡å¾—åˆ† (0-1)
        calls_per_day = self._calculate_calls_per_day(m)
        freq_score = min(1.0, calls_per_day / 10.0)  # æ¯å¤©10æ¬¡è°ƒç”¨=æ»¡åˆ†
        
        # 2. æˆåŠŸç‡å¾—åˆ† (0-1)
        success_rate = m['total_successes'] / m['total_calls'] if m['total_calls'] > 0 else 0
        
        # 3. æ€§èƒ½å¾—åˆ† (0-1, æ‰§è¡Œæ—¶é—´è¶ŠçŸ­è¶Šå¥½)
        avg_time = m['total_time'] / m['total_calls'] if m['total_calls'] > 0 else 0
        perf_score = max(0, 1.0 - avg_time / 0.5)  # 0.5så†…å®Œæˆ=æ»¡åˆ†
        
        # 4. ä»·å€¼è¡°å‡æ£€æµ‹
        decay = self._calculate_value_decay(m)
        
        # ç»¼åˆè¯„åˆ†
        weights = {'frequency': 0.3, 'success': 0.4, 'performance': 0.2, 'decay': 0.1}
        score = (
            weights['frequency'] * freq_score +
            weights['success'] * success_rate +
            weights['performance'] * perf_score +
            weights['decay'] * (1.0 + decay)  # decayä¸ºè´Ÿ,æ‰€ä»¥åŠ ä¸Šå®ƒ
        )
        
        # å¥åº·çŠ¶æ€
        if success_rate > 0.9 and decay > -0.1:
            health = 'HEALTHY'
        elif success_rate > 0.7 and decay > -0.3:
            health = 'WARNING'
        else:
            health = 'CRITICAL'
        
        # å»ºè®®
        if score > 0.7 and health == 'HEALTHY':
            recommendation = 'KEEP'
        elif score > 0.5:
            recommendation = 'IMPROVE'
        else:
            recommendation = 'DEPRECATE'
        
        return {
            'score': score,
            'usage_frequency': calls_per_day,
            'success_rate': success_rate,
            'avg_execution_time': avg_time,
            'value_decay': decay,
            'recommendation': recommendation,
            'health': health,
            'total_calls': m['total_calls'],
            'days_active': (time.time() - m['first_used']) / 86400
        }
    
    def _calculate_calls_per_day(self, metrics: Dict) -> float:
        """è®¡ç®—æ¯æ—¥å¹³å‡è°ƒç”¨æ¬¡æ•°"""
        daily_calls = metrics.get('daily_calls', [])
        if not daily_calls:
            return 0.0
        
        # æœ€è¿‘7å¤©å¹³å‡
        recent_calls = daily_calls[-7:]
        return sum(d['count'] for d in recent_calls) / len(recent_calls)
    
    def _calculate_value_decay(self, metrics: Dict) -> float:
        """
        è®¡ç®—ä»·å€¼è¡°å‡ç‡
        æ¯”è¾ƒæœ€è¿‘7å¤©ä¸ä¹‹å‰7å¤©çš„è°ƒç”¨é¢‘ç‡
        
        è¿”å›: -1.0 åˆ° 1.0 (è´Ÿæ•°=è¡°å‡, æ­£æ•°=å¢é•¿)
        """
        daily_calls = metrics.get('daily_calls', [])
        if len(daily_calls) < 14:
            return 0.0  # æ•°æ®ä¸è¶³
        
        recent_7 = daily_calls[-7:]
        previous_7 = daily_calls[-14:-7]
        
        recent_avg = sum(d['count'] for d in recent_7) / 7
        previous_avg = sum(d['count'] for d in previous_7) / 7
        
        if previous_avg == 0:
            return 0.0
        
        decay = (recent_avg - previous_avg) / previous_avg
        return max(-1.0, min(1.0, decay))  # é™åˆ¶åœ¨[-1, 1]
    
    def generate_report(self, top_n: int = 10, emit_event: bool = True) -> Dict[str, Any]:
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        åŒ…å«:
        1. Top N æœ€æœ‰ä»·å€¼æ´å¯Ÿ
        2. éœ€è¦æ”¹è¿›çš„æ´å¯Ÿ
        3. å»ºè®®å¼ƒç”¨çš„æ´å¯Ÿ
        4. æ€»ä½“ç»Ÿè®¡
        
        Args:
            top_n: è¿”å›Top Nä¸ªæœ€ä½³æ´å¯Ÿ
            emit_event: æ˜¯å¦å‘å¸ƒè¯„ä¼°å®Œæˆäº‹ä»¶ï¼ˆç”¨äºæ‹“æ‰‘å›¾ä¸­çš„äº‹ä»¶å›æµï¼‰
        """
        # è¯„ä¼°æ‰€æœ‰æ´å¯Ÿ
        evaluations = {}
        for skill_name in self.metrics['insights'].keys():
            evaluations[skill_name] = self.evaluate(skill_name)
        
        # æ’åº
        sorted_by_score = sorted(
            evaluations.items(), 
            key=lambda x: x[1]['score'], 
            reverse=True
        )
        
        # åˆ†ç±»
        top_performers = sorted_by_score[:top_n]
        need_improvement = [
            (name, eval) for name, eval in evaluations.items()
            if eval['recommendation'] == 'IMPROVE'
        ]
        deprecated = [
            (name, eval) for name, eval in evaluations.items()
            if eval['recommendation'] == 'DEPRECATE'
        ]
        
        # æ€»ä½“ç»Ÿè®¡
        total_insights = len(evaluations)
        healthy_count = sum(1 for e in evaluations.values() if e['health'] == 'HEALTHY')
        warning_count = sum(1 for e in evaluations.values() if e['health'] == 'WARNING')
        critical_count = sum(1 for e in evaluations.values() if e['health'] == 'CRITICAL')
        
        avg_score = sum(e['score'] for e in evaluations.values()) / total_insights if total_insights > 0 else 0
        avg_success_rate = sum(e['success_rate'] for e in evaluations.values()) / total_insights if total_insights > 0 else 0
        
        report = {
            'generated_at': datetime.now().isoformat(),
            'summary': {
                'total_insights': total_insights,
                'healthy': healthy_count,
                'warning': warning_count,
                'critical': critical_count,
                'average_score': avg_score,
                'average_success_rate': avg_success_rate
            },
            'top_performers': [
                {
                    'name': name,
                    **eval_data
                }
                for name, eval_data in top_performers
            ],
            'need_improvement': [
                {
                    'name': name,
                    'score': eval_data['score'],
                    'success_rate': eval_data['success_rate'],
                    'issue': self._diagnose_issue(eval_data)
                }
                for name, eval_data in need_improvement
            ],
            'deprecated': [
                {
                    'name': name,
                    'score': eval_data['score'],
                    'reason': self._deprecation_reason(eval_data)
                }
                for name, eval_data in deprecated
            ]
        }
        
        # ğŸ†• [2026-01-10] å‘å¸ƒè¯„ä¼°å®Œæˆäº‹ä»¶ï¼ˆä¿®å¤æ‹“æ‰‘å›¾ä¸­InsightEvaluatorâ†’Engineçš„eventè¿æ¥ï¼‰
        if emit_event and self._event_callback:
            try:
                import asyncio
                event_data = {
                    'report_summary': report['summary'],
                    'top_performer': top_performers[0][0] if top_performers else None,
                    'critical_count': critical_count,
                    'deprecated_count': len(deprecated)
                }
                # æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥å›è°ƒ
                if asyncio.iscoroutinefunction(self._event_callback):
                    asyncio.create_task(self._event_callback('insight_evaluation_complete', event_data))
                else:
                    self._event_callback('insight_evaluation_complete', event_data)
            except Exception as e:
                print(f"[Evaluator] âš ï¸ äº‹ä»¶å‘å¸ƒå¤±è´¥: {e}")
        
        return report
    
    def _diagnose_issue(self, eval_data: Dict) -> str:
        """è¯Šæ–­æ´å¯Ÿçš„é—®é¢˜"""
        issues = []
        
        if eval_data['success_rate'] < 0.7:
            issues.append(f"ä½æˆåŠŸç‡({eval_data['success_rate']:.1%})")
        
        if eval_data['usage_frequency'] < 1.0:
            issues.append(f"ä½ä½¿ç”¨ç‡({eval_data['usage_frequency']:.1f}æ¬¡/å¤©)")
        
        if eval_data['value_decay'] < -0.3:
            issues.append(f"ä»·å€¼è¡°å‡({eval_data['value_decay']:+.1%})")
        
        if eval_data['avg_execution_time'] > 0.5:
            issues.append(f"æ€§èƒ½æ…¢({eval_data['avg_execution_time']:.2f}s)")
        
        return ', '.join(issues) if issues else 'æœªçŸ¥é—®é¢˜'
    
    def _deprecation_reason(self, eval_data: Dict) -> str:
        """å¼ƒç”¨åŸå› """
        if eval_data['success_rate'] < 0.5:
            return f"é«˜å¤±è´¥ç‡({eval_data['success_rate']:.1%})"
        elif eval_data['usage_frequency'] < 0.1:
            return f"æå°‘ä½¿ç”¨({eval_data['usage_frequency']:.2f}æ¬¡/å¤©)"
        elif eval_data['value_decay'] < -0.5:
            return f"ä¸¥é‡è¡°å‡({eval_data['value_decay']:+.1%})"
        else:
            return f"ç»¼åˆè¯„åˆ†è¿‡ä½({eval_data['score']:.2f})"
    
    def cleanup_deprecated(self, skill_names: List[str], archive_dir: str = "data/skills/deprecated"):
        """æ¸…ç†å¼ƒç”¨çš„æ´å¯Ÿ"""
        from pathlib import Path
        import shutil
        
        archive_path = Path(archive_dir)
        archive_path.mkdir(parents=True, exist_ok=True)
        
        cleaned = []
        for skill_name in skill_names:
            try:
                # ç§»åŠ¨æ–‡ä»¶åˆ°deprecatedç›®å½•
                skill_file = Path(f"data/skills/{skill_name}.py")
                if skill_file.exists():
                    shutil.move(str(skill_file), str(archive_path / skill_file.name))
                    cleaned.append(skill_name)
                    
                    # æ ‡è®°ä¸ºå·²å¼ƒç”¨
                    if skill_name in self.metrics['insights']:
                        self.metrics['insights'][skill_name]['deprecated'] = True
                        self.metrics['insights'][skill_name]['deprecated_at'] = time.time()
                    
                    print(f"   [Evaluator] ğŸ—‘ï¸ æ¸…ç†å¼ƒç”¨æ´å¯Ÿ: {skill_name}")
            except Exception as e:
                print(f"   [Evaluator] âš ï¸ æ¸…ç†å¤±è´¥ {skill_name}: {e}")
        
        if cleaned:
            self._save_metrics()
        
        return cleaned
    
    def end_session(self):
        """ç»“æŸè¯„ä¼°ä¼šè¯,ä¿å­˜æ‰€æœ‰æŒ‡æ ‡"""
        # è®°å½•ä¼šè¯æ‘˜è¦
        session_summary = {
            'timestamp': time.time(),
            'insights_called': len(self.session_metrics),
            'total_calls': sum(m['calls'] for m in self.session_metrics.values()),
            'total_successes': sum(m['successes'] for m in self.session_metrics.values()),
            'total_failures': sum(m['failures'] for m in self.session_metrics.values())
        }
        
        self.metrics['sessions'].append(session_summary)
        
        # ä¿æŒæœ€è¿‘100ä¸ªä¼šè¯
        if len(self.metrics['sessions']) > 100:
            self.metrics['sessions'] = self.metrics['sessions'][-100:]
        
        self._save_metrics()
        
        # æ¸…ç©ºä¼šè¯ç¼“å­˜
        self.session_metrics.clear()
