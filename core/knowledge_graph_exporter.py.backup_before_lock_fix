#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
çŸ¥è¯†å›¾è°±å®æ—¶å¯¼å‡ºå™¨ (å¢å¼ºç‰ˆ)
Knowledge Graph Real-time Exporter (Enhanced)

åŠŸèƒ½ï¼š
1. ä»AGIç³»ç»Ÿæ”¶é›†çŸ¥è¯†å›¾è°±æ•°æ®
2. å¢é‡ç´¯ç§¯å†å²èŠ‚ç‚¹ï¼ˆä¸é‡ç½®ï¼‰
3. åˆ†å±‚æ˜¾ç¤ºï¼šæ ¸å¿ƒå±‚ã€ä¸­é—´å±‚ã€å¤–å›´å±‚
4. ä»NeuralMemoryè¯»å–å®Œæ•´å†å²
5. å®šæœŸå¯¼å‡ºåˆ°JSONæ–‡ä»¶ä¾›å¯è§†åŒ–ä½¿ç”¨

Author: AGI System Development Team
Date: 2026-01-17
Version: 2.0.0
"""

import json
import logging
import os
import time
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional, Set
from threading import Thread, Lock
import hashlib
import uuid

logger = logging.getLogger(__name__)


# åˆ†å±‚å®šä¹‰
class NodeLayer:
    """èŠ‚ç‚¹åˆ†å±‚å®šä¹‰"""
    CORE = "core"           # æ ¸å¿ƒå±‚ï¼šå½“å‰æ´»è·ƒï¼ˆæœ€è¿‘5åˆ†é’Ÿï¼‰
    RECENT = "recent"       # ä¸­é—´å±‚ï¼šè¿‘æœŸä½¿ç”¨ï¼ˆ1å°æ—¶å†…ï¼‰
    HISTORY = "history"     # å¤–å›´å±‚ï¼šå†å²è®°å¿†ï¼ˆæ›´æ—©ï¼‰


class FileLock:
    """
    è·¨å¹³å°æ–‡ä»¶é”å®ç°ï¼ˆåŸºäºé”æ–‡ä»¶ï¼‰
    Cross-platform file locking using lock files.

    ä½¿ç”¨æ–¹æ³•:
        with FileLock("data.lock"):
            # æ‰§è¡Œéœ€è¦ç‹¬å è®¿é—®çš„æ“ä½œ
            write_to_file()
    """

    def __init__(self, lock_file: str, timeout: float = 10.0, poll_interval: float = 0.1):
        """
        åˆå§‹åŒ–æ–‡ä»¶é”

        Args:
            lock_file: é”æ–‡ä»¶è·¯å¾„
            timeout: è·å–é”çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            poll_interval: è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰
        """
        self.lock_file = Path(lock_file)
        self.timeout = timeout
        self.poll_interval = poll_interval
        self.lock_id = None
        self.acquired = False

    def acquire(self) -> bool:
        """
        çæ¿Šç˜¯é‘¾å³°å½‡é‚å›¦æ¬¢é–¿?

        Returns:
            é„îˆšæƒé´æ„¬å§›é‘¾å³°å½‡é–¿?
        """
        start_time = time.time()

        while time.time() - start_time < self.timeout:
            try:
                # çæ¿Šç˜¯é’æ¶˜ç¼“é–¿ä½¹æƒæµ è®¹ç´™é˜ç†·ç“™é¿å¶„ç¶”é”›?
                # æµ£è·¨æ•¤ O_EXCL éœ?O_CREAT éå›§ç¹”çº­î†»ç¹šé˜ç†·ç“™é¬?
                fd = os.open(self.lock_file, os.O_CREAT | os.O_EXCL | os.O_WRONLY, 0o644) 

                # éæ¬å†é–¿ä½ºæ®‘éîˆ™ç«´éå›ªç˜‘éœå²ƒç¹˜ç»‹å¬©ä¿Šé­?
                self.lock_id = f"{os.getpid()}:{uuid.uuid4()}:{time.time()}"
                os.write(fd, self.lock_id.encode('utf-8'))
                os.close(fd)

                self.acquired = True
                # logger.debug(f"é‰?File lock acquired: {self.lock_file}")
                return True

            except FileExistsError:
                # é–¿ä½¹æƒæµ è·ºå‡¡ç€›æ¨ºæ¹ªé”›å±¾î—…éŒãƒ¦æ§¸éšï¸¿è´Ÿæ©å›¨æ¹¡é–¿?
                try:
                    lock_content = ""
                    try:
                        # ç’‡è¯²å½‡é–¿ä½¹æƒæµ è·ºå”´ç€¹?
                        with open(self.lock_file, 'r') as f:
                            lock_content = f.read().strip()
                    except Exception:
                        # ç’‡è¯²å½‡æ¾¶è¾«è§¦é”›å å½²é‘³èŠ¥î„œé¦ã„¥å•“éãƒ¦å¨é‰å†®æªºé—‚î‡€î•½é”›å¤›ç´ç’ºå® ç¹ƒéˆî„î‚¼å¦«â‚¬éŒ?
                        time.sleep(self.poll_interval)
                        continue

                    # ç‘™ï½†ç€½é–¿ä½·ä¿Šé­?
                    parts = lock_content.split(':')
                    
                    # 1. å¦«â‚¬éŒãƒ¦æ§¸éšï¹ç§´éƒ?
                    if len(parts) >= 3:
                        try:
                            lock_time = float(parts[2])
                            # æ¿¡å‚›ç‰é–¿ä½½ç§´æ©?0é’å—›æŒ“é”›å²ƒî…»æ¶“çƒ˜æ§¸æ©å›¨æ¹¡é–¿ä½¸è‹Ÿé’çŠ»æ«
                            if time.time() - lock_time > 1800:
                                logger.warning(f"éˆ¿ç‹…ç¬ Removing stale lock file (timeout): {self.lock_file}")
                                try:
                                    self.lock_file.unlink()
                                except OSError:
                                    pass
                                continue
                        except (ValueError, IndexError):
                            pass

                    # 2. å¦«â‚¬éŒãƒ©æ”£æ©æ¶šâ–¼é„îˆšæƒæ©æ¨ºæ¹ªæ©æ„¯î”‘é”›å œç²Unixé”›?
                    try:
                        if not parts or not parts[0]:
                            raise ValueError("Empty PID")
                            
                        pid = int(parts[0])
                        os.kill(pid, 0)  # å¦«â‚¬éŒãƒ¨ç¹˜ç»‹å¬«æ§¸éšï¹€ç“¨é¦?
                    except (ProcessLookupError, ValueError, IndexError):
                        # æ©æ¶šâ–¼æ¶“å¶…ç“¨é¦ã„¦å¨PIDéƒçŠ³æ™¥é”›å±½å½²æµ ãƒ¥åŠé—„ã‚‰æ”£
                        # logger.warning(f"éˆ¿ç‹…ç¬ Removing orphaned lock file: {self.lock_file}")
                        try:
                            self.lock_file.unlink()
                        except OSError:
                            pass # é’çŠ»æ«æ¾¶è¾«è§¦é”›å å½²é‘³å€Ÿî¦é—çŠµæ•¤é”›å¤›ç´è¹‡ç•Œæš
                        continue

                except Exception as e:
                    logger.debug(f"Lock check failed: {e}")

                # ç»›å¤Šç·Ÿéšåº¨å™¸ç’‡?
                time.sleep(self.poll_interval)

            except Exception as e:
                logger.error(f"Failed to acquire lock: {e}")
                return False

        logger.warning(f"éˆ´æ†‹ç¬ Timeout acquiring lock: {self.lock_file}")
        return False

    def release(self):
        """é‡Šæ”¾æ–‡ä»¶é”"""
        if self.acquired and self.lock_file.exists():
            try:
                # éªŒè¯é”æ–‡ä»¶å†…å®¹ï¼ˆé˜²æ­¢è¯¯åˆ å…¶ä»–è¿›ç¨‹çš„é”ï¼‰
                with open(self.lock_file, 'r') as f:
                    lock_content = f.read().strip()

                if lock_content.startswith(str(os.getpid())):
                    self.lock_file.unlink()
                    logger.debug(f"ğŸ”“ File lock released: {self.lock_file}")
                else:
                    logger.warning(f"âš ï¸ Attempted to release another process's lock")

            except Exception as e:
                logger.error(f"Failed to release lock: {e}")
            finally:
                self.acquired = False

    def __enter__(self):
        """æ”¯æŒ with è¯­å¥"""
        if not self.acquire():
            raise TimeoutError(f"Failed to acquire lock within {self.timeout}s: {self.lock_file}")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """æ”¯æŒ with è¯­å¥"""
        self.release()


class KnowledgeGraphExporter:
    """çŸ¥è¯†å›¾è°±å®æ—¶å¯¼å‡ºå™¨ï¼ˆå¢å¼ºç‰ˆï¼‰"""

    def __init__(self,
                 output_dir: str = "data/knowledge",
                 export_interval: int = 30,
                 max_history: int = 100):
        """
        åˆå§‹åŒ–å¯¼å‡ºå™¨

        Args:
            output_dir: è¾“å‡ºç›®å½•
            export_interval: å¯¼å‡ºé—´éš”ï¼ˆç§’ï¼‰
            max_history: æœ€å¤§å†å²è®°å½•æ•°
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.export_interval = export_interval
        self.max_history = max_history

        # ä¸»è¾“å‡ºæ–‡ä»¶
        self.main_file = self.output_dir / "arch_graph.json"
        self.backup_dir = self.output_dir / "backups"
        self.backup_dir.mkdir(exist_ok=True)

        # çº¿ç¨‹å®‰å…¨
        self.lock = Lock()
        self.is_running = False
        self.export_thread: Optional[Thread] = None

        # æ•°æ®ç¼“å­˜ï¼ˆå¢é‡ç´¯ç§¯ï¼Œä¸é‡ç½®ï¼‰
        self.current_data: Dict[str, Any] = {
            "directed": True,
            "multigraph": False,
            "graph": {
                "name": "AGI Knowledge Graph - Layered View",
                "last_updated": datetime.now().isoformat(),
                "version": "2.0.0",
                "description": "Core: Current | Recent: 1hr | History: Earlier"
            },
            "nodes": {},
            "links": [],
            "layer_stats": {
                "core": 0,
                "recent": 0,
                "history": 0
            }
        }

        # èŠ‚ç‚¹ç´¢å¼•ï¼ˆç”¨äºå¿«é€ŸæŸ¥æ‰¾å’Œå»é‡ï¼‰
        self.node_index: Dict[str, Dict[str, Any]] = {}

        # é“¾æ¥ç´¢å¼•ï¼ˆç”¨äºå¿«é€ŸæŸ¥æ‰¾å’Œå»é‡ï¼‰
        self.link_index: Dict[str, Dict[str, Any]] = {}

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "exports_count": 0,
            "last_export_time": None,
            "total_nodes": 0,
            "total_links": 0,
            "core_nodes": 0,
            "recent_nodes": 0,
            "history_nodes": 0
        }

        # å°è¯•åŠ è½½å·²æœ‰çš„å†å²æ•°æ®
        self._load_existing_data()

        logger.info(f"ğŸ“Š çŸ¥è¯†å›¾è°±å¯¼å‡ºå™¨åˆå§‹åŒ–å®Œæˆ (v2.0 - å¢å¼ºç‰ˆ)")
        logger.info(f"   - è¾“å‡ºç›®å½•: {self.output_dir}")
        logger.info(f"   - å¯¼å‡ºé—´éš”: {self.export_interval}ç§’")
        logger.info(f"   - ä¸»æ–‡ä»¶: {self.main_file}")
        logger.info(f"   - å·²æœ‰èŠ‚ç‚¹: {len(self.node_index)}")

    def _load_existing_data(self):
        """åŠ è½½å·²æœ‰çš„å†å²æ•°æ®"""
        try:
            if self.main_file.exists():
                # ä½¿ç”¨æ–‡ä»¶é”è¯»å–ï¼Œé˜²æ­¢åœ¨å†™å…¥è¿‡ç¨‹ä¸­è¯»å–
                lock_file = self.main_file.with_suffix('.lock')
                try:
                    # å°è¯•è·å–é”ï¼Œå¦‚æœè·å–å¤±è´¥åˆ™ç­‰å¾…ä¸€å°æ®µæ—¶é—´åç»§ç»­è¯»å–
                    # æ³¨æ„ï¼šè¯»å–æ“ä½œä¹Ÿå¯ä»¥è·å–é”ä»¥ç¡®ä¿ä¸€è‡´æ€§
                    with FileLock(str(lock_file), timeout=2.0, poll_interval=0.1):
                        with open(self.main_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                except TimeoutError:
                    # å¦‚æœæ— æ³•è·å–é”ï¼Œç›´æ¥è¯»å–ï¼ˆå¯èƒ½è¯»åˆ°ä¸å®Œæ•´æ•°æ®ï¼Œä½†æ€»æ¯”å¤±è´¥å¥½ï¼‰
                    logger.debug(f"æ— æ³•è·å–æ–‡ä»¶é”ï¼Œç›´æ¥è¯»å–: {self.main_file}")
                    with open(self.main_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)

                # åŠ è½½èŠ‚ç‚¹ç´¢å¼•
                if "nodes" in data:
                    nodes = data["nodes"]
                    if isinstance(nodes, dict):
                        # æ–°æ ¼å¼ï¼šå­—å…¸
                        self.node_index = nodes
                    elif isinstance(nodes, list):
                        # æ—§æ ¼å¼ï¼šåˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—å…¸
                        for node in nodes:
                            node_id = node.get("id", node.get("node_id", ""))
                            if node_id:
                                self.node_index[node_id] = node

                # ğŸ†• åŠ è½½é“¾æ¥ç´¢å¼•
                if "links" in data:
                    for link in data["links"]:
                        source = link.get("source")
                        target = link.get("target")
                        if source and target:
                            link_key = f"{source}->{target}"
                            self.link_index[link_key] = link

                logger.info(f"âœ… å·²åŠ è½½ {len(self.node_index)} ä¸ªå†å²èŠ‚ç‚¹, {len(self.link_index)} æ¡é“¾æ¥")

        except Exception as e:
            logger.warning(f"åŠ è½½å†å²æ•°æ®å¤±è´¥: {e}")

    def start(self):
        """å¯åŠ¨è‡ªåŠ¨å¯¼å‡ºçº¿ç¨‹"""
        if self.is_running:
            logger.warning("å¯¼å‡ºå™¨å·²åœ¨è¿è¡Œä¸­")
            return

        self.is_running = True
        self.export_thread = Thread(target=self._export_loop, daemon=True)
        self.export_thread.start()
        logger.info("ğŸ“Š çŸ¥è¯†å›¾è°±è‡ªåŠ¨å¯¼å‡ºå·²å¯åŠ¨")

    def stop(self):
        """åœæ­¢è‡ªåŠ¨å¯¼å‡ºçº¿ç¨‹"""
        self.is_running = False
        if self.export_thread:
            self.export_thread.join(timeout=5)
        logger.info("ğŸ“Š çŸ¥è¯†å›¾è°±è‡ªåŠ¨å¯¼å‡ºå·²åœæ­¢")

    def _export_loop(self):
        """å¯¼å‡ºå¾ªç¯ï¼ˆåå°çº¿ç¨‹ï¼‰"""
        while self.is_running:
            try:
                time.sleep(self.export_interval)
                if self.is_running:
                    self.export_now()
            except Exception as e:
                logger.error(f"å¯¼å‡ºå¾ªç¯å‡ºé”™: {e}")

    def export_now(self) -> bool:
        """
        ç«‹å³æ‰§è¡Œå¯¼å‡º

        Returns:
            æ˜¯å¦å¯¼å‡ºæˆåŠŸ
        """
        with self.lock:
            try:
                # æ›´æ–°æ—¶é—´æˆ³
                self.current_data["graph"]["last_updated"] = datetime.now().isoformat()

                # å°†èŠ‚ç‚¹ç´¢å¼•è½¬æ¢ä¸ºåˆ—è¡¨ï¼ˆæŒ‰å±‚çº§æ’åºï¼‰
                nodes_list = self._convert_index_to_list()

                # æ„å»ºæœ€ç»ˆæ•°æ®
                export_data = {
                    "directed": self.current_data["directed"],
                    "multigraph": self.current_data["multigraph"],
                    "graph": self.current_data["graph"],
                    "nodes": nodes_list,
                    "links": self.current_data["links"],
                    "layer_stats": self.current_data["layer_stats"]
                }

                # æµ£è·¨æ•¤é‚å›¦æ¬¢é–¿ä½ºâ€˜æ·‡æ¿†è‹Ÿé™æˆç•¨é?
                lock_file = self.main_file.with_suffix('.lock')
                with FileLock(str(lock_file), timeout=60.0, poll_interval=0.2):
                    # é˜ç†·ç“™é–æ §å•“éãƒ¯ç´™éå å•“æ¶“å­˜æ¤‚é‚å›¦æ¬¢é”›å±½å•€é–²å¶…æ‡¡éšå¶ç´š
                    temp_file = self.main_file.with_suffix('.tmp')
                    with open(temp_file, 'w', encoding='utf-8') as f:
                        json.dump(export_data, f, indent=2, ensure_ascii=False)

                    # ğŸ”§ [FIX 2026-01-27] Windowså…¼å®¹çš„åŸå­é‡å‘½åï¼ˆå¢å¼ºé‡è¯•æœºåˆ¶å¤„ç†æ–‡ä»¶å ç”¨ï¼‰
                    max_retries = 5
                    retry_delay = 0.5  # ç§’
                    export_success = False

                    for attempt in range(max_retries):
                        try:
                            # å°è¯•1: åŸå­é‡å‘½åï¼ˆæœ€å¿«ï¼‰
                            temp_file.replace(self.main_file)
                            logger.debug(f"âœ… åŸå­é‡å‘½åæˆåŠŸ (å°è¯• {attempt + 1}/{max_retries})")
                            export_success = True
                            break
                        except PermissionError:
                            # å°è¯•2: å¤åˆ¶+åˆ é™¤ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
                            try:
                                shutil.copy2(temp_file, self.main_file)
                                temp_file.unlink()
                                logger.debug(f"âœ… å¤åˆ¶æˆåŠŸ (å°è¯• {attempt + 1}/{max_retries})")
                                export_success = True
                                break
                            except Exception as copy_err:
                                if attempt < max_retries - 1:
                                    logger.warning(f"âš ï¸ æ–‡ä»¶æ“ä½œå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {copy_err}")
                                    logger.debug(f"   ç­‰å¾… {retry_delay}ç§’ åé‡è¯•...")
                                    time.sleep(retry_delay)
                                else:
                                    logger.error(f"âŒ æ–‡ä»¶æ“ä½œå½»åº•å¤±è´¥ (å·²é‡è¯• {max_retries} æ¬¡): {copy_err}")
                                    # ä¿ç•™ä¸´æ—¶æ–‡ä»¶ï¼Œä¸‹æ¬¡å¯¼å‡ºæ—¶ä¼šè¦†ç›–

                    if not export_success:
                        return False  # å¯¼å‡ºå¤±è´¥ï¼Œç›´æ¥è¿”å›

                # æ›´æ–°ç»Ÿè®¡
                self.stats["exports_count"] += 1
                self.stats["last_export_time"] = datetime.now().isoformat()
                self.stats["total_nodes"] = len(nodes_list)
                self.stats["total_links"] = len(self.current_data["links"])
                self.stats["core_nodes"] = self.current_data["layer_stats"]["core"]
                self.stats["recent_nodes"] = self.current_data["layer_stats"]["recent"]
                self.stats["history_nodes"] = self.current_data["layer_stats"]["history"]

                logger.debug(f"âœ… çŸ¥è¯†å›¾è°±å·²å¯¼å‡º: {self.stats['total_nodes']}ä¸ªèŠ‚ç‚¹ "
                           f"(æ ¸å¿ƒ:{self.stats['core_nodes']} "
                           f"è¿‘æœŸ:{self.stats['recent_nodes']} "
                           f"å†å²:{self.stats['history_nodes']}) "
                           f", {self.stats['total_links']}æ¡è¾¹")

                return True

            except Exception as e:
                logger.error(f"å¯¼å‡ºå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return False

    def _convert_index_to_list(self) -> List[Dict[str, Any]]:
        """å°†èŠ‚ç‚¹ç´¢å¼•è½¬æ¢ä¸ºæ’åºåˆ—è¡¨"""
        # æŒ‰å±‚çº§æ’åºï¼šæ ¸å¿ƒ -> è¿‘æœŸ -> å†å²
        layer_priority = {
            NodeLayer.CORE: 0,
            NodeLayer.RECENT: 1,
            NodeLayer.HISTORY: 2
        }

        nodes_list = list(self.node_index.values())

        # æ’åºï¼šå…ˆæŒ‰å±‚çº§ï¼Œå†æŒ‰æ—¶é—´æˆ³ï¼ˆæ–°çš„åœ¨å‰ï¼‰
        nodes_list.sort(key=lambda n: (
            layer_priority.get(n.get("layer", NodeLayer.HISTORY), 3),
            -n.get("last_seen_timestamp", 0)
        ))

        return nodes_list

    def update_from_agi_system(self, agi_engine) -> bool:
        """
        ä»AGIç³»ç»Ÿæ›´æ–°çŸ¥è¯†å›¾è°±æ•°æ®ï¼ˆå¢é‡ç´¯ç§¯ï¼‰

        Args:
            agi_engine: AGI_Life_Engineå®ä¾‹

        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        with self.lock:
            try:
                current_time = datetime.now()
                current_timestamp = current_time.timestamp()

                # 1. æ”¶é›†å½“å‰æ´»è·ƒèŠ‚ç‚¹ï¼ˆæ ¸å¿ƒå±‚ï¼‰
                core_nodes = self._collect_core_nodes(agi_engine, current_timestamp)

                # 2. ä»NeuralMemoryæ”¶é›†å†å²èŠ‚ç‚¹
                self._collect_historical_nodes(agi_engine, current_time)

                # ğŸ†• 2.5. æ”¶é›†å¯¹è¯è®°å¿†èŠ‚ç‚¹
                self._collect_conversation_nodes(current_time)

                # 3. æ›´æ–°èŠ‚ç‚¹å±‚çº§
                self._update_node_layers(current_time)

                # 4. æ”¶é›†é“¾æ¥
                links = self._collect_links(agi_engine)

                # æ›´æ–°æ•°æ®
                self.current_data["links"] = links

                # æ›´æ–°å±‚çº§ç»Ÿè®¡
                self._update_layer_stats()

                logger.debug(f"ğŸ“Š çŸ¥è¯†å›¾è°±æ•°æ®å·²æ›´æ–°: "
                           f"æ€»è®¡{len(self.node_index)}ä¸ªèŠ‚ç‚¹ "
                           f"(æ ¸å¿ƒ:{len(core_nodes)}), "
                           f"{len(links)}æ¡è¾¹")

                return True

            except Exception as e:
                logger.error(f"æ›´æ–°çŸ¥è¯†å›¾è°±æ•°æ®å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return False

    def _collect_core_nodes(self, agi_engine, current_timestamp: float) -> List[str]:
        """æ”¶é›†æ ¸å¿ƒå±‚èŠ‚ç‚¹ï¼ˆå½“å‰æ´»è·ƒï¼‰"""
        core_node_ids = []

        # 1. ä»WorkingMemoryæ”¶é›†æ¦‚å¿µèŠ‚ç‚¹
        if hasattr(agi_engine, 'working_memory') and agi_engine.working_memory:
            wm = agi_engine.working_memory

            if hasattr(wm, 'active_concepts'):
                for concept_id, concept_data in wm.active_concepts.items():
                    node_id = f"concept_{concept_id}"

                    node = {
                        "id": node_id,
                        "node_id": node_id,
                        "type": "concept",
                        "label": concept_data.get("name", concept_id),
                        "context": concept_data.get("context", ""),
                        "activation": concept_data.get("activation", 0.0),
                        "layer": NodeLayer.CORE,
                        "last_seen": datetime.now().isoformat(),
                        "last_seen_timestamp": current_timestamp,
                        "visit_count": concept_data.get("visit_count", 0),
                        "metadata": {
                            "source": "working_memory",
                            "first_seen": concept_data.get("first_seen", datetime.now().isoformat())
                        }
                    }

                    # æ›´æ–°æˆ–æ·»åŠ èŠ‚ç‚¹
                    if node_id in self.node_index:
                        # ä¿ç•™é¦–æ¬¡å‡ºç°æ—¶é—´
                        node["metadata"]["first_seen"] = self.node_index[node_id].get("metadata", {}).get(
                            "first_seen", node["metadata"]["first_seen"]
                        )

                    self.node_index[node_id] = node
                    core_node_ids.append(node_id)

        # 2. ç³»ç»ŸçŠ¶æ€èŠ‚ç‚¹
        if hasattr(agi_engine, 'entropy_regulator') and agi_engine.entropy_regulator:
            er = agi_engine.entropy_regulator

            if hasattr(er, 'entropy_history') and er.entropy_history:
                latest_entropy = er.entropy_history[-1] if er.entropy_history else 0.5

                node_id = "system_state"
                node = {
                    "id": node_id,
                    "node_id": node_id,
                    "type": "system",
                    "label": f"System State (Entropy: {latest_entropy:.2f})",
                    "context": "Current AGI System State",
                    "entropy": latest_entropy,
                    "layer": NodeLayer.CORE,
                    "last_seen": datetime.now().isoformat(),
                    "last_seen_timestamp": current_timestamp,
                    "visit_count": 1,
                    "metadata": {
                        "source": "entropy_regulator",
                        "history_length": len(er.entropy_history) if hasattr(er, 'entropy_history') else 0
                    }
                }

                self.node_index[node_id] = node
                core_node_ids.append(node_id)

        return core_node_ids

    def _collect_historical_nodes(self, agi_engine, current_time: datetime):
        """ä»NeuralMemoryæ”¶é›†å†å²èŠ‚ç‚¹"""
        # 1. ä»ValueNetworkæ”¶é›†å†³ç­–èŠ‚ç‚¹
        if hasattr(agi_engine, 'evolution_controller') and agi_engine.evolution_controller:
            evo = agi_engine.evolution_controller

            if hasattr(evo, 'value_network'):
                vn = evo.value_network

                if hasattr(vn, 'decision_history'):
                    for decision in vn.decision_history[-100:]:  # æœ€è¿‘100ä¸ªå†³ç­–
                        context = decision.get('context', '')
                        node_id = f"decision_{hash(context) & 0xffffffff}"

                        # è§£ææ—¶é—´æˆ³
                        timestamp_str = decision.get("timestamp", "")
                        try:
                            if isinstance(timestamp_str, str):
                                decision_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                            else:
                                decision_time = current_time
                        except:
                            decision_time = current_time

                        # è®¡ç®—æ—¶é—´å·®ï¼ˆç§’ï¼‰
                        time_diff = (current_time - decision_time).total_seconds()

                        # ç¡®å®šå±‚çº§
                        if time_diff < 300:  # 5åˆ†é’Ÿå†…
                            layer = NodeLayer.CORE
                        elif time_diff < 3600:  # 1å°æ—¶å†…
                            layer = NodeLayer.RECENT
                        else:
                            layer = NodeLayer.HISTORY

                        node = {
                            "id": node_id,
                            "node_id": node_id,
                            "type": "decision",
                            "label": decision.get("action", "unknown"),
                            "context": context,
                            "outcome": decision.get("outcome", 0.0),
                            "layer": layer,
                            "last_seen": timestamp_str,
                            "last_seen_timestamp": decision_time.timestamp(),
                            "visit_count": 1,
                            "metadata": {
                                "source": "value_network",
                                "confidence": decision.get("confidence", 0.0)
                            }
                        }

                        # åªæ·»åŠ æ–°èŠ‚ç‚¹æˆ–æ›´æ–°ç°æœ‰èŠ‚ç‚¹
                        if node_id not in self.node_index:
                            self.node_index[node_id] = node

        # 2. ä»NeuralMemoryæ”¶é›†é•¿æœŸè®°å¿†
        if hasattr(agi_engine, 'neural_memory') and agi_engine.neural_memory:
            nm = agi_engine.neural_memory

            # å°è¯•ä»ChromaDBè·å–è®°å¿†
            try:
                if hasattr(nm, 'collection'):
                    # è·å–æœ€è¿‘çš„è®°å¿†ï¼ˆé™åˆ¶æ•°é‡ä»¥é¿å…æ€§èƒ½é—®é¢˜ï¼‰
                    result = nm.collection.get(
                        limit=500,
                        include=["metadatas", "documents"]
                    )

                    if result and result['ids']:
                        for i, memory_id in enumerate(result['ids']):
                            node_id = f"memory_{memory_id}"
                            metadata = result['metadatas'][i] if result['metadatas'] else {}
                            document = result['documents'][i] if result['documents'] else ""

                            # ä»metadataè·å–æ—¶é—´æˆ³
                            created_str = metadata.get("created_at", "")
                            try:
                                if isinstance(created_str, str):
                                    created_time = datetime.fromisoformat(created_str.replace('Z', '+00:00'))
                                else:
                                    created_time = current_time - timedelta(hours=24)  # é»˜è®¤24å°æ—¶å‰
                            except:
                                created_time = current_time - timedelta(hours=24)

                            time_diff = (current_time - created_time).total_seconds()

                            # ç¡®å®šå±‚çº§
                            if time_diff < 300:
                                layer = NodeLayer.CORE
                            elif time_diff < 3600:
                                layer = NodeLayer.RECENT
                            else:
                                layer = NodeLayer.HISTORY

                            node = {
                                "id": node_id,
                                "node_id": node_id,
                                "type": metadata.get("type", "memory"),
                                "label": document[:50] + "..." if len(document) > 50 else document,
                                "context": metadata.get("context", ""),
                                "layer": layer,
                                "last_seen": created_str,
                                "last_seen_timestamp": created_time.timestamp(),
                                "visit_count": metadata.get("visit_count", 1),
                                "metadata": {
                                    "source": "neural_memory",
                                    "vitality": metadata.get("vitality", 0.5),
                                    "importance": metadata.get("importance", 0.5)
                                }
                            }

                            # åªæ·»åŠ æ–°èŠ‚ç‚¹
                            if node_id not in self.node_index:
                                self.node_index[node_id] = node

                            # é™åˆ¶æ€»èŠ‚ç‚¹æ•°ï¼ˆé¿å…æ— é™å¢é•¿ï¼‰
                            if len(self.node_index) > 50000:
                                # ç§»é™¤æœ€å¤è€çš„å†å²èŠ‚ç‚¹
                                self._prune_old_nodes()

            except Exception as e:
                logger.debug(f"ä»NeuralMemoryæ”¶é›†èŠ‚ç‚¹å¤±è´¥: {e}")

    def _collect_conversation_nodes(self, current_time: datetime):
        """
        ğŸ†• ä» user_intents.jsonl æ”¶é›†å¯¹è¯è®°å¿†èŠ‚ç‚¹
        å°†ç”¨æˆ·é—®é¢˜ã€ç–‘é—®å’Œç³»ç»Ÿå“åº”æ˜ å°„ä¸ºçŸ¥è¯†å›¾è°±èŠ‚ç‚¹
        """
        intents_file = Path(__file__).parent.parent / "data" / "intent_bridge" / "user_intents.jsonl"

        if not intents_file.exists():
            logger.debug(f"å¯¹è¯æ„å›¾æ–‡ä»¶ä¸å­˜åœ¨: {intents_file}")
            return

        try:
            current_timestamp = current_time.timestamp()
            intent_states = {}  # è¿½è¸ªæ„å›¾çŠ¶æ€ï¼Œç”¨äºå…³è”é—®é¢˜å’Œå“åº”

            with open(intents_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if not line.strip():
                        continue

                    try:
                        data = json.loads(line)

                        # å¤„ç†æ„å›¾è®°å½•
                        if 'raw_input' in data:
                            intent_id = data.get('id', '')
                            raw_input = data.get('raw_input', '')
                            timestamp = data.get('timestamp', 0)
                            state = data.get('state', 'pending')
                            execution_result = data.get('execution_result')
                            error_message = data.get('error_message')

                            # ç¡®å®šèŠ‚ç‚¹ç±»å‹
                            node_type = self._classify_intent_type(raw_input, state, execution_result, error_message)

                            # è§£ææ—¶é—´
                            try:
                                intent_time = datetime.fromtimestamp(timestamp)
                            except:
                                intent_time = current_time

                            # è®¡ç®—æ—¶é—´å·®å¹¶ç¡®å®šå±‚çº§
                            time_diff = current_timestamp - timestamp
                            if time_diff < 300:  # 5åˆ†é’Ÿå†…
                                layer = NodeLayer.CORE
                            elif time_diff < 3600:  # 1å°æ—¶å†…
                                layer = NodeLayer.RECENT
                            else:
                                layer = NodeLayer.HISTORY

                            # åˆ›å»ºç”¨æˆ·é—®é¢˜èŠ‚ç‚¹
                            question_node_id = f"user_question_{intent_id}"
                            question_node = {
                                "id": question_node_id,
                                "node_id": question_node_id,
                                "type": node_type,
                                "label": raw_input[:60] + "..." if len(raw_input) > 60 else raw_input,
                                "context": raw_input,
                                "layer": layer,
                                "last_seen": intent_time.isoformat(),
                                "last_seen_timestamp": timestamp,
                                "visit_count": 1,
                                "metadata": {
                                    "source": "user_intent",
                                    "intent_state": state,
                                    "intent_id": intent_id,
                                    "question_type": self._detect_question_pattern(raw_input),
                                    "first_seen": intent_time.isoformat()
                                }
                            }

                            # æ›´æ–°æˆ–æ·»åŠ é—®é¢˜èŠ‚ç‚¹
                            if question_node_id in self.node_index:
                                # ä¿ç•™é¦–æ¬¡å‡ºç°æ—¶é—´
                                question_node["metadata"]["first_seen"] = self.node_index[question_node_id].get(
                                    "metadata", {}).get("first_seen", question_node["metadata"]["first_seen"]
                                )

                            self.node_index[question_node_id] = question_node

                            # ä¿å­˜æ„å›¾çŠ¶æ€ç”¨äºåç»­å…³è”
                            if execution_result:
                                intent_states[intent_id] = {
                                    "question_node_id": question_node_id,
                                    "execution_result": execution_result,
                                    "timestamp": timestamp
                                }

                        # å¤„ç†çŠ¶æ€æ›´æ–°è®°å½•ï¼ˆæ‰§è¡Œç»“æœï¼‰
                        elif data.get('type') == 'state_update':
                            intent_id = data.get('intent_id', '')
                            new_state = data.get('new_state', '')
                            execution_result = data.get('execution_result')
                            timestamp = data.get('timestamp', current_timestamp)

                            if intent_id in intent_states or execution_result:
                                # åˆ›å»ºç³»ç»Ÿå“åº”èŠ‚ç‚¹
                                response_node_id = f"system_response_{intent_id}"

                                # æå–æ‰§è¡Œç»“æœæ‘˜è¦
                                if execution_result:
                                    result_summary = execution_result[:100] + "..." if len(execution_result) > 100 else execution_result
                                elif intent_id in intent_states:
                                    result_summary = intent_states[intent_id]['execution_result'][:100] + "..." \
                                        if len(intent_states[intent_id]['execution_result']) > 100 \
                                        else intent_states[intent_id]['execution_result']
                                else:
                                    result_summary = f"State: {new_state}"

                                # ç¡®å®šå±‚çº§
                                time_diff = current_timestamp - timestamp
                                if time_diff < 300:
                                    layer = NodeLayer.CORE
                                elif time_diff < 3600:
                                    layer = NodeLayer.RECENT
                                else:
                                    layer = NodeLayer.HISTORY

                                response_node = {
                                    "id": response_node_id,
                                    "node_id": response_node_id,
                                    "type": "response",
                                    "label": result_summary,
                                    "context": execution_result or f"Intent state: {new_state}",
                                    "layer": layer,
                                    "last_seen": datetime.fromtimestamp(timestamp).isoformat(),
                                    "last_seen_timestamp": timestamp,
                                    "visit_count": 1,
                                    "metadata": {
                                        "source": "system_response",
                                        "intent_id": intent_id,
                                        "intent_state": new_state,
                                        "first_seen": datetime.fromtimestamp(timestamp).isoformat()
                                    }
                                }

                                # æ·»åŠ å“åº”èŠ‚ç‚¹
                                if response_node_id not in self.node_index:
                                    self.node_index[response_node_id] = response_node

                                # åˆ›å»ºé—®é¢˜â†’å“åº”çš„é“¾æ¥ï¼ˆå¦‚æœé—®é¢˜èŠ‚ç‚¹å­˜åœ¨ï¼‰
                                question_node_id = f"user_question_{intent_id}"
                                if question_node_id in self.node_index:
                                    link_key = f"{question_node_id}->{response_node_id}"
                                    if link_key not in self.link_index:
                                        self.link_index[link_key] = {
                                            "source": question_node_id,
                                            "target": response_node_id,
                                            "relation": "answered_by",
                                            "metadata": {
                                                "timestamp": timestamp,
                                                "response_type": new_state
                                            }
                                        }

                    except json.JSONDecodeError as e:
                        logger.debug(f"è§£ææ„å›¾è¡Œå¤±è´¥: {e}")
                        continue

            logger.debug(f"ğŸ“ ä»å¯¹è¯æ„å›¾æ”¶é›†äº† {len([n for n in self.node_index.values() if n.get('metadata', {}).get('source') in ['user_intent', 'system_response']])} ä¸ªå¯¹è¯èŠ‚ç‚¹")

        except Exception as e:
            logger.error(f"æ”¶é›†å¯¹è¯èŠ‚ç‚¹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    def _classify_intent_type(self, raw_input: str, state: str, execution_result: str = None, error_message: str = None) -> str:
        """
        ğŸ†• æ ¹æ®è¾“å…¥å†…å®¹åˆ†ç±»æ„å›¾ç±»å‹
        è¿”å›: 'question', 'doubt', 'request', 'feedback', 'conversation'
        """
        # ç–‘é—®æ¨¡å¼æ£€æµ‹
        doubt_patterns = ['ç–‘é—®', 'éœ€è¦æˆ‘', 'å¸®åŠ©å—', 'æœ‰ä»€ä¹ˆ', 'æ˜¯å¦', 'èƒ½ä¸èƒ½', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•']
        if any(pattern in raw_input for pattern in doubt_patterns):
            return 'doubt'

        # é—®é¢˜æ¨¡å¼æ£€æµ‹
        question_patterns = ['ï¼Ÿ', '?', 'å—', 'å‘¢', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'æ˜¯å¦', 'èƒ½ä¸èƒ½']
        if any(pattern in raw_input for pattern in question_patterns):
            return 'question'

        # åé¦ˆæ¨¡å¼
        feedback_patterns = ['é”™è¯¯', 'å¤±è´¥', 'æ­£ç¡®', 'å¥½çš„', 'åŒæ„', 'ç¡®è®¤']
        if any(pattern in raw_input for pattern in feedback_patterns):
            return 'feedback'

        # é»˜è®¤ä¸ºå¯¹è¯
        return 'conversation'

    def _detect_question_pattern(self, text: str) -> str:
        """
        ğŸ†• æ£€æµ‹é—®é¢˜çš„è¯­ä¹‰æ¨¡å¼
        è¿”å›: 'inquiry', 'confirmation', 'clarification', 'philosophical', 'technical'
        """
        inquiry_patterns = ['ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªäº›', 'å“ªä¸ª']
        if any(pattern in text for pattern in inquiry_patterns):
            return 'inquiry'

        confirmation_patterns = ['æ˜¯å¦', 'èƒ½ä¸èƒ½', 'å¯ä»¥å—', 'ç¡®è®¤', 'åŒæ„']
        if any(pattern in text for pattern in confirmation_patterns):
            return 'confirmation'

        philosophical_patterns = ['æ™ºèƒ½', 'æ„è¯†', 'æœ¬è´¨', 'æ„ä¹‰', 'å­˜åœ¨', 'æ€ç»´']
        if any(pattern in text for pattern in philosophical_patterns):
            return 'philosophical'

        technical_patterns = ['ä»£ç ', 'æ–‡ä»¶', 'ä¿®å¤', 'é”™è¯¯', 'ç³»ç»Ÿ', 'åŠŸèƒ½']
        if any(pattern in text for pattern in technical_patterns):
            return 'technical'

        return 'clarification'

    def _update_node_layers(self, current_time: datetime):
        """æ›´æ–°æ‰€æœ‰èŠ‚ç‚¹çš„å±‚çº§"""
        current_timestamp = current_time.timestamp()

        for node_id, node in self.node_index.items():
            last_seen = node.get("last_seen_timestamp", current_timestamp)
            time_diff = current_timestamp - last_seen

            # æ›´æ–°å±‚çº§
            if time_diff < 300:  # 5åˆ†é’Ÿå†…
                node["layer"] = NodeLayer.CORE
            elif time_diff < 3600:  # 1å°æ—¶å†…
                node["layer"] = NodeLayer.RECENT
            else:
                node["layer"] = NodeLayer.HISTORY

    def _update_layer_stats(self):
        """æ›´æ–°å±‚çº§ç»Ÿè®¡"""
        stats = {
            NodeLayer.CORE: 0,
            NodeLayer.RECENT: 0,
            NodeLayer.HISTORY: 0
        }

        for node in self.node_index.values():
            layer = node.get("layer", NodeLayer.HISTORY)
            stats[layer] = stats.get(layer, 0) + 1

        self.current_data["layer_stats"] = stats

    def _prune_old_nodes(self, max_nodes: int = 50000):
        """ä¿®å‰ªæœ€å¤è€çš„å†å²èŠ‚ç‚¹"""
        # æŒ‰last_seen_timestampæ’åºï¼Œç§»é™¤æœ€å¤è€çš„HISTORYå±‚èŠ‚ç‚¹
        history_nodes = [
            (node_id, node)
            for node_id, node in self.node_index.items()
            if node.get("layer") == NodeLayer.HISTORY
        ]

        history_nodes.sort(key=lambda x: x[1].get("last_seen_timestamp", 0))

        # ç§»é™¤æœ€å¤è€çš„èŠ‚ç‚¹
        nodes_to_remove = len(self.node_index) - max_nodes
        for i in range(min(nodes_to_remove, len(history_nodes))):
            node_id = history_nodes[i][0]
            del self.node_index[node_id]

        logger.debug(f"ä¿®å‰ªäº† {nodes_to_remove} ä¸ªå†å²èŠ‚ç‚¹")

    def _collect_links(self, agi_engine) -> List[Dict[str, Any]]:
        """æ”¶é›†è¾¹ï¼ˆå…³ç³»ï¼‰æ•°æ®"""
        links = []
        # è·å–å½“å‰æ‰€æœ‰èŠ‚ç‚¹IDçš„é›†åˆï¼Œç”¨äºéªŒè¯è¾¹çš„æœ‰æ•ˆæ€§
        valid_node_ids = set(self.node_index.keys())

        # 1. ä» TopologicalMemoryCore æå–æ‹“æ‰‘è¿æ¥ï¼ˆä»…ä¿ç•™è¿æ¥åˆ°æœ‰æ•ˆèŠ‚ç‚¹çš„è¾¹ï¼‰
        if hasattr(agi_engine, 'topology_memory') and agi_engine.topology_memory:
            topology = agi_engine.topology_memory
            if hasattr(topology, 'graph'):
                adj = topology.graph
                for source_idx, edges in adj.items():
                    for edge in edges:
                        source_id = f"topo_node_{source_idx}"
                        target_id = f"topo_node_{edge.to_idx}"
                        # åªæ·»åŠ ä¸¤ç«¯èŠ‚ç‚¹éƒ½å­˜åœ¨çš„è¾¹
                        if source_id in valid_node_ids and target_id in valid_node_ids:
                            links.append({
                                "source": source_id,
                                "target": target_id,
                                "type": "topological",
                                "weight": float(edge.weight),
                                "kind": edge.kind,
                                "from_port": edge.from_port,
                                "to_port": edge.to_port,
                                "usage": edge.usage
                            })

        # 2. ä» BiologicalMemorySystem æå–ç”Ÿç‰©è®°å¿†æ‹“æ‰‘ï¼ˆä»…ä¿ç•™è¿æ¥åˆ°æœ‰æ•ˆèŠ‚ç‚¹çš„è¾¹ï¼‰
        if hasattr(agi_engine, 'biological_memory') and agi_engine.biological_memory:
            biomemory = agi_engine.biological_memory
            if hasattr(biomemory, 'topology') and biomemory.topology:
                bio_topology = biomemory.topology
                if hasattr(bio_topology, 'graph'):
                    bio_adj = bio_topology.graph
                    for source_idx, edges in bio_adj.items():
                        for edge in edges:
                            source_id = f"bio_node_{source_idx}"
                            target_id = f"bio_node_{edge.to_idx}"
                            # åªæ·»åŠ ä¸¤ç«¯èŠ‚ç‚¹éƒ½å­˜åœ¨çš„è¾¹
                            if source_id in valid_node_ids and target_id in valid_node_ids:
                                links.append({
                                    "source": source_id,
                                    "target": target_id,
                                    "type": "biological",
                                    "weight": float(edge.weight),
                                    "kind": edge.kind,
                                    "from_port": edge.from_port,
                                    "to_port": edge.to_port,
                                    "usage": edge.usage
                                })

        # 3. ä» Knowledge Graph (NetworkX) æå–æ¦‚å¿µå…³ç³»ï¼ˆä¸»è¦æ¥æºï¼‰
        if hasattr(agi_engine, 'memory') and agi_engine.memory:
            kg = agi_engine.memory
            if hasattr(kg, 'graph'):
                try:
                    # NetworkX graph edges
                    for source, target, edge_data in kg.graph.edges(data=True):
                        # åˆ›å»ºæ›´å‹å¥½çš„èŠ‚ç‚¹ID
                        source_id = str(source)
                        target_id = str(target)
                        # åªæ·»åŠ ä¸¤ç«¯èŠ‚ç‚¹éƒ½å­˜åœ¨çš„è¾¹
                        if source_id in valid_node_ids and target_id in valid_node_ids:
                            links.append({
                                "source": source_id,
                                "target": target_id,
                                "type": "knowledge_graph",
                                "weight": float(edge_data.get("weight", 1.0)),
                                "relation": edge_data.get("relation", "related")
                            })
                except Exception as e:
                    logger.debug(f"ä»Knowledge Graphæå–è¾¹å¤±è´¥: {e}")

        # ğŸ†• 4. ä» link_index æ·»åŠ å¯¹è¯é“¾æ¥ï¼ˆé—®é¢˜â†’å“åº”ï¼‰
        for link_key, link_data in self.link_index.items():
            source = link_data.get("source")
            target = link_data.get("target")
            # åªæ·»åŠ ä¸¤ç«¯èŠ‚ç‚¹éƒ½å­˜åœ¨çš„è¾¹
            if source in valid_node_ids and target in valid_node_ids:
                links.append({
                    "source": source,
                    "target": target,
                    "relation": link_data.get("relation", "answered_by"),
                    "metadata": link_data.get("metadata", {})
                })

        logger.info(f"ğŸ“Š æå–äº† {len(links)} æ¡è¾¹ (æ‹“æ‰‘è¿æ¥: {len([l for l in links if l.get('type') == 'topological'])}, "
                   f"ç”Ÿç‰©è®°å¿†: {len([l for l in links if l.get('type') == 'biological'])}, "
                   f"çŸ¥è¯†å›¾è°±: {len([l for l in links if l.get('type') == 'knowledge_graph'])}, "
                   f"ğŸ†• å¯¹è¯é“¾æ¥: {len([l for l in links if l.get('relation') == 'answered_by'])})")

        return links

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return self.stats.copy()

    def get_current_data(self) -> Dict[str, Any]:
        """è·å–å½“å‰æ•°æ®ï¼ˆç”¨äºAPIæŸ¥è¯¢ï¼‰"""
        with self.lock:
            nodes_list = self._convert_index_to_list()
            return {
                "graph": self.current_data["graph"],
                "nodes": nodes_list,
                "links": self.current_data["links"],
                "layer_stats": self.current_data["layer_stats"]
            }

    def create_backup(self):
        """åˆ›å»ºå¤‡ä»½"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_file = self.backup_dir / f"arch_graph_{timestamp}.json"

            # å¯¼å‡ºå½“å‰æ•°æ®åˆ°å¤‡ä»½
            nodes_list = self._convert_index_to_list()
            backup_data = {
                "directed": self.current_data["directed"],
                "multigraph": self.current_data["multigraph"],
                "graph": self.current_data["graph"],
                "nodes": nodes_list,
                "links": self.current_data["links"],
                "layer_stats": self.current_data["layer_stats"]
            }

            with open(backup_file, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, indent=2, ensure_ascii=False)

            logger.info(f"âœ… å¤‡ä»½å·²åˆ›å»º: {backup_file}")

            # æ¸…ç†æ—§å¤‡ä»½
            self._cleanup_old_backups()

        except Exception as e:
            logger.error(f"åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")

    def _cleanup_old_backups(self):
        """æ¸…ç†æ—§å¤‡ä»½ï¼Œä¿ç•™æœ€è¿‘çš„max_historyä¸ª"""
        try:
            backups = sorted(self.backup_dir.glob("arch_graph_*.json"),
                           key=os.path.getmtime,
                           reverse=True)

            if len(backups) > self.max_history:
                for old_backup in backups[self.max_history:]:
                    old_backup.unlink()
                    logger.debug(f"å·²åˆ é™¤æ—§å¤‡ä»½: {old_backup}")

        except Exception as e:
            logger.warning(f"æ¸…ç†æ—§å¤‡ä»½å¤±è´¥: {e}")


# ä¾¿æ·å‡½æ•°
def create_exporter(output_dir: str = "data/knowledge",
                   export_interval: int = 30) -> KnowledgeGraphExporter:
    """åˆ›å»ºçŸ¥è¯†å›¾è°±å¯¼å‡ºå™¨å®ä¾‹"""
    return KnowledgeGraphExporter(
        output_dir=output_dir,
        export_interval=export_interval
    )
