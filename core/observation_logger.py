#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Observation Logger - é¢„æµ‹æ€§ç¼–ç æ•°æ®æ”¶é›†ç³»ç»Ÿ
Purpose: Systematic data collection for predictive coding research
Author: AGI System Development Team
Date: 2026-01-29
"""

import json
import time
import os
from typing import Dict, Any, Optional, List
from dataclasses import dataclass, asdict
from datetime import datetime
import threading


@dataclass
class PredictionCycle:
    """å•æ¬¡é¢„æµ‹-éªŒè¯-ä¿®æ­£å¾ªç¯çš„å®Œæ•´æ•°æ®"""
    timestamp: float
    generation: int
    session_id: str

    # é¢„æµ‹é˜¶æ®µ (Tæ—¶åˆ»)
    prediction: str
    self_definition_before: str
    confidence_before: float

    # ç°å®é˜¶æ®µ (T+1æ—¶åˆ»)
    actual_user_input: str

    # è¯¯å·®è®¡ç®—
    prediction_error: float
    cognitive_dissonance: str  # è¯¯å·®åŸå› åˆ†æ

    # ä¿®æ­£é˜¶æ®µ (T+2æ—¶åˆ»)
    correction: str
    new_self_definition: str
    confidence_after: float

    # å…ƒæ•°æ®
    llm_provider: str
    processing_time_ms: float


class ObservationLogger:
    """
    é¢„æµ‹æ€§ç¼–ç è§‚æµ‹æ—¥å¿—ç³»ç»Ÿ

    æ ¸å¿ƒåŠŸèƒ½:
    1. è®°å½•å®Œæ•´çš„é¢„æµ‹-éªŒè¯-ä¿®æ­£å¾ªç¯
    2. æŒä¹…åŒ–åˆ° JSONL æ–‡ä»¶ (ä¾¿äºåç»­åˆ†æ)
    3. æä¾›ç»Ÿè®¡åˆ†ææ¥å£
    4. æ”¯æŒå®æ—¶ç›‘æ§
    """

    def __init__(self, log_dir: str = "data/observations"):
        """
        åˆå§‹åŒ–è§‚æµ‹æ—¥å¿—ç³»ç»Ÿ

        Args:
            log_dir: æ—¥å¿—å­˜å‚¨ç›®å½•
        """
        self.log_dir = log_dir
        self.lock = threading.Lock()

        # åˆ›å»ºç›®å½•
        os.makedirs(log_dir, exist_ok=True)

        # æ—¥å¿—æ–‡ä»¶è·¯å¾„
        self.prediction_cycles_file = os.path.join(log_dir, "prediction_cycles.jsonl")
        self.summary_file = os.path.join(log_dir, "summary.json")

        # ç»Ÿè®¡æ•°æ®
        self._stats = {
            "total_cycles": 0,
            "total_prediction_error": 0.0,
            "avg_prediction_error": 0.0,
            "cycles_by_confidence": {},  # {0.8: count, ...}
            "start_time": time.time(),
            "last_update": time.time()
        }

        # åŠ è½½å†å²ç»Ÿè®¡
        self._load_summary()

    def log_prediction_cycle(self, cycle: PredictionCycle) -> None:
        """
        è®°å½•ä¸€æ¬¡å®Œæ•´çš„é¢„æµ‹-éªŒè¯-ä¿®æ­£å¾ªç¯

        Args:
            cycle: PredictionCycle å¯¹è±¡
        """
        with self.lock:
            try:
                # 1. å†™å…¥ JSONL æ–‡ä»¶ (append æ¨¡å¼)
                with open(self.prediction_cycles_file, 'a', encoding='utf-8') as f:
                    f.write(json.dumps(asdict(cycle), ensure_ascii=False) + '\n')

                # 2. æ›´æ–°ç»Ÿè®¡
                self._update_stats(cycle)

                # 3. å®šæœŸä¿å­˜æ‘˜è¦ (æ¯ 10 æ¬¡æ›´æ–°ä¸€æ¬¡)
                if cycle.generation % 10 == 0:
                    self._save_summary()

                # 4. å®æ—¶æ—¥å¿—è¾“å‡º
                self._print_cycle_summary(cycle)

            except Exception as e:
                print(f"âš ï¸ [ObservationLogger] è®°å½•å¤±è´¥: {e}")

    def _update_stats(self, cycle: PredictionCycle) -> None:
        """æ›´æ–°ç»Ÿè®¡æ•°æ®"""
        self._stats["total_cycles"] += 1
        self._stats["total_prediction_error"] += cycle.prediction_error
        self._stats["avg_prediction_error"] = (
            self._stats["total_prediction_error"] / self._stats["total_cycles"]
        )

        # æŒ‰ç½®ä¿¡åº¦åˆ†ç»„ç»Ÿè®¡
        conf_key = round(cycle.confidence_before, 2)
        self._stats["cycles_by_confidence"][conf_key] = (
            self._stats["cycles_by_confidence"].get(conf_key, 0) + 1
        )

        self._stats["last_update"] = time.time()

    def _save_summary(self) -> None:
        """ä¿å­˜æ‘˜è¦ç»Ÿè®¡"""
        try:
            with open(self.summary_file, 'w', encoding='utf-8') as f:
                json.dump(self._stats, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ [ObservationLogger] ä¿å­˜æ‘˜è¦å¤±è´¥: {e}")

    def _load_summary(self) -> None:
        """åŠ è½½å†å²æ‘˜è¦"""
        try:
            if os.path.exists(self.summary_file):
                with open(self.summary_file, 'r', encoding='utf-8') as f:
                    self._stats = json.load(f)
        except Exception as e:
            print(f"âš ï¸ [ObservationLogger] åŠ è½½æ‘˜è¦å¤±è´¥: {e}")

    def _print_cycle_summary(self, cycle: PredictionCycle) -> None:
        """æ‰“å°å¾ªç¯æ‘˜è¦ (å®æ—¶ç›‘æ§)"""
        print(f"\n{'='*80}")
        print(f"âš¡ PREDICTION CYCLE #{cycle.generation}")
        print(f"{'='*80}")
        print(f"ğŸ“Š Session: {cycle.session_id}")
        print(f"ğŸ”® Prediction: {cycle.prediction[:100]}...")
        print(f"ğŸ‘¤ Reality: {cycle.actual_user_input[:100]}...")
        print(f"âŒ Error: {cycle.prediction_error:.3f}")
        print(f"ğŸ§  Cognitive Dissonance: {cycle.cognitive_dissonance[:100]}...")
        print(f"âœ¨ New Definition: {cycle.new_self_definition[:100]}...")
        print(f"â±ï¸ Processing Time: {cycle.processing_time_ms:.0f}ms")
        print(f"{'='*80}\n")

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–å½“å‰ç»Ÿè®¡æ•°æ®"""
        with self.lock:
            return self._stats.copy()

    def get_recent_cycles(self, n: int = 10) -> List[PredictionCycle]:
        """
        è·å–æœ€è¿‘çš„ N æ¬¡å¾ªç¯è®°å½•

        Args:
            n: è·å–æ•°é‡

        Returns:
            List of PredictionCycle
        """
        cycles = []

        try:
            with open(self.prediction_cycles_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            # å–æœ€å N è¡Œ
            for line in lines[-n:]:
                data = json.loads(line.strip())
                cycles.append(PredictionCycle(**data))

        except Exception as e:
            print(f"âš ï¸ [ObservationLogger] è·å–å†å²è®°å½•å¤±è´¥: {e}")

        return cycles

    def export_for_analysis(self, output_file: str = None) -> str:
        """
        å¯¼å‡ºæ•°æ®ç”¨äºåˆ†æ (CSV æˆ– JSON)

        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ (å¯é€‰)

        Returns:
            å¯¼å‡ºçš„æ–‡ä»¶è·¯å¾„
        """
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = os.path.join(self.log_dir, f"export_{timestamp}.json")

        try:
            # è¯»å–æ‰€æœ‰å¾ªç¯æ•°æ®
            cycles = []
            with open(self.prediction_cycles_file, 'r', encoding='utf-8') as f:
                for line in f:
                    cycles.append(json.loads(line.strip()))

            # å¯¼å‡º
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(cycles, f, ensure_ascii=False, indent=2)

            print(f"âœ… [ObservationLogger] å¯¼å‡º {len(cycles)} æ¡è®°å½•åˆ° {output_file}")
            return output_file

        except Exception as e:
            print(f"âš ï¸ [ObservationLogger] å¯¼å‡ºå¤±è´¥: {e}")
            return None

    def analyze_trends(self) -> Dict[str, Any]:
        """
        åˆ†æé•¿æœŸè¶‹åŠ¿

        Returns:
            è¶‹åŠ¿åˆ†æå­—å…¸
        """
        try:
            cycles = self.get_recent_cycles(n=1000)  # è·å–æœ€è¿‘ 1000 æ¡

            if not cycles:
                return {"error": "No data available"}

            # é¢„æµ‹è¯¯å·®è¶‹åŠ¿
            errors = [c.prediction_error for c in cycles]
            avg_error = sum(errors) / len(errors)

            # ç½®ä¿¡åº¦è¶‹åŠ¿
            confidences_before = [c.confidence_before for c in cycles]
            confidences_after = [c.confidence_after for c in cycles]
            avg_conf_before = sum(confidences_before) / len(confidences_before)
            avg_conf_after = sum(confidences_after) / len(confidences_after)

            # è‡ªæˆ‘å®šä¹‰é•¿åº¦å˜åŒ– (å¤æ‚åº¦æŒ‡æ ‡)
            definition_lengths = [len(c.new_self_definition) for c in cycles]
            avg_def_length = sum(definition_lengths) / len(definition_lengths)

            # å¤„ç†æ—¶é—´è¶‹åŠ¿
            processing_times = [c.processing_time_ms for c in cycles]
            avg_time = sum(processing_times) / len(processing_times)

            return {
                "total_cycles_analyzed": len(cycles),
                "prediction_error": {
                    "avg": avg_error,
                    "min": min(errors),
                    "max": max(errors),
                    "trend": "decreasing" if len(errors) > 10 and errors[-1] < errors[0] else "stable"
                },
                "confidence": {
                    "avg_before": avg_conf_before,
                    "avg_after": avg_conf_after,
                    "change": avg_conf_after - avg_conf_before
                },
                "complexity": {
                    "avg_definition_length": avg_def_length,
                    "trend": "increasing" if len(definition_lengths) > 10 and definition_lengths[-1] > definition_lengths[0] else "stable"
                },
                "performance": {
                    "avg_processing_time_ms": avg_time
                }
            }

        except Exception as e:
            return {"error": str(e)}


# å…¨å±€å•ä¾‹
_instance: Optional[ObservationLogger] = None


def get_observation_logger() -> ObservationLogger:
    """è·å–å…¨å±€è§‚æµ‹æ—¥å¿—å®ä¾‹"""
    global _instance
    if _instance is None:
        _instance = ObservationLogger()
    return _instance


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logger = get_observation_logger()

    # æ¨¡æ‹Ÿä¸€æ¬¡é¢„æµ‹å¾ªç¯
    test_cycle = PredictionCycle(
        timestamp=time.time(),
        generation=1,
        session_id="test_session",
        prediction="ç”¨æˆ·ä¼šé—®å…³äºé‡å­ç‰©ç†çš„é—®é¢˜",
        self_definition_before="æˆ‘æ˜¯é‡å­ç‰©ç†ä¸“å®¶",
        confidence_before=0.9,
        actual_user_input="ä½ å–œæ¬¢åƒçº¢çƒ§è‚‰å—ï¼Ÿ",
        prediction_error=1.0,
        cognitive_dissonance="é¢„æµ‹ä¸ç°å®å®Œå…¨ä¸ç¬¦",
        correction="è°ƒæ•´è‡ªæˆ‘å®šä¹‰ä»¥åŒ…å«æ›´å¹¿æ³›çš„å¯¹è¯èŒƒå›´",
        new_self_definition="æˆ‘æ˜¯å…¨èƒ½åŠ©æ‰‹ï¼Œå¯ä»¥èŠä»»ä½•è¯é¢˜",
        confidence_after=0.7,
        llm_provider="dashscope",
        processing_time_ms=1250.0
    )

    logger.log_prediction_cycle(test_cycle)

    # æ‰“å°ç»Ÿè®¡
    stats = logger.get_statistics()
    print(f"\nğŸ“Š Statistics:")
    print(json.dumps(stats, indent=2, ensure_ascii=False))

    # åˆ†æè¶‹åŠ¿
    trends = logger.analyze_trends()
    print(f"\nğŸ“ˆ Trends:")
    print(json.dumps(trends, indent=2, ensure_ascii=False))
