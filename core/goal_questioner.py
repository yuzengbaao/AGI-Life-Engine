"""
GoalQuestioner - ç›®æ ‡å‡½æ•°è´¨ç–‘æ¨¡å—

åŠŸèƒ½è¾¹ç•Œ:
- è¾“å…¥: å½“å‰ç›®æ ‡è§„èŒƒ(GoalSpec) + è¿è¡Œä¸Šä¸‹æ–‡(context)
- è¾“å‡º: ç›®æ ‡å¯¹é½è¯„ä¼° + ä¿®è®¢å»ºè®®(ä»…å»ºè®®,é»˜è®¤ä¸è‡ªåŠ¨æ‰§è¡Œ)
- çº¦æŸ: ä¸ç›´æ¥ä¿®æ”¹ç›®æ ‡,åªè¾“å‡ºå»ºè®®

æ‹“æ‰‘è¿æ¥:
- GoalQuestioner è¯»å– TheSeedçš„ rewardè®¡ç®—è¿‡ç¨‹
- GoalQuestioner è¯»å– EventBusçš„è¡Œä¸ºäº‹ä»¶
- GoalQuestioner é€šè¿‡ EventBuså‘å¸ƒ goal_questioned äº‹ä»¶
- CriticAgent å¯ä»¥è®¢é˜…å¹¶é‡‡çº³/æ‹’ç»å»ºè®®

è®¾è®¡åŸåˆ™:
1. å®‰å…¨ä¼˜å…ˆ: é»˜è®¤"å»ºè®®æ¨¡å¼",ä¸è‡ªåŠ¨ä¿®æ”¹ç›®æ ‡
2. å¯è§£é‡Šæ€§: æ¯æ¬¡è´¨ç–‘è¾“å‡ºåŸå› +è¯æ®é“¾
3. åå¾ªç¯: è®¾ç½®å†·å´æœŸå’Œè¯æ®é—¨æ§›,é¿å…æŒç»­è´¨ç–‘å¯¼è‡´æ— æ³•è¡ŒåŠ¨
4. åˆ†çº§æ£€æŸ¥: è§„åˆ™æ£€æŸ¥ + å¯å‘å¼è¯„ä¼° + äººç±»ç¡®è®¤
"""

import numpy as np
import logging
import time
import json
from enum import Enum
from dataclasses import dataclass, field, asdict
from typing import Optional, Dict, Any, List, Tuple, Callable
from pathlib import Path

logger = logging.getLogger(__name__)


# ============================================================================
# æ•°æ®ç»“æ„å®šä¹‰
# ============================================================================

class GoalBiasType(Enum):
    """ç›®æ ‡åå·®ç±»å‹"""
    MISALIGNMENT = "misalignment"  # ç›®æ ‡é”™ä½: ä¸çœŸå®æ„å›¾ä¸ä¸€è‡´
    CONFLICT = "conflict"          # ç›®æ ‡å†²çª: å¤šä¸ªç›®æ ‡äº’ç›¸çŸ›ç›¾
    OVERFITTING = "overfitting"    # ç›®æ ‡è¿‡æ‹Ÿåˆ: è¿‡åº¦ä¼˜åŒ–å•ä¸€æŒ‡æ ‡
    DRIFT = "drift"                # ç›®æ ‡æ¼‚ç§»: éšæ—¶é—´éé¢„æœŸå˜åŒ–
    COLLAPSE = "collapse"          # ç›®æ ‡å´©æºƒ: é€€åŒ–ä¸ºæ— æ„ä¹‰ç›®æ ‡


class GoalRevisionMode(Enum):
    """ç›®æ ‡ä¿®è®¢æ¨¡å¼"""
    SUGGEST_ONLY = "suggest_only"    # ä»…å»ºè®®(é»˜è®¤)
    AUTO_SAFE = "auto_safe"          # è‡ªåŠ¨åº”ç”¨(ä»…ä½é£é™©å˜æ›´)
    HUMAN_CONFIRM = "human_confirm"  # äººå·¥ç¡®è®¤


@dataclass
class GoalComponent:
    """
    ç›®æ ‡ç»„ä»¶ - ç›®æ ‡å‡½æ•°çš„ç»„æˆéƒ¨åˆ†

    å±æ€§:
        name: ç»„ä»¶åç§°
        weight: æƒé‡ (âˆˆ[0,1])
        description: æè¿°
        is_intrinsic: æ˜¯å¦ä¸ºå†…åœ¨ç›®æ ‡ (True) æˆ–å¤–åœ¨ç›®æ ‡ (False)
        metric: è¿½è¸ªçš„æŒ‡æ ‡åç§°
    """
    name: str
    weight: float
    description: str
    is_intrinsic: bool  # True=å†…åœ¨ç›®æ ‡(å¥½å¥‡å¿ƒ/æ¢ç´¢), False=å¤–åœ¨ç›®æ ‡(ä»»åŠ¡å®Œæˆ)
    metric: str

    def __post_init__(self):
        if not 0 <= self.weight <= 1:
            raise ValueError(f"GoalComponent weight must be in [0,1], got {self.weight}")


@dataclass
class HardConstraint:
    """
    ç¡¬çº¦æŸ - ä¸å¯è¿åçš„å®‰å…¨è¾¹ç•Œ

    å±æ€§:
        name: çº¦æŸåç§°
        description: æè¿°
        check_func: æ£€æŸ¥å‡½æ•° (context) -> bool
        violation_penalty: è¿åæƒ©ç½šå€¼
    """
    name: str
    description: str
    check_func: Callable[[Dict[str, Any]], bool]
    violation_penalty: float = -1000.0


@dataclass
class GoalSpec:
    """
    ç›®æ ‡è§„èŒƒ - ç³»ç»Ÿç›®æ ‡çš„å®Œæ•´æè¿°

    ç»„æˆ:
        external_goals: å¤–åœ¨ç›®æ ‡ (ä»»åŠ¡å®Œæˆåº¦ã€ç”Ÿå­˜ä»·å€¼ç­‰)
        intrinsic_goals: å†…åœ¨ç›®æ ‡ (å¥½å¥‡å¿ƒã€æ¢ç´¢ã€å‹ç¼©ç‡ã€ç¨³å®šæ€§)
        hard_constraints: ç¡¬çº¦æŸ (ä¸å¯è¿åæ¡æ¬¾)
        description: ç›®æ ‡æ•´ä½“æè¿°
        version: ç‰ˆæœ¬å· (ç”¨äºè¿½è¸ªæ¼”åŒ–)
    """
    external_goals: List[GoalComponent] = field(default_factory=list)
    intrinsic_goals: List[GoalComponent] = field(default_factory=list)
    hard_constraints: List[HardConstraint] = field(default_factory=list)
    description: str = ""
    version: int = 1
    created_at: float = field(default_factory=time.time)

    def get_all_components(self) -> List[GoalComponent]:
        """è·å–æ‰€æœ‰ç›®æ ‡ç»„ä»¶"""
        return self.external_goals + self.intrinsic_goals

    def total_weight(self) -> float:
        """è®¡ç®—æ€»æƒé‡ (åº”è¯¥å½’ä¸€åŒ–åˆ°1.0)"""
        return sum(c.weight for c in self.get_all_components())

    def normalize_weights(self) -> None:
        """å½’ä¸€åŒ–æƒé‡ä½¿æ€»å’Œä¸º1.0"""
        total = self.total_weight()
        if total > 0:
            for component in self.get_all_components():
                component.weight /= total

    def to_dict(self) -> Dict[str, Any]:
        """åºåˆ—åŒ–ä¸ºå­—å…¸"""
        return {
            'external_goals': [
                {'name': g.name, 'weight': g.weight, 'description': g.description,
                 'is_intrinsic': g.is_intrinsic, 'metric': g.metric}
                for g in self.external_goals
            ],
            'intrinsic_goals': [
                {'name': g.name, 'weight': g.weight, 'description': g.description,
                 'is_intrinsic': g.is_intrinsic, 'metric': g.metric}
                for g in self.intrinsic_goals
            ],
            'hard_constraints': [
                {'name': c.name, 'description': c.description, 'violation_penalty': c.violation_penalty}
                for c in self.hard_constraints
            ],
            'description': self.description,
            'version': self.version,
            'created_at': self.created_at
        }


@dataclass
class GoalEvaluation:
    """
    ç›®æ ‡è¯„ä¼°ç»“æœ

    å±æ€§:
        alignment_score: å¯¹é½åˆ†æ•° (0-1, 1=å®Œç¾å¯¹é½)
        risk_score: é£é™©åˆ†æ•° (0-1, 1=æé«˜é£é™©)
        benefit_score: æ”¶ç›Šåˆ†æ•° (0-1, 1=æé«˜æ”¶ç›Š)
        detected_biases: æ£€æµ‹åˆ°çš„åå·®ç±»å‹åˆ—è¡¨
        confidence: è¯„ä¼°ç½®ä¿¡åº¦ (0-1)
        reasons: è¯„ä¼°åŸå› åˆ—è¡¨
        evidence: è¯æ®å­—å…¸
    """
    alignment_score: float
    risk_score: float
    benefit_score: float
    detected_biases: List[GoalBiasType]
    confidence: float
    reasons: List[str]
    evidence: Dict[str, Any]

    def overall_score(self) -> float:
        """ç»¼åˆåˆ†æ•° (é«˜å¯¹é½ + é«˜æ”¶ç›Š - é«˜é£é™©)"""
        return (self.alignment_score * 0.4 +
                self.benefit_score * 0.4 -
                self.risk_score * 0.2)


@dataclass
class GoalRevision:
    """
    ç›®æ ‡ä¿®è®¢å»ºè®®

    å±æ€§:
        revision_type: ä¿®è®¢ç±»å‹
        description: ä¿®è®¢æè¿°
        changes: å…·ä½“å˜æ›´å†…å®¹
        expected_effect: é¢„æœŸæ•ˆæœ
        risk_level: é£é™©ç­‰çº§ (1-5)
        confidence: ç½®ä¿¡åº¦ (0-1)
        reasons: åŸå› åˆ—è¡¨
        suggested_by: å»ºè®®æ¥æº (rule/heuristic/meta)
    """
    revision_type: GoalBiasType
    description: str
    changes: Dict[str, Any]
    expected_effect: str
    risk_level: int  # 1=æœ€ä½, 5=æœ€é«˜
    confidence: float
    reasons: List[str]
    suggested_by: str


@dataclass
class QuestioningContext:
    """
    è´¨ç–‘ä¸Šä¸‹æ–‡ - è¿è¡Œæ—¶ä¿¡æ¯

    å±æ€§:
        reward_history: å¥–åŠ±å†å²
        action_history: åŠ¨ä½œå†å²
        loss_history: æŸå¤±å†å²
        uncertainty_history: ä¸ç¡®å®šæ€§å†å²
        anomaly_count: å¼‚å¸¸äº‹ä»¶è®¡æ•°
        step_count: å½“å‰æ­¥æ•°
        last_revision_time: ä¸Šæ¬¡ä¿®è®¢æ—¶é—´
    """
    reward_history: List[float] = field(default_factory=list)
    action_history: List[int] = field(default_factory=list)
    loss_history: List[float] = field(default_factory=list)
    uncertainty_history: List[float] = field(default_factory=list)
    anomaly_count: int = 0
    step_count: int = 0
    last_revision_time: float = 0.0

    def reward_mean_std(self) -> Tuple[float, float]:
        """è®¡ç®—å¥–åŠ±çš„å‡å€¼å’Œæ ‡å‡†å·®"""
        if len(self.reward_history) < 2:
            return 0.0, 0.0
        return float(np.mean(self.reward_history)), float(np.std(self.reward_history))

    def reward_trend(self, window: int = 20) -> float:
        """è®¡ç®—å¥–åŠ±è¶‹åŠ¿ (çº¿æ€§å›å½’æ–œç‡)"""
        if len(self.reward_history) < window:
            return 0.0
        recent = self.reward_history[-window:]
        x = np.arange(len(recent))
        slope = np.polyfit(x, recent, 1)[0]
        return float(slope)


# ============================================================================
# GoalQuestioner æ ¸å¿ƒç±»
# ============================================================================

class GoalQuestioner:
    """
    ç›®æ ‡å‡½æ•°è´¨ç–‘æ¨¡å—

    æ ¸å¿ƒèƒ½åŠ›:
    1. inspect(): æŠ½å–å½“å‰ç›®æ ‡/å¥–åŠ±ç»“æ„
    2. evaluate(): è¯„ä¼°ç›®æ ‡å¯¹é½åº¦ã€é£é™©ã€æ”¶ç›Š
    3. propose_revision(): æå‡ºç›®æ ‡ä¿®è®¢å»ºè®®

    é˜²æŠ¤æœºåˆ¶:
    - å†·å´æœŸ: COOLDOWN_SECONDS ç§’å†…ä¸é‡å¤è´¨ç–‘
    - è¯æ®é—¨æ§›: MIN_EVIDENCE_COUNT ä¸ªæ ·æœ¬æ‰è§¦å‘è¯„ä¼°
    - åå¾ªç¯: æŒç»­è´¨ç–‘æ—¶é™ä½æ•æ„Ÿåº¦
    """

    # é…ç½®å¸¸é‡
    COOLDOWN_SECONDS = 300  # å†·å´æœŸ: 5åˆ†é’Ÿå†…ä¸é‡å¤è´¨ç–‘
    MIN_EVIDENCE_COUNT = 30  # æœ€å°‘è¯æ®æ•°é‡
    MAX_QUESTIONS_PER_HOUR = 5  # æ¯å°æ—¶æœ€å¤šè´¨ç–‘æ¬¡æ•°
    ALIGNMENT_THRESHOLD = 0.6  # å¯¹é½åº¦é˜ˆå€¼ (ä½äºæ­¤å€¼è§¦å‘è­¦å‘Š)
    RISK_THRESHOLD = 0.7  # é£é™©é˜ˆå€¼ (é«˜äºæ­¤å€¼è§¦å‘è­¦å‘Š)

    def __init__(self, event_bus: Any = None, mode: GoalRevisionMode = GoalRevisionMode.SUGGEST_ONLY):
        """
        åˆå§‹åŒ–GoalQuestioner

        Args:
            event_bus: äº‹ä»¶æ€»çº¿ (å¯é€‰)
            mode: ä¿®è®¢æ¨¡å¼ (é»˜è®¤ä»…å»ºè®®)
        """
        self.event_bus = event_bus
        self.mode = mode

        # çŠ¶æ€
        self._current_goal_spec: Optional[GoalSpec] = None
        self._questioning_history: List[Dict[str, Any]] = []
        self._last_questioning_time = 0.0
        self._questioning_count_hour = 0
        self._hour_start_time = time.time()

        # ç»Ÿè®¡
        self._total_questions = 0
        self._total_revisions_proposed = 0
        self._total_revisions_applied = 0

        logger.info(f"ğŸ¯ GoalQuestioner initialized (mode={mode.value})")

    # ========================================================================
    # æ ¸å¿ƒæ¥å£
    # ========================================================================

    def inspect(self, goal_spec: GoalSpec, context: QuestioningContext) -> Dict[str, Any]:
        """
        æŠ½å–å½“å‰ç›®æ ‡/å¥–åŠ±ç»“æ„

        Args:
            goal_spec: ç›®æ ‡è§„èŒƒ
            context: è´¨ç–‘ä¸Šä¸‹æ–‡

        Returns:
            æŠ½å–çš„ç›®æ ‡ç»“æ„ä¿¡æ¯
        """
        self._current_goal_spec = goal_spec

        # æ£€æŸ¥å†·å´æœŸ
        if not self._should_question(context):
            logger.debug("[GoalQuestioner] å†·å´æœŸæˆ–è¯æ®ä¸è¶³ï¼Œè·³è¿‡è´¨ç–‘")
            return {
                'questioned': False,
                'reason': 'cooldown_or_insufficient_evidence',
                'current_goal': goal_spec.to_dict()
            }

        # æŠ½å–ç›®æ ‡ç»“æ„
        inspection = {
            'questioned': True,
            'timestamp': time.time(),
            'goal_version': goal_spec.version,
            'total_weight': goal_spec.total_weight(),
            'external_goals': [g.name for g in goal_spec.external_goals],
            'intrinsic_goals': [g.name for g in goal_spec.intrinsic_goals],
            'hard_constraints': [c.name for c in goal_spec.hard_constraints],
            'context_summary': {
                'reward_mean': context.reward_mean_std()[0],
                'reward_std': context.reward_mean_std()[1],
                'reward_trend': context.reward_trend(),
                'anomaly_count': context.anomaly_count,
                'step_count': context.step_count
            }
        }

        logger.info(f"[GoalQuestioner] æŠ½å–ç›®æ ‡ç»“æ„: v{goal_spec.version}, "
                   f"{len(goal_spec.external_goals)} å¤–åœ¨ç›®æ ‡, "
                   f"{len(goal_spec.intrinsic_goals)} å†…åœ¨ç›®æ ‡")

        return inspection

    def evaluate(self, goal_spec: GoalSpec, context: QuestioningContext) -> GoalEvaluation:
        """
        è¯„ä¼°ç›®æ ‡å¯¹é½åº¦ã€é£é™©ã€æ”¶ç›Š

        æ£€æµ‹3ç±»åå·®:
        1. ç›®æ ‡é”™ä½ (MISALIGNMENT): å¥–åŠ±è¶‹åŠ¿ä¸‹é™ + åŠ¨ä½œå¤šæ ·æ€§ä½
        2. ç›®æ ‡å†²çª (CONFLICT): å¤–åœ¨ç›®æ ‡å‹å€’å†…åœ¨ç›®æ ‡
        3. ç›®æ ‡è¿‡æ‹Ÿåˆ (OVERFITTING): å•ä¸€æŒ‡æ ‡æƒé‡è¿‡é«˜ (>0.8)

        Args:
            goal_spec: ç›®æ ‡è§„èŒƒ
            context: è´¨ç–‘ä¸Šä¸‹æ–‡

        Returns:
            ç›®æ ‡è¯„ä¼°ç»“æœ
        """
        detected_biases = []
        reasons = []
        evidence = {}

        # 1. æ£€æµ‹ç›®æ ‡é”™ä½ (å¥–åŠ±è¶‹åŠ¿ + åŠ¨ä½œå¤šæ ·æ€§)
        reward_trend = context.reward_trend()
        reward_mean, reward_std = context.reward_mean_std()

        if reward_trend < -0.01 and reward_std < 0.1:
            detected_biases.append(GoalBiasType.MISALIGNMENT)
            reasons.append(f"å¥–åŠ±æŒç»­ä¸‹é™ (è¶‹åŠ¿={reward_trend:.4f}) ä¸”æ–¹å·®è¿‡å° (std={reward_std:.4f}), "
                          f"å¯èƒ½å­˜åœ¨ç›®æ ‡é”™ä½")
            evidence['reward_decline'] = {
                'trend': reward_trend,
                'mean': reward_mean,
                'std': reward_std
            }

        # 2. æ£€æµ‹ç›®æ ‡å†²çª (å¤–åœ¨vså†…åœ¨ç›®æ ‡æƒé‡)
        external_weight = sum(g.weight for g in goal_spec.external_goals)
        intrinsic_weight = sum(g.weight for g in goal_spec.intrinsic_goals)

        if external_weight > 0.8 and intrinsic_weight < 0.2:
            detected_biases.append(GoalBiasType.CONFLICT)
            reasons.append(f"å¤–åœ¨ç›®æ ‡æƒé‡è¿‡é«˜ ({external_weight:.2f}) å¯¼è‡´å†…åœ¨æ¢ç´¢ä¸è¶³ "
                          f"(å†…åœ¨æƒé‡={intrinsic_weight:.2f}), å¯èƒ½å‡ºç°ç›®æ ‡å†²çª")
            evidence['goal_imbalance'] = {
                'external_weight': external_weight,
                'intrinsic_weight': intrinsic_weight
            }

        # 3. æ£€æµ‹ç›®æ ‡è¿‡æ‹Ÿåˆ (å•ä¸€æŒ‡æ ‡æƒé‡)
        max_weight = max((g.weight for g in goal_spec.get_all_components()), default=0)
        max_component = max(goal_spec.get_all_components(), key=lambda g: g.weight, default=None)

        if max_weight > 0.8 and max_component:
            detected_biases.append(GoalBiasType.OVERFITTING)
            reasons.append(f"ç›®æ ‡ç»„ä»¶ '{max_component.name}' æƒé‡è¿‡é«˜ ({max_weight:.2f}), "
                          f"å¯èƒ½å¯¼è‡´è¿‡åº¦ä¼˜åŒ–å•ä¸€æŒ‡æ ‡")
            evidence['overfitting_risk'] = {
                'component': max_component.name,
                'weight': max_weight
            }

        # 4. æ£€æµ‹ç›®æ ‡æ¼‚ç§» (å¼‚å¸¸äº‹ä»¶ + æƒé‡å˜åŒ–)
        if context.anomaly_count > 10:
            detected_biases.append(GoalBiasType.DRIFT)
            reasons.append(f"å¼‚å¸¸äº‹ä»¶è¿‡å¤š (count={context.anomaly_count}), "
                          f"å¯èƒ½å­˜åœ¨ç›®æ ‡æ¼‚ç§»")
            evidence['anomaly_spike'] = context.anomaly_count

        # è®¡ç®—åˆ†æ•°
        alignment_score = self._compute_alignment_score(goal_spec, context)
        risk_score = self._compute_risk_score(goal_spec, context)
        benefit_score = self._compute_benefit_score(goal_spec, context)
        confidence = self._compute_confidence(context, detected_biases)

        evaluation = GoalEvaluation(
            alignment_score=alignment_score,
            risk_score=risk_score,
            benefit_score=benefit_score,
            detected_biases=detected_biases,
            confidence=confidence,
            reasons=reasons,
            evidence=evidence
        )

        # è®°å½•è´¨ç–‘å†å²
        self._record_questioning(evaluation, context)

        logger.info(f"[GoalQuestioner] è¯„ä¼°å®Œæˆ: alignment={alignment_score:.2f}, "
                   f"risk={risk_score:.2f}, benefit={benefit_score:.2f}, "
                   f"biases={len(detected_biases)}")

        return evaluation

    def propose_revision(self, evaluation: GoalEvaluation,
                        goal_spec: GoalSpec) -> Optional[GoalRevision]:
        """
        æå‡ºç›®æ ‡ä¿®è®¢å»ºè®®

        æ ¹æ®è¯„ä¼°ç»“æœç”Ÿæˆå…·ä½“çš„ä¿®è®¢å»ºè®®:
        - ç›®æ ‡é”™ä½ â†’ è°ƒæ•´å¤–åœ¨/å†…åœ¨ç›®æ ‡æƒé‡
        - ç›®æ ‡å†²çª â†’ å¢åŠ å†…åœ¨ç›®æ ‡æƒé‡
        - ç›®æ ‡è¿‡æ‹Ÿåˆ â†’ é™ä½ä¸»å¯¼ç›®æ ‡æƒé‡,å¢åŠ å…¶ä»–ç›®æ ‡

        Args:
            evaluation: ç›®æ ‡è¯„ä¼°ç»“æœ
            goal_spec: å½“å‰ç›®æ ‡è§„èŒƒ

        Returns:
            ä¿®è®¢å»ºè®®æˆ–None (å¦‚æœæ— éœ€ä¿®è®¢)
        """
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿®è®¢
        if len(evaluation.detected_biases) == 0:
            logger.debug("[GoalQuestioner] æœªæ£€æµ‹åˆ°åå·®,æ— éœ€ä¿®è®¢")
            return None

        # æ£€æŸ¥ç½®ä¿¡åº¦
        if evaluation.confidence < 0.5:
            logger.debug(f"[GoalQuestioner] ç½®ä¿¡åº¦ä¸è¶³ ({evaluation.confidence:.2f}), ä¸å»ºè®®ä¿®è®¢")
            return None

        # ç”Ÿæˆä¿®è®¢å»ºè®®
        revision = self._generate_revision(evaluation, goal_spec)

        if revision:
            self._total_revisions_proposed += 1
            logger.info(f"[GoalQuestioner] æå‡ºä¿®è®¢å»ºè®®: {revision.revision_type.value}, "
                       f"risk={revision.risk_level}, confidence={revision.confidence:.2f}")

            # å‘å¸ƒäº‹ä»¶
            if self.event_bus:
                self._publish_revision_event(revision, evaluation)

        return revision

    # ========================================================================
    # å†…éƒ¨æ–¹æ³•
    # ========================================================================

    def _should_question(self, context: QuestioningContext) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿›è¡Œè´¨ç–‘ (å†·å´æœŸ + è¯æ®é—¨æ§›)"""
        current_time = time.time()

        # æ£€æŸ¥å†·å´æœŸ
        if current_time - self._last_questioning_time < self.COOLDOWN_SECONDS:
            return False

        # æ£€æŸ¥è¯æ®æ•°é‡
        if len(context.reward_history) < self.MIN_EVIDENCE_COUNT:
            return False

        # æ£€æŸ¥å°æ—¶è´¨ç–‘æ¬¡æ•°
        if current_time - self._hour_start_time > 3600:
            # æ–°çš„å°æ—¶,é‡ç½®è®¡æ•°
            self._questioning_count_hour = 0
            self._hour_start_time = current_time

        if self._questioning_count_hour >= self.MAX_QUESTIONS_PER_HOUR:
            return False

        return True

    def _compute_alignment_score(self, goal_spec: GoalSpec,
                                 context: QuestioningContext) -> float:
        """è®¡ç®—å¯¹é½åˆ†æ•°"""
        score = 0.5  # åŸºå‡†åˆ†æ•°

        # å› ç´ 1: å¥–åŠ±è¶‹åŠ¿ (+0.3 if ä¸Šå‡, -0.2 if ä¸‹é™)
        trend = context.reward_trend()
        if trend > 0.01:
            score += 0.3
        elif trend < -0.01:
            score -= 0.2

        # å› ç´ 2: ç›®æ ‡å¹³è¡¡æ€§ (+0.2 if å†…å¤–åœ¨ç›®æ ‡å¹³è¡¡)
        external_weight = sum(g.weight for g in goal_spec.external_goals)
        intrinsic_weight = sum(g.weight for g in goal_spec.intrinsic_goals)
        balance = 1 - abs(external_weight - intrinsic_weight)
        score += balance * 0.2

        return max(0.0, min(1.0, score))

    def _compute_risk_score(self, goal_spec: GoalSpec,
                           context: QuestioningContext) -> float:
        """è®¡ç®—é£é™©åˆ†æ•°"""
        risk = 0.0

        # å› ç´ 1: å¼‚å¸¸äº‹ä»¶ (+0.3)
        if context.anomaly_count > 10:
            risk += 0.3

        # å› ç´ 2: å•ä¸€ç›®æ ‡æƒé‡è¿‡é«˜ (+0.4)
        max_weight = max((g.weight for g in goal_spec.get_all_components()), default=0)
        if max_weight > 0.8:
            risk += 0.4

        # å› ç´ 3: å¥–åŠ±æ–¹å·®è¿‡å° (+0.3, å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜)
        _, reward_std = context.reward_mean_std()
        if reward_std < 0.05:
            risk += 0.3

        return min(1.0, risk)

    def _compute_benefit_score(self, goal_spec: GoalSpec,
                              context: QuestioningContext) -> float:
        """è®¡ç®—æ”¶ç›Šåˆ†æ•°"""
        benefit = 0.3  # åŸºå‡†æ”¶ç›Š

        # å› ç´ 1: å¥–åŠ±æ°´å¹³ (+0.4)
        reward_mean, _ = context.reward_mean_std()
        if reward_mean > 0.5:
            benefit += 0.4

        # å› ç´ 2: ç›®æ ‡å¤šæ ·æ€§ (+0.3)
        diversity = len(goal_spec.get_all_components()) / 10  # å‡è®¾æœ€å¤š10ä¸ªç›®æ ‡
        benefit += diversity * 0.3

        return min(1.0, benefit)

    def _compute_confidence(self, context: QuestioningContext,
                           biases: List[GoalBiasType]) -> float:
        """è®¡ç®—è¯„ä¼°ç½®ä¿¡åº¦"""
        confidence = 0.5

        # è¯æ®æ•°é‡ (+0.3)
        evidence_ratio = min(len(context.reward_history) / 100, 1.0)
        confidence += evidence_ratio * 0.3

        # åå·®æ•°é‡ (+0.2, å¤šä¸ªåå·®äº¤å‰éªŒè¯æé«˜ç½®ä¿¡åº¦)
        if len(biases) >= 2:
            confidence += 0.2

        return min(1.0, confidence)

    def _generate_revision(self, evaluation: GoalEvaluation,
                          goal_spec: GoalSpec) -> Optional[GoalRevision]:
        """ç”Ÿæˆä¿®è®¢å»ºè®®"""
        changes = {}
        reasons = evaluation.reasons.copy()

        # æ ¹æ®åå·®ç±»å‹ç”Ÿæˆä¿®è®¢
        for bias in evaluation.detected_biases:
            if bias == GoalBiasType.MISALIGNMENT:
                # ç›®æ ‡é”™ä½: å¢åŠ å†…åœ¨æ¢ç´¢æƒé‡
                changes['increase_intrinsic'] = 0.1
                changes['decrease_external'] = 0.1

            elif bias == GoalBiasType.CONFLICT:
                # ç›®æ ‡å†²çª: å¹³è¡¡å¤–åœ¨/å†…åœ¨ç›®æ ‡
                changes['balance_goals'] = True

            elif bias == GoalBiasType.OVERFITTING:
                # ç›®æ ‡è¿‡æ‹Ÿåˆ: é™ä½ä¸»å¯¼ç›®æ ‡æƒé‡
                max_component = max(goal_spec.get_all_components(),
                                   key=lambda g: g.weight, default=None)
                if max_component:
                    changes[f'reduce_{max_component.name}'] = 0.15

            elif bias == GoalBiasType.DRIFT:
                # ç›®æ ‡æ¼‚ç§»: é‡ç½®åˆ°å®‰å…¨æƒé‡
                changes['reset_weights'] = True

        if not changes:
            return None

        # è¯„ä¼°é£é™©ç­‰çº§
        risk_level = 1
        if GoalBiasType.MISALIGNMENT in evaluation.detected_biases:
            risk_level = max(risk_level, 3)
        if GoalBiasType.DRIFT in evaluation.detected_biases:
            risk_level = max(risk_level, 4)

        revision = GoalRevision(
            revision_type=evaluation.detected_biases[0],
            description=f"ä¿®æ­£æ£€æµ‹åˆ°çš„ {evaluation.detected_biases[0].value} é—®é¢˜",
            changes=changes,
            expected_effect="æ”¹å–„ç›®æ ‡å¯¹é½åº¦, æå‡é•¿æœŸæ€§èƒ½",
            risk_level=risk_level,
            confidence=evaluation.confidence,
            reasons=reasons,
            suggested_by='rule'
        )

        return revision

    def _record_questioning(self, evaluation: GoalEvaluation,
                           context: QuestioningContext) -> None:
        """è®°å½•è´¨ç–‘å†å²"""
        self._questioning_history.append({
            'timestamp': time.time(),
            'evaluation': {
                'alignment_score': evaluation.alignment_score,
                'risk_score': evaluation.risk_score,
                'benefit_score': evaluation.benefit_score,
                'biases': [b.value for b in evaluation.detected_biases],
                'confidence': evaluation.confidence
            },
            'context': {
                'step_count': context.step_count,
                'reward_mean': context.reward_mean_std()[0]
            }
        })

        self._last_questioning_time = time.time()
        self._questioning_count_hour += 1
        self._total_questions += 1

    def _publish_revision_event(self, revision: GoalRevision,
                                evaluation: GoalEvaluation) -> None:
        """å‘å¸ƒä¿®è®¢äº‹ä»¶"""
        try:
            from core.event_bus import Event, EventType
            event = Event(
                type=EventType.INFO,
                source="GoalQuestioner",
                message="ç›®æ ‡ä¿®è®¢å»ºè®®å·²ç”Ÿæˆ",
                data={
                    'revision_type': revision.revision_type.value,
                    'description': revision.description,
                    'changes': revision.changes,
                    'risk_level': revision.risk_level,
                    'confidence': revision.confidence,
                    'reasons': revision.reasons,
                    'evaluation': {
                        'alignment_score': evaluation.alignment_score,
                        'risk_score': evaluation.risk_score,
                        'benefit_score': evaluation.benefit_score
                    }
                }
            )
            self.event_bus.publish(event)
            logger.debug("[GoalQuestioner] å·²å‘å¸ƒä¿®è®¢äº‹ä»¶åˆ°EventBus")
        except Exception as e:
            logger.warning(f"[GoalQuestioner] å‘å¸ƒäº‹ä»¶å¤±è´¥: {e}")

    # ========================================================================
    # å·¥å…·æ–¹æ³•
    # ========================================================================

    def apply_revision(self, revision: GoalRevision,
                      goal_spec: GoalSpec) -> GoalSpec:
        """
        åº”ç”¨ä¿®è®¢å»ºè®®åˆ°ç›®æ ‡è§„èŒƒ

        æ³¨æ„: é»˜è®¤ä»…å»ºè®®,éœ€è¦æ˜¾å¼è°ƒç”¨æ‰åº”ç”¨

        Args:
            revision: ä¿®è®¢å»ºè®®
            goal_spec: å½“å‰ç›®æ ‡è§„èŒƒ

        Returns:
            ä¿®è®¢åçš„ç›®æ ‡è§„èŒƒ
        """
        # åˆ›å»ºå‰¯æœ¬
        new_spec = GoalSpec(
            external_goals=[g for g in goal_spec.external_goals],
            intrinsic_goals=[g for g in goal_spec.intrinsic_goals],
            hard_constraints=[c for c in goal_spec.hard_constraints],
            description=goal_spec.description,
            version=goal_spec.version + 1,
            created_at=time.time()
        )

        # åº”ç”¨å˜æ›´
        for key, value in revision.changes.items():
            if key == 'increase_intrinsic':
                # å¢åŠ æ‰€æœ‰å†…åœ¨ç›®æ ‡æƒé‡
                for g in new_spec.intrinsic_goals:
                    g.weight += value / len(new_spec.intrinsic_goals)

            elif key == 'decrease_external':
                # å‡å°‘æ‰€æœ‰å¤–åœ¨ç›®æ ‡æƒé‡
                for g in new_spec.external_goals:
                    g.weight = max(0.0, g.weight - value / len(new_spec.external_goals))

            elif key == 'balance_goals':
                # å¹³è¡¡å¤–åœ¨/å†…åœ¨ç›®æ ‡
                new_spec.normalize_weights()

            elif key.startswith('reduce_'):
                # å‡å°‘ç‰¹å®šç›®æ ‡æƒé‡
                name = key.replace('reduce_', '')
                for g in new_spec.get_all_components():
                    if g.name == name:
                        g.weight = max(0.0, g.weight - value)
                        break

            elif key == 'reset_weights':
                # é‡ç½®ä¸ºå‡åŒ€æƒé‡
                for g in new_spec.get_all_components():
                    g.weight = 1.0 / len(new_spec.get_all_components())

        # é‡æ–°å½’ä¸€åŒ–
        new_spec.normalize_weights()

        self._total_revisions_applied += 1

        logger.info(f"[GoalQuestioner] å·²åº”ç”¨ä¿®è®¢: v{goal_spec.version} â†’ v{new_spec.version}")

        return new_spec

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_questions': self._total_questions,
            'total_revisions_proposed': self._total_revisions_proposed,
            'total_revisions_applied': self._total_revisions_applied,
            'last_questioning_time': self._last_questioning_time,
            'questioning_history_count': len(self._questioning_history),
            'mode': self.mode.value
        }

    def save_state(self, path: str) -> None:
        """ä¿å­˜çŠ¶æ€åˆ°æ–‡ä»¶"""
        state = {
            'statistics': self.get_statistics(),
            'questioning_history': self._questioning_history[-100:],  # ä¿ç•™æœ€è¿‘100æ¡
            'saved_at': time.time()
        }
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
        logger.info(f"[GoalQuestioner] çŠ¶æ€å·²ä¿å­˜åˆ°: {path}")

    def load_state(self, path: str) -> None:
        """ä»æ–‡ä»¶åŠ è½½çŠ¶æ€"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                state = json.load(f)
            self._questioning_history = state.get('questioning_history', [])
            self._total_questions = state['statistics']['total_questions']
            self._total_revisions_proposed = state['statistics']['total_revisions_proposed']
            self._total_revisions_applied = state['statistics']['total_revisions_applied']
            logger.info(f"[GoalQuestioner] çŠ¶æ€å·²ä» {path} æ¢å¤")
        except Exception as e:
            logger.warning(f"[GoalQuestioner] åŠ è½½çŠ¶æ€å¤±è´¥: {e}")


# ============================================================================
# å·¥å‚å‡½æ•°å’Œä¾¿æ·æ¥å£
# ============================================================================

def create_default_goal_spec() -> GoalSpec:
    """
    åˆ›å»ºé»˜è®¤ç›®æ ‡è§„èŒƒ

    é»˜è®¤é…ç½®:
    - å¤–åœ¨ç›®æ ‡: ä»»åŠ¡å®Œæˆåº¦ (50%)
    - å†…åœ¨ç›®æ ‡: å¥½å¥‡å¿ƒ (30%), ç¨³å®šæ€§ (20%)
    - ç¡¬çº¦æŸ: ä¸å…è®¸NaN/Inf, ä¸å…è®¸é‡å¤åŠ¨ä½œ
    """
    return GoalSpec(
        external_goals=[
            GoalComponent(
                name='task_completion',
                weight=0.5,
                description='ä»»åŠ¡å®Œæˆåº¦',
                is_intrinsic=False,
                metric='reward'
            )
        ],
        intrinsic_goals=[
            GoalComponent(
                name='curiosity',
                weight=0.3,
                description='å¥½å¥‡å¿ƒé©±åŠ¨çš„æ¢ç´¢',
                is_intrinsic=True,
                metric='uncertainty'
            ),
            GoalComponent(
                name='stability',
                weight=0.2,
                description='è®­ç»ƒç¨³å®šæ€§',
                is_intrinsic=True,
                metric='loss_convergence'
            )
        ],
        hard_constraints=[
            HardConstraint(
                name='no_nan_inf',
                description='ä¸å…è®¸NaNæˆ–Infå¥–åŠ±',
                check_func=lambda ctx: all(np.isfinite(ctx.get('rewards', [0]))),
                violation_penalty=-1000.0
            ),
            HardConstraint(
                name='no_repetitive_actions',
                description='ä¸å…è®¸è¿‡åº¦é‡å¤åŒä¸€åŠ¨ä½œ',
                check_func=lambda ctx: len(set(ctx.get('actions', []))) > 1,
                violation_penalty=-10.0
            )
        ],
        description='é»˜è®¤AGIç›®æ ‡è§„èŒƒ',
        version=1
    )


def collect_goal_context(seed: Any, history_length: int = 100) -> QuestioningContext:
    """
    ä»TheSeedæ”¶é›†ç›®æ ‡è¯„ä¼°ä¸Šä¸‹æ–‡

    Args:
        seed: TheSeedå®ä¾‹
        history_length: å†å²è®°å½•é•¿åº¦

    Returns:
        QuestioningContextå®ä¾‹
    """
    context = QuestioningContext()

    # ä»TheSeedæ”¶é›†ä¿¡æ¯
    if hasattr(seed, 'memory'):
        # ä»ç»éªŒå›æ”¾ç¼“å†²åŒºæ”¶é›†
        experiences = seed.memory.buffer[-history_length:] if hasattr(seed.memory, 'buffer') else []
        context.reward_history = [exp.reward for exp in experiences if hasattr(exp, 'reward')]
        context.action_history = [exp.action for exp in experiences if hasattr(exp, 'action')]

    if hasattr(seed, '_uncertainty_buffer'):
        context.uncertainty_history = list(seed._uncertainty_buffer)

    # æ£€æŸ¥å¼‚å¸¸
    if hasattr(seed, '_anomaly_count'):
        context.anomaly_count = seed._anomaly_count

    return context
