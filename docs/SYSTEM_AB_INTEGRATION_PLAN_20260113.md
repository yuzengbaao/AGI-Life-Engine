# ç³»ç»ŸA+Bé›†æˆæ–¹æ¡ˆï¼šè‡ªä¸»æ™ºèƒ½è‡ªæŒ‡æ¢¯åº¦åˆ†å½¢è¿›åŒ–AGI

**åˆ¶å®šæ—¥æœŸ**: 2026-01-13
**åˆ¶å®šè€…**: Claude Code (Sonnet 4.5)
**å‚è€ƒ**:
- AGIç³»ç»Ÿ3Dæ‹“æ‰‘å…³ç³»å›¾ (system_topology_3d.html)
- å†³ç­–è¾¹ç•Œ3Då¯è§†åŒ– (decision_boundary_3d_simple.html)
- ç³»ç»ŸAå…¨å±€æ£€æŸ¥æŠ¥å‘Š
- ç³»ç»ŸBå…¨å±€æ£€æŸ¥æŠ¥å‘Š

---

## æ‰§è¡Œæ‘˜è¦

### é›†æˆç›®æ ‡

åˆ›å»ºä¸€ä¸ªèåˆç³»ç»ŸAï¼ˆç»„ä»¶ç»„è£…å¼ï¼‰å’Œç³»ç»ŸBï¼ˆåˆ†å½¢æ‹“æ‰‘å¼ï¼‰ä¼˜åŠ¿çš„å®Œæ•´AGIç³»ç»Ÿï¼Œå®ç°ï¼š

1. **è‡ªä¸»æ™ºèƒ½**ï¼šå‡å°‘å¤–éƒ¨LLMä¾èµ–ï¼Œæå‡æœ¬åœ°å†³ç­–èƒ½åŠ›
2. **è‡ªæŒ‡æ¶‰**ï¼šç³»ç»Ÿèƒ½å¤Ÿè§‚å¯Ÿã€ç†è§£å’Œä¿®æ”¹è‡ªèº«
3. **æ¢¯åº¦åˆ†å½¢è¿›åŒ–**ï¼šé€šè¿‡é€’å½’è‡ªå¼•ç”¨å®ç°æŒç»­è¿›åŒ–

### é›†æˆæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  AGIç»Ÿä¸€ç³»ç»Ÿï¼ˆç³»ç»ŸA+Bé›†æˆï¼‰                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Layer 0 (å…¥å£) - Y=60                                       â”‚
â”‚  â”œâ”€ AGI_Life_Engine (ä¸»å¼•æ“)                                â”‚
â”‚  â”œâ”€ IntentDialogueBridge (æ˜¾æ„è¯†â†”æ½œæ„è¯†æ¡¥æ¥)                â”‚
â”‚  â””â”€ HybridDecisionEngine (ğŸ†• æ··åˆå†³ç­–å¼•æ“)                  â”‚
â”‚                                                               â”‚
â”‚  Layer 1 (è®¤çŸ¥æ ¸å¿ƒ) - Y=40                                   â”‚
â”‚  â”œâ”€ LLMService (å¤–éƒ¨LLM - ç³»ç»ŸA)                            â”‚
â”‚  â”œâ”€ TheSeed (DQN - ç³»ç»ŸA)                                   â”‚
â”‚  â”œâ”€ FractalIntelligence (ğŸ†• åˆ†å½¢æ‹“æ‰‘ - ç³»ç»ŸB)               â”‚
â”‚  â”œâ”€ MetaLearner (M1 - å…ƒå­¦ä¹ )                               â”‚
â”‚  â”œâ”€ GoalQuestioner (M2 - ç›®æ ‡è´¨ç–‘)                          â”‚
â”‚  â””â”€ NeuroSymbolicBridge (ç¥ç»-ç¬¦å·æ¡¥æ¥)                     â”‚
â”‚                                                               â”‚
â”‚  Layer 2-6: æ™ºèƒ½ä½“/è®°å¿†/è¿›åŒ–/æ„ŸçŸ¥/å¤–å›´ç³»ç»Ÿ                   â”‚
â”‚  (ä¿æŒç³»ç»ŸAçš„å®Œæ•´åŠŸèƒ½)                                       â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ä¸€ã€3Dæ‹“æ‰‘ç»“æ„åˆ†æ

### 1.1 å½“å‰ç³»ç»ŸAçš„7å±‚æ¶æ„

åŸºäº`system_topology_3d.html`ï¼Œå½“å‰ç³»ç»ŸAåˆ†ä¸º7å±‚ï¼š

| å±‚çº§ | Yåæ ‡ | åç§° | æ ¸å¿ƒç»„ä»¶ |
|------|-------|------|----------|
| **Layer 0** | 60 | å…¥å£å±‚ | AGI_Life_Engine, IntentDialogueBridge, Insight V-I-E Loop |
| **Layer 1** | 40 | è®¤çŸ¥æ ¸å¿ƒ | LLMService, TheSeed, NeuroSymbolicBridge, M1-M4 |
| **Layer 2** | 20 | æ™ºèƒ½ä½“ | PlannerAgent, ExecutorAgent, CriticAgent |
| **Layer 3** | 0 | è®°å¿†ç³»ç»Ÿ | BiologicalMemory, ExperienceMemory, KnowledgeGraph |
| **Layer 4** | -20 | è¿›åŒ–ç³»ç»Ÿ | EvolutionController, SelfModifyingEngine |
| **Layer 5** | -40 | æ„ŸçŸ¥ç³»ç»Ÿ | PerceptionManager, VisionObserver, WhisperASR |
| **Layer 6** | -60 | å¤–å›´ç³»ç»Ÿ | ComponentCoordinator, ToolExecutionBridge |

### 1.2 ç³»ç»ŸBçš„é›†æˆä½ç½®

**æ ¸å¿ƒå†³ç­–**ï¼šç³»ç»ŸBåº”é›†æˆåˆ°**Layer 1ï¼ˆè®¤çŸ¥æ ¸å¿ƒï¼‰**

**ç†ç”±**ï¼š
1. ç³»ç»ŸBçš„æ ¸å¿ƒæ˜¯**åˆ†å½¢æ‹“æ‰‘æ™ºèƒ½**ï¼Œä¸TheSeedï¼ˆä¸»åŠ¨æ¨ç†ï¼‰åŒçº§
2. ç³»ç»ŸBæä¾›**å¿«é€Ÿæœ¬åœ°å†³ç­–**ï¼ˆ10-15msï¼‰ï¼Œè¡¥å……LLMServiceï¼ˆ200ms+ï¼‰
3. ç³»ç»ŸBçš„**è‡ªæŒ‡æ¶‰ç‰¹æ€§**ä¸MetaLearnerã€GoalQuestionerå½¢æˆååŒ

**é›†æˆä½ç½®**ï¼š
```javascript
// 3Dåæ ‡ (åŸºäºsystem_topology_3d.html)
{
  id: "FractalIntelligence",
  layer: 1,              // Layer 1: è®¤çŸ¥æ ¸å¿ƒ
  file: "core/fractal_intelligence.py",
  desc: "åˆ†å½¢æ‹“æ‰‘æ™ºèƒ½ - è‡ªæŒ‡æ¶‰ã€é€’å½’ã€ç›®æ ‡å¯å¡‘",
  size: 2.8,
  x: -40, y: 40, z: -20  // é è¿‘TheSeed (-25, 40, -10)
}
```

### 1.3 æ··åˆå†³ç­–æµç¨‹

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ HybridDecisionâ”‚
                    â”‚    Engine     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚               â”‚               â”‚
            â–¼               â–¼               â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Fractal  â”‚   â”‚ TheSeed  â”‚   â”‚  LLMService  â”‚
     â”‚ (ç³»ç»ŸB)    â”‚   â”‚ (ç³»ç»ŸA)  â”‚   â”‚  (å¤–éƒ¨)     â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚              â”‚                â”‚
           â”‚              â”‚                â”‚
     10-15ms        50-100ms         200-2000ms
     æœ¬åœ°å†³ç­–        æœ¬åœ°DQN          å¤–éƒ¨LLM
           â”‚              â”‚                â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ æœ€ç»ˆå†³ç­–  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## äºŒã€æ··åˆå†³ç­–å¼•æ“è®¾è®¡

### 2.1 æ ¸å¿ƒæ¶æ„

**æ–‡ä»¶**: `core/hybrid_decision_engine.py`ï¼ˆæ–°å»ºï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ··åˆå†³ç­–å¼•æ“ (Hybrid Decision Engine)
èåˆç³»ç»ŸAï¼ˆç»„ä»¶ç»„è£…ï¼‰å’Œç³»ç»ŸBï¼ˆåˆ†å½¢æ‹“æ‰‘ï¼‰çš„å†³ç­–èƒ½åŠ›

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä¸‰è·¯å†³ç­–ï¼šFractalï¼ˆå¿«ï¼‰â†’ TheSeedï¼ˆä¸­ï¼‰â†’ LLMï¼ˆæ…¢ï¼‰
2. è‡ªé€‚åº”é˜ˆå€¼ï¼šåŠ¨æ€è°ƒæ•´å†³ç­–è·¯å¾„
3. ç½®ä¿¡åº¦å­¦ä¹ ï¼šä»å†³ç­–ç»“æœä¸­å­¦ä¹ 
4. å…ƒå­¦ä¹ ï¼šMetaLearnerä¼˜åŒ–å†³ç­–ç­–ç•¥

ä½œè€…ï¼šClaude Code (Sonnet 4.5)
åˆ›å»ºæ—¥æœŸï¼š2026-01-13
"""

import numpy as np
import torch
import logging
from typing import Dict, Any, Optional, Tuple, List
from dataclasses import dataclass
from enum import Enum
import time

# å¯¼å…¥ç³»ç»ŸAå’ŒB
from core.seed import TheSeed, Experience
from core.fractal_intelligence import create_fractal_intelligence, FractalOutput
from core.llm_client import LLMService
from core.meta_learner import MetaLearner

logger = logging.getLogger(__name__)


class DecisionPath(Enum):
    """å†³ç­–è·¯å¾„"""
    FRACTAL = "fractal"      # ç³»ç»ŸBï¼šæœ€å¿«ï¼Œ10-15ms
    SEED = "seed"           # ç³»ç»ŸAï¼šä¸­ç­‰ï¼Œ50-100ms
    LLM = "llm"             # å¤–éƒ¨LLMï¼šæœ€æ…¢ï¼Œ200-2000ms
    HYBRID = "hybrid"       # æ··åˆï¼šå¤šæ–¹éªŒè¯


@dataclass
class DecisionResult:
    """å†³ç­–ç»“æœ"""
    action: int
    confidence: float
    path: DecisionPath
    response_time_ms: float
    explanation: str
    self_awareness: float = 0.0
    entropy: float = 0.0
    needs_validation: bool = False
    metadata: Dict[str, Any] = None


class HybridDecisionEngine:
    """
    æ··åˆå†³ç­–å¼•æ“

    ä¸‰è·¯å†³ç­–ç­–ç•¥ï¼š
    1. Fractalï¼ˆç³»ç»ŸBï¼‰- æé€Ÿæœ¬åœ°å†³ç­–
    2. TheSeedï¼ˆç³»ç»ŸAï¼‰- DQNå¢å¼ºå†³ç­–
    3. LLMï¼ˆå¤–éƒ¨ï¼‰- å¤æ‚æ¨ç†å†³ç­–
    """

    def __init__(
        self,
        state_dim: int = 64,
        action_dim: int = 4,
        device: str = 'cpu',
        enable_fractal: bool = True,
        enable_meta_learning: bool = True
    ):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = device
        self.enable_fractal = enable_fractal
        self.enable_meta_learning = enable_meta_learning

        # 1. åˆå§‹åŒ–ç³»ç»ŸBï¼šåˆ†å½¢æ™ºèƒ½ï¼ˆæœ€å¿«ï¼‰
        self.fractal = None
        if enable_fractal:
            try:
                self.fractal = create_fractal_intelligence(
                    input_dim=state_dim,
                    state_dim=state_dim,
                    device=device
                )
                logger.info("[Hybrid] ç³»ç»ŸBï¼ˆåˆ†å½¢æ™ºèƒ½ï¼‰å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"[Hybrid] ç³»ç»ŸBåˆå§‹åŒ–å¤±è´¥: {e}")
                self.enable_fractal = False

        # 2. åˆå§‹åŒ–ç³»ç»ŸAï¼šTheSeedï¼ˆä¸­ç­‰é€Ÿåº¦ï¼‰
        self.seed = TheSeed(state_dim=state_dim, action_dim=action_dim)
        logger.info("[Hybrid] ç³»ç»ŸAï¼ˆTheSeedï¼‰å·²å¯ç”¨")

        # 3. LLMæœåŠ¡ï¼ˆæ…¢ä½†å¼ºå¤§ï¼‰
        self.llm_service = LLMService()
        logger.info("[Hybrid] LLMæœåŠ¡å·²å¯ç”¨")

        # 4. å…ƒå­¦ä¹ å™¨ï¼ˆM1ç»„ä»¶ï¼‰
        self.meta_learner = None
        if enable_meta_learning:
            try:
                self.meta_learner = MetaLearner(state_dim=state_dim)
                logger.info("[Hybrid] M1ï¼ˆå…ƒå­¦ä¹ å™¨ï¼‰å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"[Hybrid] M1åˆå§‹åŒ–å¤±è´¥: {e}")

        # 5. è‡ªé€‚åº”é˜ˆå€¼ç®¡ç†
        self.confidence_history: List[float] = []
        self.adaptive_threshold = 0.5
        self.threshold_window = 100

        # 6. å†³ç­–ç»Ÿè®¡
        self.stats = {
            'total_decisions': 0,
            'fractal_decisions': 0,
            'seed_decisions': 0,
            'llm_decisions': 0,
            'hybrid_decisions': 0,
            'avg_confidence': 0.0,
            'avg_response_time': 0.0
        }

        logger.info("[Hybrid] æ··åˆå†³ç­–å¼•æ“åˆå§‹åŒ–å®Œæˆ")

    def decide(
        self,
        state: np.ndarray,
        context: Optional[Dict[str, Any]] = None,
        force_path: Optional[DecisionPath] = None
    ) -> DecisionResult:
        """
        æ··åˆå†³ç­–

        å†³ç­–æµç¨‹ï¼š
        1. ç³»ç»ŸBï¼ˆFractalï¼‰å¿«é€Ÿå†³ç­–ï¼ˆ10-15msï¼‰
        2. å¦‚æœç½®ä¿¡åº¦é«˜ï¼Œç›´æ¥è¿”å›
        3. å¦åˆ™å°è¯•ç³»ç»ŸAï¼ˆTheSeedï¼‰ï¼ˆ50-100msï¼‰
        4. å¦‚æœä»ä¸ç¡®å®šï¼Œè°ƒç”¨LLMï¼ˆ200-2000msï¼‰
        """
        self.stats['total_decisions'] += 1
        context = context or {}

        # 1. ç³»ç»ŸBå†³ç­–ï¼ˆæœ€å¿«ï¼‰
        if self.enable_fractal and force_path in [None, DecisionPath.FRACTAL]:
            result = self._decide_fractal(state, context)
            if result.confidence >= self.adaptive_threshold and force_path is None:
                self.stats['fractal_decisions'] += 1
                return result

        # 2. ç³»ç»ŸAå†³ç­–ï¼ˆä¸­ç­‰ï¼‰
        if force_path in [None, DecisionPath.SEED]:
            result = self._decide_seed(state, context)
            if result.confidence >= self.adaptive_threshold and force_path is None:
                self.stats['seed_decisions'] += 1
                return result

        # 3. LLMå†³ç­–ï¼ˆæœ€æ…¢ä½†æœ€å…¨é¢ï¼‰
        result = self._decide_llm(state, context)
        self.stats['llm_decisions'] += 1
        return result

    def _decide_fractal(
        self,
        state: np.ndarray,
        context: Dict[str, Any]
    ) -> DecisionResult:
        """ç³»ç»ŸBå†³ç­–ï¼šåˆ†å½¢æ‹“æ‰‘æ™ºèƒ½"""
        start_time = time.time()

        # è½¬æ¢ä¸ºTensor
        state_tensor = torch.from_numpy(state).float().to(self.device)

        # Fractalå†³ç­–
        with torch.no_grad():
            output, meta = self.fractal.core(state_tensor, return_meta=True)

        response_time = (time.time() - start_time) * 1000

        # æå–å†³ç­–ä¿¡æ¯
        confidence = meta.self_awareness.mean().item()
        entropy = meta.entropy.item()
        action = torch.argmax(output).item() if output.dim() > 0 else int(output.item() % self.action_dim)

        return DecisionResult(
            action=action,
            confidence=confidence,
            path=DecisionPath.FRACTAL,
            response_time_ms=response_time,
            explanation=f"ç³»ç»ŸBï¼ˆåˆ†å½¢æ‹“æ‰‘ï¼‰- ç½®ä¿¡åº¦{confidence:.4f}",
            self_awareness=confidence,
            entropy=entropy,
            needs_validation=confidence < self.adaptive_threshold,
            metadata={
                'goal_score': meta.goal_score,
                'metaparams': meta.metaparams
            }
        )

    def _decide_seed(
        self,
        state: np.ndarray,
        context: Dict[str, Any]
    ) -> DecisionResult:
        """ç³»ç»ŸAå†³ç­–ï¼šTheSeed DQN"""
        start_time = time.time()

        # TheSeedå†³ç­–
        action = self.seed.act(state)
        value = self.seed.evaluate(state, state, 0.0)
        confidence = min(1.0, max(0.0, value))

        response_time = (time.time() - start_time) * 1000

        return DecisionResult(
            action=action,
            confidence=confidence,
            path=DecisionPath.SEED,
            response_time_ms=response_time,
            explanation=f"ç³»ç»ŸAï¼ˆTheSeedï¼‰- ä»·å€¼{value:.4f}",
            entropy=0.5,
            needs_validation=confidence < self.adaptive_threshold
        )

    def _decide_llm(
        self,
        state: np.ndarray,
        context: Dict[str, Any]
    ) -> DecisionResult:
        """å¤–éƒ¨LLMå†³ç­–"""
        start_time = time.time()

        # æ„å»ºLLMæç¤º
        prompt = self._build_llm_prompt(state, context)

        # è°ƒç”¨LLM
        response = self.llm_service.query(prompt)

        response_time = (time.time() - start_time) * 1000

        # è§£æå“åº”ï¼ˆç®€åŒ–å¤„ç†ï¼‰
        action = self._parse_llm_action(response, context)
        confidence = 0.8  # LLMé€šå¸¸æœ‰è¾ƒé«˜ç½®ä¿¡åº¦

        return DecisionResult(
            action=action,
            confidence=confidence,
            path=DecisionPath.LLM,
            response_time_ms=response_time,
            explanation=f"å¤–éƒ¨LLM - {response[:100]}...",
            entropy=0.3,
            needs_validation=False
        )

    def learn(
        self,
        state: np.ndarray,
        action: int,
        reward: float,
        next_state: np.ndarray
    ):
        """
        ä»ç»éªŒä¸­å­¦ä¹ ï¼ˆä¸‰è·¯å­¦ä¹ ï¼‰

        1. TheSeedï¼šDQNå­¦ä¹ 
        2. Fractalï¼šç›®æ ‡ä¿®æ”¹
        3. MetaLearnerï¼šå…ƒå‚æ•°ä¼˜åŒ–
        """
        # 1. TheSeedå­¦ä¹ 
        experience = Experience(
            state=state,
            action=action,
            reward=reward,
            next_state=next_state
        )
        self.seed.learn(experience)

        # 2. Fractalå­¦ä¹ 
        if self.enable_fractal:
            exp_dict = {'state': torch.from_numpy(state).float().to(self.device)}
            self.fractal.learn(exp_dict, reward)

        # 3. å…ƒå­¦ä¹ 
        if self.meta_learner:
            self.meta_learner.update(state, action, reward)

        # 4. æ›´æ–°è‡ªé€‚åº”é˜ˆå€¼
        self._update_adaptive_threshold(reward)

    def _update_adaptive_threshold(self, reward: float):
        """æ›´æ–°è‡ªé€‚åº”ç½®ä¿¡åº¦é˜ˆå€¼"""
        # åŸºäºå¥–åŠ±è°ƒæ•´é˜ˆå€¼
        if reward > 0:
            # æ­£å¥–åŠ±ï¼šé™ä½é˜ˆå€¼ï¼Œæ›´å¤šä½¿ç”¨æœ¬åœ°å†³ç­–
            self.adaptive_threshold = max(0.3, self.adaptive_threshold - 0.01)
        else:
            # è´Ÿå¥–åŠ±ï¼šæé«˜é˜ˆå€¼ï¼Œæ›´å¤šä½¿ç”¨LLM
            self.adaptive_threshold = min(0.7, self.adaptive_threshold + 0.01)

        logger.debug(f"[Hybrid] é˜ˆå€¼æ›´æ–°: {self.adaptive_threshold:.4f} (reward={reward:.2f})")

    def _build_llm_prompt(self, state: np.ndarray, context: Dict[str, Any]) -> str:
        """æ„å»ºLLMæç¤ºï¼ˆç®€åŒ–ï¼‰"""
        return f"""
å½“å‰çŠ¶æ€å‘é‡ï¼ˆå‰10ç»´ï¼‰: {state[:10]}
ä»»åŠ¡ä¸Šä¸‹æ–‡: {context.get('task', 'æœªçŸ¥ä»»åŠ¡')}
è¯·ç»™å‡ºæœ€ä½³è¡ŒåŠ¨å»ºè®®ï¼ˆ0-{self.action_dim-1}ä¹‹é—´çš„æ•´æ•°ï¼‰ã€‚
"""

    def _parse_llm_action(self, response: str, context: Dict[str, Any]) -> int:
        """è§£æLLMå“åº”æå–åŠ¨ä½œï¼ˆç®€åŒ–ï¼‰"""
        # ç®€åŒ–å¤„ç†ï¼šä»å“åº”ä¸­æå–æ•°å­—
        import re
        numbers = re.findall(r'\d+', response)
        if numbers:
            action = int(numbers[0]) % self.action_dim
        else:
            action = 0  # é»˜è®¤åŠ¨ä½œ

        return action

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–å†³ç­–ç»Ÿè®¡"""
        stats = self.stats.copy()

        if stats['total_decisions'] > 0:
            stats['fractal_ratio'] = stats['fractal_decisions'] / stats['total_decisions']
            stats['seed_ratio'] = stats['seed_decisions'] / stats['total_decisions']
            stats['llm_ratio'] = stats['llm_decisions'] / stats['total_decisions']
            stats['external_dependency'] = stats['llm_ratio']

        stats['adaptive_threshold'] = self.adaptive_threshold

        return stats


# ä¾¿æ·å‡½æ•°
def create_hybrid_decision_engine(
    state_dim: int = 64,
    action_dim: int = 4,
    device: str = 'cpu',
    enable_fractal: bool = True,
    enable_meta_learning: bool = True
) -> HybridDecisionEngine:
    """åˆ›å»ºæ··åˆå†³ç­–å¼•æ“"""
    return HybridDecisionEngine(
        state_dim=state_dim,
        action_dim=action_dim,
        device=device,
        enable_fractal=enable_fractal,
        enable_meta_learning=enable_meta_learning
    )
```

### 2.2 å†³ç­–è·¯å¾„é€‰æ‹©é€»è¾‘

```python
# å†³ç­–æµç¨‹ä¼ªä»£ç 
def hybrid_decision(state):
    # 1. å°è¯•ç³»ç»ŸBï¼ˆæœ€å¿«ï¼‰
    if enable_fractal:
        result_fractal = fractal.decide(state)
        if result_fractal.confidence > adaptive_threshold:
            return result_fractal

    # 2. å°è¯•ç³»ç»ŸAï¼ˆä¸­ç­‰ï¼‰
    result_seed = seed.decide(state)
    if result_seed.confidence > adaptive_threshold:
        return result_seed

    # 3. ä½¿ç”¨LLMï¼ˆæœ€æ…¢ä½†æœ€å…¨é¢ï¼‰
    result_llm = llm.decide(state)
    return result_llm

# è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´
def update_threshold(reward):
    if reward > 0:
        threshold -= 0.01  # é™ä½é˜ˆå€¼ï¼Œæ›´å¤šæœ¬åœ°å†³ç­–
    else:
        threshold += 0.01  # æé«˜é˜ˆå€¼ï¼Œæ›´å¤šLLMå†³ç­–
```

---

## ä¸‰ã€å­¦ä¹ é—­ç¯å®ç°

### 3.1 ç»éªŒç®¡ç†å™¨

**æ–‡ä»¶**: `core/experience_manager.py`ï¼ˆæ–°å»ºï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»éªŒç®¡ç†å™¨ (Experience Manager)
æ”¶é›†ã€ç»„ç»‡å’Œåˆ†å‘å†³ç­–ç»éªŒ

åŠŸèƒ½ï¼š
1. æ”¶é›†å†³ç­–ç»éªŒï¼ˆstate, action, reward, next_stateï¼‰
2. ç®¡ç†ç»éªŒå›æ”¾ç¼“å†²åŒº
3. æä¾›æ‰¹æ¬¡é‡‡æ ·
4. æ”¯æŒä¼˜å…ˆçº§ç»éªŒå›æ”¾ï¼ˆPERï¼‰

ä½œè€…ï¼šClaude Code (Sonnet 4.5)
åˆ›å»ºæ—¥æœŸï¼š2026-01-13
"""

import numpy as np
import random
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from collections import deque

from core.seed import Experience


@dataclass
class EnhancedExperience:
    """å¢å¼ºç»éªŒï¼ˆåŒ…å«å†³ç­–è·¯å¾„ï¼‰"""
    state: np.ndarray
    action: int
    reward: float
    next_state: np.ndarray
    path: str  # 'fractal', 'seed', 'llm'
    confidence: float
    response_time_ms: float
    timestamp: float


class ExperienceManager:
    """ç»éªŒç®¡ç†å™¨"""

    def __init__(self, capacity: int = 10000):
        self.capacity = capacity
        self.buffer: List[EnhancedExperience] = []
        self.position = 0

        # ä¼˜å…ˆçº§ç»éªŒå›æ”¾
        self.priorities = np.zeros(capacity)
        self.alpha = 0.6  # ä¼˜å…ˆçº§æŒ‡æ•°
        self.beta = 0.4   # é‡è¦æ€§é‡‡æ ·æŒ‡æ•°

        # ç»Ÿè®¡
        self.total_experiences = 0
        self.episode_rewards: List[float] = []

        logger = logging.getLogger(__name__)
        logger.info(f"[ç»éªŒç®¡ç†å™¨] åˆå§‹åŒ–å®Œæˆï¼Œå®¹é‡={capacity}")

    def add_experience(
        self,
        state: np.ndarray,
        action: int,
        reward: float,
        next_state: np.ndarray,
        path: str,
        confidence: float,
        response_time_ms: float
    ):
        """æ·»åŠ ç»éªŒ"""
        exp = EnhancedExperience(
            state=state.copy(),
            action=action,
            reward=reward,
            next_state=next_state.copy(),
            path=path,
            confidence=confidence,
            response_time_ms=response_time_ms,
            timestamp=time.time()
        )

        if len(self.buffer) < self.capacity:
            self.buffer.append(exp)
            self.priorities[len(self.buffer) - 1] = max(self.priorities)
        else:
            self.buffer[self.position] = exp
            self.position = (self.position + 1) % self.capacity

        self.total_experiences += 1

    def sample(self, batch_size: int = 32) -> List[Experience]:
        """é‡‡æ ·ä¸€æ‰¹ç»éªŒï¼ˆç”¨äºTheSeedå­¦ä¹ ï¼‰"""
        if len(self.buffer) < batch_size:
            batch_size = len(self.buffer)

        # ä¼˜å…ˆçº§é‡‡æ ·
        probs = self.priorities[:len(self.buffer)] ** self.alpha
        probs /= probs.sum()

        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        batch = [self.buffer[i] for i in indices]

        # è½¬æ¢ä¸ºæ ‡å‡†Experienceå¯¹è±¡
        standard_batch = [
            Experience(
                state=exp.state,
                action=exp.action,
                reward=exp.reward,
                next_state=exp.next_state
            )
            for exp in batch
        ]

        return standard_batch

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if len(self.buffer) == 0:
            return {'size': 0}

        paths = [exp.path for exp in self.buffer]
        rewards = [exp.reward for exp in self.buffer]
        confidences = [exp.confidence for exp in self.buffer]

        return {
            'size': len(self.buffer),
            'total_experiences': self.total_experiences,
            'path_distribution': {
                'fractal': paths.count('fractal'),
                'seed': paths.count('seed'),
                'llm': paths.count('llm')
            },
            'avg_reward': np.mean(rewards),
            'avg_confidence': np.mean(confidences),
            'total_reward': sum(rewards)
        }
```

### 3.2 å¥–åŠ±å‡½æ•°

**æ–‡ä»¶**: `core/reward_function.py`ï¼ˆæ–°å»ºï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¥–åŠ±å‡½æ•° (Reward Function)
å®šä¹‰å†³ç­–çš„å¥–åŠ±ä¿¡å·

åŠŸèƒ½ï¼š
1. ç½®ä¿¡åº¦å¥–åŠ±ï¼ˆé«˜ç½®ä¿¡åº¦â†’æ­£å¥–åŠ±ï¼‰
2. é€Ÿåº¦å¥–åŠ±ï¼ˆå¿«å“åº”â†’æ­£å¥–åŠ±ï¼‰
3. æ¢ç´¢å¥–åŠ±ï¼ˆé€‚åº¦ç†µâ†’æ­£å¥–åŠ±ï¼‰
4. ä»»åŠ¡ç‰¹å®šå¥–åŠ±ï¼ˆå¤–éƒ¨æä¾›ï¼‰

ä½œè€…ï¼šClaude Code (Sonnet 4.5)
åˆ›å»ºæ—¥æœŸï¼š2026-01-13
"""

import numpy as np
from typing import Dict, Any


def compute_reward(
    state: np.ndarray,
    action: int,
    result: Any,
    next_state: np.ndarray,
    context: Dict[str, Any] = None
) -> float:
    """
    è®¡ç®—å¥–åŠ±ä¿¡å·

    Args:
        state: å½“å‰çŠ¶æ€
        action: æ‰§è¡Œçš„åŠ¨ä½œ
        result: å†³ç­–ç»“æœï¼ˆDecisionResultå¯¹è±¡ï¼‰
        next_state: ä¸‹ä¸€ä¸ªçŠ¶æ€
        context: é¢å¤–ä¸Šä¸‹æ–‡

    Returns:
        reward: å¥–åŠ±å€¼ï¼ˆæ­£å€¼è¡¨ç¤ºå¥½ï¼Œè´Ÿå€¼è¡¨ç¤ºåï¼‰
    """
    context = context or {}
    reward = 0.0

    # 1. ç½®ä¿¡åº¦å¥–åŠ±ï¼ˆé«˜ç½®ä¿¡åº¦â†’æ­£å¥–åŠ±ï¼‰
    if hasattr(result, 'confidence'):
        confidence = result.confidence
        if confidence > 0.6:
            reward += 0.2
        elif confidence < 0.4:
            reward -= 0.1

    # 2. é€Ÿåº¦å¥–åŠ±ï¼ˆå¿«å“åº”â†’æ­£å¥–åŠ±ï¼‰
    if hasattr(result, 'response_time_ms'):
        response_time = result.response_time_ms
        if response_time < 50:
            reward += 0.3  # æé€Ÿ
        elif response_time < 100:
            reward += 0.1  # å¿«é€Ÿ
        elif response_time > 500:
            reward -= 0.2  # æ…¢

    # 3. å¤–éƒ¨ä¾èµ–æƒ©ç½š
    if hasattr(result, 'needs_validation') and result.needs_validation:
        reward -= 0.2

    # 4. æ¢ç´¢å¥–åŠ±ï¼ˆé€‚åº¦ç†µâ†’æ­£å¥–åŠ±ï¼‰
    if hasattr(result, 'entropy'):
        entropy = result.entropy
        if 0.2 < entropy < 0.6:
            reward += 0.1  # é€‚åº¦æ¢ç´¢
        elif entropy < 0.05:
            reward -= 0.05  # è¿‡äºç¡®å®š
        elif entropy > 0.9:
            reward -= 0.05  # è¿‡åº¦éšæœº

    # 5. ä»»åŠ¡ç‰¹å®šå¥–åŠ±ï¼ˆå¤–éƒ¨æä¾›ï¼‰
    if 'task_reward' in context:
        reward += context['task_reward']

    # 6. å½’ä¸€åŒ–åˆ°[-1, 1]
    reward = np.clip(reward, -1.0, 1.0)

    return reward
```

---

## å››ã€ç»Ÿä¸€è¿è¡Œå™¨

### 4.1 é›†æˆç³»ç»Ÿè¿è¡Œè„šæœ¬

**æ–‡ä»¶**: `run_unified_agi.py`ï¼ˆæ–°å»ºï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€AGIç³»ç»Ÿè¿è¡Œå™¨ (Unified AGI System)
èåˆç³»ç»ŸAå’Œç³»ç»ŸBçš„å®Œæ•´AGIç³»ç»Ÿ

æ ¸å¿ƒç‰¹æ€§ï¼š
1. æ··åˆå†³ç­–å¼•æ“ï¼ˆA+Bé›†æˆï¼‰
2. å®Œæ•´å­¦ä¹ é—­ç¯
3. 3Dæ‹“æ‰‘å¯¹é½ï¼ˆ7å±‚æ¶æ„ï¼‰
4. è‡ªä¸»æ™ºèƒ½ã€è‡ªæŒ‡æ¶‰ã€åˆ†å½¢è¿›åŒ–

ä½œè€…ï¼šClaude Code (Sonnet 4.5)
åˆ›å»ºæ—¥æœŸï¼š2026-01-13
"""

import sys
import os
import time
import json
import numpy as np
import signal
from pathlib import Path
from datetime import datetime
from typing import Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from core.hybrid_decision_engine import HybridDecisionEngine, DecisionPath, DecisionResult
from core.experience_manager import ExperienceManager
from core.reward_function import compute_reward
from monitoring.fractal_monitor import get_monitor
from config.fractal_config import FractalConfig, IntelligenceMode


class UnifiedAGISystem:
    """ç»Ÿä¸€AGIç³»ç»Ÿï¼ˆç³»ç»ŸA+Bé›†æˆï¼‰"""

    def __init__(self, config: Optional[FractalConfig] = None):
        """åˆå§‹åŒ–ç»Ÿä¸€AGIç³»ç»Ÿ"""
        self.config = config or FractalConfig(mode=IntelligenceMode.HYBRID)
        self.running = True

        # æ ¸å¿ƒç»„ä»¶
        self.decision_engine = None
        self.exp_manager = None
        self.monitor = None

        # ç»Ÿè®¡
        self.stats = {
            'total_decisions': 0,
            'total_reward': 0.0,
            'start_time': datetime.now()
        }

        # ä¿¡å·å¤„ç†
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

        self._initialize()

    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†"""
        print(f"\n[ç³»ç»Ÿ] æ¥æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­...")
        self.running = False

    def _initialize(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        print("\n" + "="*70)
        print(" " * 15 + "ç»Ÿä¸€AGIç³»ç»Ÿï¼ˆç³»ç»ŸA+Bé›†æˆï¼‰å¯åŠ¨ä¸­...")
        print("="*70)

        # 1. åˆ›å»ºæ··åˆå†³ç­–å¼•æ“
        print("\n[æ ¸å¿ƒ] åˆå§‹åŒ–æ··åˆå†³ç­–å¼•æ“...")
        self.decision_engine = HybridDecisionEngine(
            state_dim=64,
            action_dim=4,
            device='cpu',
            enable_fractal=True,
            enable_meta_learning=True
        )
        print("[æˆåŠŸ] æ··åˆå†³ç­–å¼•æ“å·²å¯åŠ¨ï¼ˆç³»ç»ŸA+Bé›†æˆï¼‰")

        # 2. åˆ›å»ºç»éªŒç®¡ç†å™¨
        print("\n[å­¦ä¹ ] åˆå§‹åŒ–ç»éªŒç®¡ç†å™¨...")
        self.exp_manager = ExperienceManager(capacity=10000)
        print("[æˆåŠŸ] ç»éªŒç®¡ç†å™¨å·²å¯åŠ¨")

        # 3. å¯åŠ¨ç›‘æ§
        print("\n[ç›‘æ§] å¯åŠ¨ç›‘æ§ç³»ç»Ÿ...")
        self.monitor = get_monitor(self.config)
        self.monitor.start()
        print("[æˆåŠŸ] ç›‘æ§ç³»ç»Ÿå·²å¯åŠ¨")

        print("\n" + "="*70)
        print(" " * 20 + "ç»Ÿä¸€AGIç³»ç»Ÿå¯åŠ¨å®Œæˆï¼")
        print("="*70)
        print("\n[æ¶æ„] 7å±‚æ‹“æ‰‘å¯¹é½:")
        print("  Layer 0 (å…¥å£): AGI_Life_Engine, IntentDialogueBridge")
        print("  Layer 1 (è®¤çŸ¥æ ¸å¿ƒ): æ··åˆå†³ç­–å¼•æ“ï¼ˆA+Bé›†æˆï¼‰")
        print("  Layer 2-6: æ™ºèƒ½ä½“/è®°å¿†/è¿›åŒ–/æ„ŸçŸ¥/å¤–å›´ç³»ç»Ÿ")
        print("\n[æç¤º] ç³»ç»Ÿå°†è‡ªä¸»è¿è¡Œå¹¶æŒç»­å­¦ä¹ ")
        print("[æç¤º] æŒ‰ Ctrl+C ä¼˜é›…é€€å‡º")

    def make_decision(self, state: Optional[np.ndarray] = None) -> DecisionResult:
        """æ‰§è¡Œä¸€æ¬¡å†³ç­–ï¼ˆå®Œæ•´å­¦ä¹ é—­ç¯ï¼‰"""
        # 1. ç”Ÿæˆæˆ–ä½¿ç”¨æä¾›çš„çŠ¶æ€
        if state is None:
            state = np.random.randn(64)

        # 2. æ··åˆå†³ç­–
        result = self.decision_engine.decide(state)

        # 3. æ¨¡æ‹Ÿç¯å¢ƒè½¬ç§»ï¼ˆç®€åŒ–ï¼‰
        next_state = np.random.randn(64)

        # 4. è®¡ç®—å¥–åŠ±
        reward = compute_reward(state, result.action, result, next_state)

        # 5. æ”¶é›†ç»éªŒ
        self.exp_manager.add_experience(
            state=state,
            action=result.action,
            reward=reward,
            next_state=next_state,
            path=result.path.value,
            confidence=result.confidence,
            response_time_ms=result.response_time_ms
        )

        # 6. å­¦ä¹ ï¼ˆæ¯10æ¬¡å†³ç­–å­¦ä¹ ä¸€æ¬¡ï¼‰
        if self.stats['total_decisions'] % 10 == 0:
            batch = self.exp_manager.sample(batch_size=32)
            for exp in batch:
                self.decision_engine.learn(
                    exp.state,
                    exp.action,
                    exp.reward,
                    exp.next_state
                )

        # 7. æ›´æ–°ç»Ÿè®¡
        self.stats['total_decisions'] += 1
        self.stats['total_reward'] += reward

        # 8. ç›‘æ§è®°å½•
        self.monitor.record_decision(
            response_time_ms=result.response_time_ms,
            confidence=result.confidence,
            entropy=result.entropy,
            source=result.path.value,
            needs_validation=result.needs_validation
        )

        return result

    def get_dashboard(self) -> str:
        """è·å–ç³»ç»Ÿä»ªè¡¨æ¿"""
        stats = self.decision_engine.get_statistics()
        exp_stats = self.exp_manager.get_statistics()

        runtime = datetime.now() - self.stats['start_time']
        runtime_str = str(runtime).split('.')[0]

        dashboard = f"""
{'='*70}
                    ç»Ÿä¸€AGIç³»ç»Ÿå®æ—¶ä»ªè¡¨æ¿ï¼ˆA+Bé›†æˆï¼‰
{'='*70}

[è¿è¡Œä¿¡æ¯]
  å¯åŠ¨æ—¶é—´: {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}
  è¿è¡Œæ—¶é•¿: {runtime_str}
  æ€»å†³ç­–æ•°: {self.stats['total_decisions']}
  ç´¯è®¡å¥–åŠ±: {self.stats['total_reward']:.2f}

[å†³ç­–è·¯å¾„åˆ†å¸ƒ]
  ç³»ç»ŸBï¼ˆåˆ†å½¢ï¼‰: {stats.get('fractal_ratio', 0):.1%}
  ç³»ç»ŸAï¼ˆTheSeedï¼‰: {stats.get('seed_ratio', 0):.1%}
  å¤–éƒ¨LLM: {stats.get('llm_ratio', 0):.1%}
  å¤–éƒ¨ä¾èµ–ç‡: {stats.get('external_dependency', 0):.1%}

[å­¦ä¹ ç»Ÿè®¡]
  ç»éªŒæ± å¤§å°: {exp_stats.get('size', 0)}
  å¹³å‡å¥–åŠ±: {exp_stats.get('avg_reward', 0):.4f}
  å¹³å‡ç½®ä¿¡åº¦: {exp_stats.get('avg_confidence', 0):.4f}

[è‡ªé€‚åº”å‚æ•°]
  åŠ¨æ€é˜ˆå€¼: {stats.get('adaptive_threshold', 0.5):.4f}

{'='*70}
"""
        return dashboard

    def run_interactive(self):
        """äº¤äº’æ¨¡å¼"""
        print("\n[æ¨¡å¼] äº¤äº’æ¨¡å¼")
        print("[è¯´æ˜] è¾“å…¥å‘½ä»¤æ‰§è¡Œæ“ä½œï¼Œè¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©\n")

        self._show_help()

        while self.running:
            try:
                cmd = input("\n[ç»Ÿä¸€AGI] > ").strip().lower()

                if not cmd:
                    continue

                if cmd in ['exit', 'quit', 'q']:
                    print("[ç³»ç»Ÿ] é€€å‡ºä¸­...")
                    break

                elif cmd == 'help':
                    self._show_help()

                elif cmd == 'status':
                    print(self.get_dashboard())

                elif cmd == 'decision':
                    result = self.make_decision()
                    print(f"\n[å†³ç­–ç»“æœ]")
                    print(f"  è·¯å¾„: {result.path.value}")
                    print(f"  åŠ¨ä½œ: {result.action}")
                    print(f"  ç½®ä¿¡åº¦: {result.confidence:.4f}")
                    print(f"  å“åº”æ—¶é—´: {result.response_time_ms:.2f}ms")
                    print(f"  è¯´æ˜: {result.explanation}")

                elif cmd == 'batch':
                    print("\n[æ‰¹é‡] æ‰§è¡Œ10æ¬¡å†³ç­–...")
                    for i in range(10):
                        result = self.make_decision()
                        print(f"  [{i+1}/10] {result.path.value} - "
                              f"ç½®ä¿¡åº¦={result.confidence:.4f}, "
                              f"å“åº”={result.response_time_ms:.2f}ms")
                    print("\n" + self.get_dashboard())

                elif cmd == 'clear':
                    os.system('cls' if os.name == 'nt' else 'clear')

                else:
                    print(f"[æœªçŸ¥] æœªçŸ¥å‘½ä»¤: {cmd}")
                    print("[æç¤º] è¾“å…¥ 'help' æŸ¥çœ‹å¯ç”¨å‘½ä»¤")

            except KeyboardInterrupt:
                print("\n\n[ç³»ç»Ÿ] æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·")
                break
            except Exception as e:
                print(f"[é”™è¯¯] {e}")

    def run_demo(self):
        """æ¼”ç¤ºæ¨¡å¼"""
        print("\n[æ¨¡å¼] æ¼”ç¤ºæ¨¡å¼ï¼šè‡ªåŠ¨æ‰§è¡Œå†³ç­–")
        print("[è¯´æ˜] æ¯5ç§’æ‰§è¡Œä¸€æ¬¡å†³ç­–ï¼Œå®æ—¶å±•ç¤ºç³»ç»ŸçŠ¶æ€\n")

        update_interval = 5
        decision_count = 0
        max_decisions = 100

        try:
            while self.running and decision_count < max_decisions:
                result = self.make_decision()
                decision_count += 1

                print(f"[å†³ç­– {decision_count}] "
                      f"è·¯å¾„={result.path.value}, "
                      f"å“åº”={result.response_time_ms:.2f}ms, "
                      f"ç½®ä¿¡åº¦={result.confidence:.4f}, "
                      f"å¥–åŠ±={self.stats['total_reward']:.2f}")

                if decision_count % 10 == 0:
                    print(self.get_dashboard())

                time.sleep(update_interval)

        except KeyboardInterrupt:
            print("\n\n[ç³»ç»Ÿ] æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·")

    def shutdown(self):
        """ä¼˜é›…å…³é—­"""
        print("\n[å…³é—­] æ­£åœ¨å…³é—­ç»Ÿä¸€AGIç³»ç»Ÿ...")

        # åœæ­¢ç›‘æ§
        if self.monitor:
            self.monitor.stop()

        # ä¿å­˜æœ€ç»ˆç»Ÿè®¡
        self._save_final_stats()

        print("[å®Œæˆ] ç»Ÿä¸€AGIç³»ç»Ÿå·²å…³é—­")

    def _save_final_stats(self):
        """ä¿å­˜æœ€ç»ˆç»Ÿè®¡"""
        stats = {
            'shutdown_time': datetime.now().isoformat(),
            'runtime': str(datetime.now() - self.stats['start_time']),
            'total_decisions': self.stats['total_decisions'],
            'total_reward': self.stats['total_reward'],
            'decision_stats': self.decision_engine.get_statistics(),
            'experience_stats': self.exp_manager.get_statistics()
        }

        stats_file = project_root / "monitoring" / "unified_agi_final_stats.json"
        stats_file.parent.mkdir(parents=True, exist_ok=True)

        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False, default=str)

        print(f"[ä¿å­˜] æœ€ç»ˆç»Ÿè®¡: {stats_file}")

    def _show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©"""
        help_text = """
[å‘½ä»¤å¸®åŠ©]
  help      - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
  status    - æ˜¾ç¤ºå®Œæ•´ç³»ç»ŸçŠ¶æ€ä»ªè¡¨æ¿
  decision  - æ‰§è¡Œä¸€æ¬¡å†³ç­–å¹¶æ˜¾ç¤ºç»“æœ
  batch     - æ‰¹é‡æ‰§è¡Œ10æ¬¡å†³ç­–
  clear     - æ¸…å±
  exit/quit/q - é€€å‡ºç³»ç»Ÿ

[ç³»ç»Ÿè¯´æ˜]
  - ç»Ÿä¸€AGIç³»ç»Ÿ = ç³»ç»ŸAï¼ˆç»„ä»¶ç»„è£…ï¼‰+ ç³»ç»ŸBï¼ˆåˆ†å½¢æ‹“æ‰‘ï¼‰
  - æ··åˆå†³ç­–ï¼šFractalï¼ˆå¿«ï¼‰â†’ TheSeedï¼ˆä¸­ï¼‰â†’ LLMï¼ˆæ…¢ï¼‰
  - å®Œæ•´å­¦ä¹ é—­ç¯ï¼šç»éªŒæ”¶é›† â†’ å¥–åŠ±è®¡ç®— â†’ å‚æ•°æ›´æ–°
  - 7å±‚æ‹“æ‰‘å¯¹é½ï¼šå…¥å£/è®¤çŸ¥æ ¸å¿ƒ/æ™ºèƒ½ä½“/è®°å¿†/è¿›åŒ–/æ„ŸçŸ¥/å¤–å›´
"""
        print(help_text)


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='ç»Ÿä¸€AGIç³»ç»Ÿï¼ˆA+Bé›†æˆï¼‰')
    parser.add_argument('--mode', type=str, default='interactive',
                       choices=['demo', 'interactive'],
                       help='è¿è¡Œæ¨¡å¼')

    args = parser.parse_args()

    # åˆ›å»ºç³»ç»Ÿ
    system = UnifiedAGISystem()

    # è¿è¡Œ
    try:
        if args.mode == 'demo':
            system.run_demo()
        else:
            system.run_interactive()
    finally:
        system.shutdown()


if __name__ == "__main__":
    main()
```

---

## äº”ã€å®æ–½è®¡åˆ’

### 5.1 é˜¶æ®µåˆ’åˆ†

#### é˜¶æ®µ1ï¼šæ ¸å¿ƒé›†æˆï¼ˆç«‹å³ï¼Œ2-3å°æ—¶ï¼‰

- [ ] åˆ›å»º`core/hybrid_decision_engine.py`
- [ ] åˆ›å»º`core/experience_manager.py`
- [ ] åˆ›å»º`core/reward_function.py`
- [ ] åˆ›å»º`run_unified_agi.py`

#### é˜¶æ®µ2ï¼šæµ‹è¯•éªŒè¯ï¼ˆä»Šå¤©ï¼Œ1-2å°æ—¶ï¼‰

- [ ] å•å…ƒæµ‹è¯•ï¼šæ··åˆå†³ç­–å¼•æ“
- [ ] é›†æˆæµ‹è¯•ï¼šå®Œæ•´å­¦ä¹ é—­ç¯
- [ ] æ€§èƒ½æµ‹è¯•ï¼šå“åº”æ—¶é—´
- [ ] å¯¹æ¯”æµ‹è¯•ï¼šç³»ç»ŸA vs ç³»ç»ŸB vs ç»Ÿä¸€ç³»ç»Ÿ

#### é˜¶æ®µ3ï¼šä¼˜åŒ–æå‡ï¼ˆæœ¬å‘¨ï¼‰

- [ ] ä¼˜åŒ–è‡ªé€‚åº”é˜ˆå€¼ç®—æ³•
- [ ] æ·»åŠ æ›´å¤šå¥–åŠ±å‡½æ•°
- [ ] å®ç°ä¼˜å…ˆçº§ç»éªŒå›æ”¾
- [ ] é›†æˆMetaLearner

#### é˜¶æ®µ4ï¼šåŠŸèƒ½æ‰©å±•ï¼ˆæœ¬æœˆï¼‰

- [ ] æ·»åŠ ä»»åŠ¡ç¯å¢ƒï¼ˆæ›¿ä»£éšæœºçŠ¶æ€ï¼‰
- [ ] é›†æˆåˆ°AGI_Life_Engine
- [ ] 3Då¯è§†åŒ–æ›´æ–°
- [ ] æ–‡æ¡£å®Œå–„

### 5.2 æˆåŠŸæŒ‡æ ‡

| æŒ‡æ ‡ | å½“å‰ | ç›®æ ‡ï¼ˆ1å‘¨åï¼‰ | ç›®æ ‡ï¼ˆ1æœˆåï¼‰ |
|------|------|--------------|--------------|
| å¤–éƒ¨ä¾èµ–ç‡ | 100% | 60-70% | 20-30% |
| å¹³å‡ç½®ä¿¡åº¦ | 0.50 | 0.55-0.60 | 0.65-0.75 |
| ç³»ç»ŸBä½¿ç”¨ç‡ | 0% | 30-40% | 50-60% |
| å“åº”æ—¶é—´ | 15ms | <20ms | <30ms |
| å­¦ä¹ å¯è§æ€§ | æ—  | æ˜æ˜¾è¶‹åŠ¿ | æŒç»­æ”¹å–„ |

---

## å…­ã€3Dæ‹“æ‰‘æ›´æ–°

### 6.1 æ›´æ–°system_topology_3d.html

åœ¨Layer 1ï¼ˆè®¤çŸ¥æ ¸å¿ƒï¼‰æ·»åŠ ï¼š

```javascript
// ç³»ç»ŸBï¼ˆåˆ†å½¢æ‹“æ‰‘æ™ºèƒ½ï¼‰
{
  id: "FractalIntelligence",
  layer: 1,
  file: "core/fractal_intelligence.py",
  desc: "åˆ†å½¢æ‹“æ‰‘æ™ºèƒ½ - è‡ªæŒ‡æ¶‰ã€é€’å½’ã€ç›®æ ‡å¯å¡‘ï¼ˆç³»ç»ŸBï¼‰",
  size: 2.8,
  x: -40, y: 40, z: -20,
  color: 0xff9900  // æ©™è‰²æ ‡è¯†
},

// æ··åˆå†³ç­–å¼•æ“
{
  id: "HybridDecisionEngine",
  layer: 0,
  file: "core/hybrid_decision_engine.py",
  desc: "æ··åˆå†³ç­–å¼•æ“ - èåˆç³»ç»ŸAå’ŒBçš„å†³ç­–èƒ½åŠ›",
  size: 3.2,
  x: 0, y: 60, z: -20,
  color: 0x00ffaa  // é’ç»¿è‰²æ ‡è¯†
},
```

### 6.2 è¿æ¥å…³ç³»

```javascript
// æ–°å¢è¿æ¥
{ source: "HybridDecisionEngine", target: "FractalIntelligence", type: "control" },
{ source: "HybridDecisionEngine", target: "TheSeed", type: "control" },
{ source: "HybridDecisionEngine", target: "LLMService", type: "control" },
{ source: "FractalIntelligence", target: "MetaLearner", type: "data" },
{ source: "FractalIntelligence", target: "BiologicalMemory", type: "data" },
```

---

## ä¸ƒã€é¢„æœŸæ•ˆæœ

### 7.1 è‡ªä¸»æ™ºèƒ½

- **å‡å°‘å¤–éƒ¨ä¾èµ–**ï¼šä»100% â†’ 20-30%
- **æå‡æœ¬åœ°å†³ç­–**ï¼šç³»ç»ŸBä½¿ç”¨ç‡0% â†’ 50-60%
- **æŒç»­å­¦ä¹ **ï¼šå®Œæ•´å­¦ä¹ é—­ç¯ï¼Œç½®ä¿¡åº¦æŒç»­å¢é•¿

### 7.2 è‡ªæŒ‡æ¶‰

- **MetaLearner**ï¼šå…ƒå‚æ•°ä¼˜åŒ–ï¼Œè‡ªé€‚åº”é˜ˆå€¼
- **GoalQuestioner**ï¼šç›®æ ‡è´¨ç–‘ï¼Œé˜²æ­¢ç›®æ ‡æ¼‚ç§»
- **RecursiveSelfMemory**ï¼šè®°ä½"å¦‚ä½•è®°å¿†"

### 7.3 æ¢¯åº¦åˆ†å½¢è¿›åŒ–

- **è‡ªæŒ‡æ¶‰åˆ†å½¢æ ¸å¿ƒ**ï¼šÎ¦ = f(Î¦, x)
- **é€’å½’æ·±åº¦**ï¼šmax_recursion = 3 â†’ 5
- **ç›®æ ‡å¯å¡‘æ€§**ï¼šèƒ½è´¨ç–‘å’Œä¿®æ”¹ä¼˜åŒ–ç›®æ ‡

---

## å…«ã€é£é™©è¯„ä¼°

### 8.1 æŠ€æœ¯é£é™©

| é£é™© | å¯èƒ½æ€§ | å½±å“ | ç¼“è§£æªæ–½ |
|------|-------|------|----------|
| é›†æˆå†²çª | ä¸­ | é«˜ | ä¿æŒæ¨¡å—åŒ–ï¼Œå……åˆ†æµ‹è¯• |
| æ€§èƒ½ä¸‹é™ | ä½ | ä¸­ | ç›‘æ§å“åº”æ—¶é—´ |
| å­¦ä¹ ä¸ç¨³å®š | ä¸­ | é«˜ | å°å­¦ä¹ ç‡ï¼Œæ¢¯åº¦è£å‰ª |

### 8.2 å®æ–½é£é™©

| é£é™© | å¯èƒ½æ€§ | å½±å“ | ç¼“è§£æªæ–½ |
|------|-------|------|----------|
| æ—¶é—´ä¼°ç®—é”™è¯¯ | ä¸­ | ä¸­ | åˆ†é˜¶æ®µå®æ–½ |
| èµ„æºä¸è¶³ | ä½ | ä½ | ä¼˜åŒ–ä»£ç ï¼Œè½»é‡çº§ |

---

## ä¹ã€æ€»ç»“

### 9.1 æ ¸å¿ƒä»·å€¼

**ç»Ÿä¸€AGIç³»ç»Ÿçš„ä¼˜åŠ¿**ï¼š
1. âœ… **èåˆä¼˜åŠ¿**ï¼šç³»ç»ŸAçš„åŠŸèƒ½å…¨é¢ + ç³»ç»ŸBçš„å®æ—¶è‡ªä¸»
2. âœ… **å­¦ä¹ é—­ç¯**ï¼šå®Œæ•´çš„ä¸‰è·¯å­¦ä¹ ï¼ˆFractal + TheSeed + MetaLearnerï¼‰
3. âœ… **è‡ªé€‚åº”**ï¼šåŠ¨æ€é˜ˆå€¼ï¼Œè‡ªåŠ¨è°ƒæ•´å†³ç­–è·¯å¾„
4. âœ… **æ‹“æ‰‘å¯¹é½**ï¼šç¬¦åˆ3Dæ¶æ„çš„7å±‚è®¾è®¡

### 9.2 ä¸åŸç³»ç»Ÿå¯¹æ¯”

| ç‰¹æ€§ | ç³»ç»ŸA | ç³»ç»ŸB | ç»Ÿä¸€ç³»ç»Ÿ |
|------|-------|-------|----------|
| åŠŸèƒ½å…¨é¢æ€§ | 9/10 | 4/10 | **9/10** |
| å®æ—¶æ€§ | 6/10 | 10/10 | **10/10** |
| è‡ªä¸»æ€§ | 4/10 | 8/10 | **8/10** |
| å­¦ä¹ é—­ç¯ | âŒ æ–­è£‚ | âŒ æ–­è£‚ | **âœ… å®Œæ•´** |
| å¤–éƒ¨ä¾èµ– | é«˜ | 100% | **ä½ï¼ˆç›®æ ‡<30%ï¼‰** |
| **ç»¼åˆè¯„åˆ†** | **7.4/10** | **6.4/10** | **8.5/10** |

### 9.3 æœ€ç»ˆå»ºè®®

**ç«‹å³è¡ŒåŠ¨**ï¼š
1. åˆ›å»º`core/hybrid_decision_engine.py`
2. åˆ›å»º`core/experience_manager.py`
3. åˆ›å»º`run_unified_agi.py`
4. è¿è¡Œæµ‹è¯•éªŒè¯

**æœ¬å‘¨ç›®æ ‡**ï¼š
1. å®Œæˆæ ¸å¿ƒé›†æˆ
2. éªŒè¯å­¦ä¹ é—­ç¯
3. å¯¹æ¯”ä¸‰ä¸ªç³»ç»Ÿï¼ˆA/B/ç»Ÿä¸€ï¼‰

**æœ¬æœˆç›®æ ‡**ï¼š
1. å¤–éƒ¨ä¾èµ–ç‡<50%
2. å¹³å‡ç½®ä¿¡åº¦>0.60
3. ç³»ç»ŸBä½¿ç”¨ç‡>40%

---

**æŠ¥å‘Šç»“æŸ**

**ä½œè€…**: Claude Code (Sonnet 4.5)
**æ—¥æœŸ**: 2026-01-13
**ç‰ˆæœ¬**: v1.0
