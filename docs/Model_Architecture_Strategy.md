# AGI 模型架构策略：全云端 API 主导 (Cloud-First Strategy)

## 1. 核心架构决策
**当前状态**: 系统坚持 **"全云端 API 主导" (Cloud-First)** 的架构路线。
*   **拒绝本地化**: 暂时不引入 Ollama 等本地模型，避免增加部署复杂度和硬件门槛。
*   **集中智能**: 所有认知、规划、执行指令均由云端大模型 (DeepSeek-V3/GPT-4) 统一分发，确保逻辑一致性。

## 2. Token 消耗与成本管理
*   **上下文窗口**: `AGI_Life_Engine` 限制文件读取为前1000字符，并只采样10-15个文件作为上下文。
*   **成本模型**: 按 Token 计费。通过精简 Prompt 和 Context 来控制成本，而不是通过引入免费的本地“弱智”模型。

## 3. 云端 API 的不可替代性
在当前阶段，云端 API 提供了本地模型无法比拟的优势：

| 特性 | 云端大模型 (DeepSeek-V3) | 优势分析 |
| :--- | :--- | :--- |
| **智能深度** | **高 (System 2)** | 能够处理复杂的代码重构、架构反思，这是 AGI 的核心。 |
| **部署成本** | **零** | 无需购买昂贵的显卡，无需维护本地推理服务。 |
| **一致性** | **高** | 统一的知识库和推理逻辑，避免“大小脑”打架。 |

## 4. 演进路线调整
我们**取消**了向 "端云协同" 演进的计划，转而专注于 **"云端效能优化"**：

1.  **Prompt Engineering**: 优化云端指令，用更少的 Token 做更多的事。
2.  **Context Management**: 智能筛选上传的文件上下文，只传最重要的代码片段。
3.  **Result Caching**: 对重复的分析结果进行缓存，减少重复 API 调用。

*(注：已从 `core/llm_client.py` 中移除了 Ollama 相关代码，保持系统纯净)*
