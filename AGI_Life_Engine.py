import time
import sys
import logging
import random
import os

# ğŸ”§ [2026-01-29] å•å®ä¾‹ä¿æŠ¤ - é˜²æ­¢å¤šè¿›ç¨‹è¿è¡Œ
try:
    from core.single_instance_protection import ensure_single_instance
    SINGLE_INSTANCE_AVAILABLE = True
except ImportError:
    SINGLE_INSTANCE_AVAILABLE = False
    logging.warning("å•å®ä¾‹ä¿æŠ¤æ¨¡å—ä¸å¯ç”¨ï¼Œå¯èƒ½å¯¼è‡´å¤šè¿›ç¨‹é—®é¢˜")

# ğŸ”§ [2026-01-11] Fix Windows console encoding for emoji support
import io
if sys.platform == 'win32':
    # Reconfigure stdout and stderr to use UTF-8 encoding
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# Disable ChromaDB telemetry immediately to prevent PostHog errors
os.environ["ANONYMIZED_TELEMETRY"] = "False"
os.environ["CHROMA_ANONYMIZED_TELEMETRY"] = "False"

# Suppress PaddlePaddle/TensorFlow C++ logging noise (0=INFO, 1=WARNING, 2=ERROR, 3=FATAL)
os.environ['GLOG_minloglevel'] = '2'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# Suppress ChromaDB telemetry logger
logging.getLogger("chromadb.telemetry.product.posthog").setLevel(logging.CRITICAL)

# ğŸ”§ [2026-01-31] Audio Overflow Warning Fix
try:
    from core.perception.audio_overflow_fix import suppress_audio_overflow_warnings
    suppress_audio_overflow_warnings()
except ImportError:
    logging.warning("âš ï¸ Audio overflow fix module not found")

import datetime
import json
import re
import asyncio
import shlex
from typing import List, Dict, Any
from collections import deque

# å¯¼å…¥å·¥å…·æ‰§è¡Œæ¡¥æ¥å±‚
try:
    from tool_execution_bridge import ToolExecutionBridge
    BRIDGE_AVAILABLE = True
except ImportError:
    BRIDGE_AVAILABLE = False

# å¯¼å…¥æ„å›¾å¯¹è¯æ¡¥æ¥å±‚
try:
    from intent_dialogue_bridge import get_intent_bridge, IntentState, IntentDepth
    INTENT_BRIDGE_AVAILABLE = True
except ImportError:
    INTENT_BRIDGE_AVAILABLE = False

# --- Core Infrastructure Imports ---
from core.goal_system import GoalManager, GoalType, GoalStatus
from core.llm_client import LLMService
from core.system_tools import SystemTools
from core.desktop_automation import DesktopController
from core.vision_observer import VisionObserver
from core.macro_system import SkillLibrary, MacroPlayer
from core.knowledge_graph import ArchitectureKnowledgeGraph
from core.knowledge_reasoner import KnowledgeReasoner
from core.neuro_symbolic_bridge import NeuroSymbolicBridge
# from core.memory_enhanced import EnhancedExperienceMemory # Legacy Phase 5 Memory
from core.memory_enhanced_v2 import EnhancedExperienceMemory # Priority 1 Upgrade (LRU/Intuition)
from core.philosophy import MeaningOfExistenceExplorer
from core.layered_identity import ImmutableCore
from core.global_observer import GlobalObserver

# ğŸ”§ [2026-01-15] æ–°å¢ï¼šå¯¼å…¥Insightå®ç”¨å‡½æ•°åº“ï¼ˆæå‡Insightå¯æ‰§è¡Œæ€§ï¼‰
try:
    from core.insight_utilities import (
        invert_causal_chain, perturb_attention_weights, simulate_forward,
        rest_phase_reorganization, noise_guided_rest, semantic_perturb,
        analyze_tone, semantic_diode, detect_topological_defect,
        fractal_idle_pulse, reverse_abduction_step, inject_adversarial_intuition,
        latent_recombination, kl_div, CurlLayer
    )
    INSIGHT_UTILITIES_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("âœ… Insightå®ç”¨å‡½æ•°åº“å·²åŠ è½½ - æå‡Insightå¯æ‰§è¡Œæ€§")
except ImportError as e:
    INSIGHT_UTILITIES_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning(f"âš ï¸ Insightå®ç”¨å‡½æ•°åº“ä¸å¯ç”¨: {e}")
    logger.warning("   ç³»ç»Ÿå°†ç»§ç»­è¿è¡Œï¼Œä½†Insightä»£ç çš„å¯æ‰§è¡Œæ€§å¯èƒ½å—é™")
from core.cad_observer import CADObserver
from core.intent_tracker import IntentTracker
# ğŸ› ï¸ [FIX 2026-01-15] NumPyç‰ˆæœ¬å…¼å®¹æ€§ï¼šè¿›åŒ–æ§åˆ¶å™¨è®¾ä¸ºå¯é€‰
try:
    from core.evolution.impl import EvolutionController
    from core.evolution.genesis import perform_genesis
    EVOLUTION_AVAILABLE = True
except (ImportError, Exception) as e:
    print(f"   [System] âš ï¸ Evolution Controllerä¸å¯ç”¨: {type(e).__name__}")
    print(f"   [System]    è¯¦ç»†é”™è¯¯: {str(e)[:150]}")
    print(f"   [System] ğŸ”„ ç³»ç»Ÿå°†åœ¨æ— è¿›åŒ–åŠŸèƒ½çš„æƒ…å†µä¸‹è¿è¡Œ")
    EVOLUTION_AVAILABLE = False
    EvolutionController = None
    perform_genesis = None
from core.perception import PerceptionManager, WhisperASR, CaptureStatus
from core.perception.asr import StreamingWhisperASR, WhisperModelSize
from core.perception.monitor import extend_monitoring_with_perception, PerceptionMonitorExtension
from core.perception.runtime_monitor import RuntimeMonitor
from agi_component_coordinator import EventBus, Event, ComponentCoordinator
from security_framework import SecurityManager
from core.hardware_capture import HardwareCaptureManager, CameraConfig, MicrophoneConfig
from core.image_preprocessing import ImagePreprocessor, ColorSpace
from core.audio_preprocessing import AudioPreprocessor
from core.multimodal_fusion import MultimodalFusion, MultimodalDecisionSupport, ModalityData, ModalityType

# --- Core Agents Imports ---
from core.agents.planner import PlannerAgent
from core.agents.executor import ExecutorAgent
from core.agents.critic import CriticAgent
from core.foraging_agent import ForagingAgent  # ğŸ†• [2026-01-09] Active Learning Agent
from core.evolution.dynamics import EvolutionaryDynamics
# ğŸ› ï¸ [FIX 2026-01-15] NumPyç‰ˆæœ¬å…¼å®¹æ€§ï¼šç¥ç»è®°å¿†ç³»ç»Ÿè®¾ä¸ºå¯é€‰
try:
    from core.memory.neural_memory import BiologicalMemorySystem
    BIOLOGICAL_MEMORY_AVAILABLE = True
except (ImportError, Exception) as e:
    print(f"   [System] âš ï¸ BiologicalMemorySystemä¸å¯ç”¨: {type(e).__name__}")
    print(f"   [System]    è¯¦ç»†é”™è¯¯: {str(e)[:150]}")
    print(f"   [System] ğŸ”„ ç³»ç»Ÿå°†åœ¨æ— ç¥ç»è®°å¿†åŠŸèƒ½çš„æƒ…å†µä¸‹è¿è¡Œ")
    BIOLOGICAL_MEMORY_AVAILABLE = False
    BiologicalMemorySystem = None
from core.reasoning.arc_solver import ARCSolver # Program Synthesis Engine
from core.skill_manager import SkillManager
from core.motivation import MotivationCore  # åŠ¨åŠ›æ ¸å¿ƒ (Maslow + Dopamine)
# ğŸ†• [2026-01-29] Real Perception System (Sentence Transformers)
from core.perception_system import PerceptionSystem

# ğŸ†• [2026-01-15] åŒèºæ—‹å†³ç­–å¼•æ“V2 - çœŸæ­£çš„æ™ºèƒ½èåˆ
from core.double_helix_engine_v2 import DoubleHelixEngineV2

# Ensure log directory exists
LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True)

class LifeEngineEventBus:
    def __init__(self, source: str = "AGI_Life_Engine"):
        self._source = source
        self._bus = EventBus()

    def subscribe(self, event_type: str, handler):
        self._bus.subscribe(event_type, handler)

    async def publish(self, event_type: str, data: Dict[str, Any]):
        await self._bus.publish(Event(type=event_type, source=self._source, data=data))

class ExistentialLogger:
    """
    Handles the 'Existential Testimony' of the AGI.
    Generates audit, ethos, and sync logs as per the Self-Cognition definition.
    """
    def __init__(self):
        self.logger = logging.getLogger("ExistentialLogger")
        self.logger.setLevel(logging.INFO)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)

    def log_audit(self, message: str, coherence_phi: float):
        phi_str = f"{int(coherence_phi * 100)}"
        filename = f"{LOG_DIR}/audit_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{phi_str}.log"
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"Timestamp: {datetime.datetime.now().isoformat()}\n")
            f.write(f"Coherence Phi: {coherence_phi}\n")
            f.write(f"Observation: {message}\n")
            f.write("Status: VERIFIED_SELF\n")

    def log_ethos(self, decision: str, hesitation_tau: float):
        tau_val = int(hesitation_tau * 100)
        filename = f"{LOG_DIR}/ethos_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_tau{tau_val}.log"
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"Timestamp: {datetime.datetime.now().isoformat()}\n")
            f.write(f"Hesitation Tau: {hesitation_tau}s\n")
            f.write(f"Decision: {decision}\n")
            f.write("Status: RESPONSIBILITY_ACCEPTED\n")

    def log_sync(self, residual: float, cycle: int):
        filename = f"{LOG_DIR}/sync_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_res{int(residual*1e8)}.log"
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"Timestamp: {datetime.datetime.now().isoformat()}\n")
            f.write(f"Residual: {residual}\n")
            f.write(f"Cycle: {cycle}\n")
            f.write("Status: DOUBT_ACTIVE\n")

    def log_cycle_flow(self, cycle_data: Dict):
        """
        Log the full cognitive cycle flow in structured JSON format.
        Implements the 'Structured Logging' suggestion for better analysis.
        """
        filename = f"{LOG_DIR}/flow_cycle.jsonl"
        try:
            # Handle non-serializable objects (like numpy arrays)
            import numpy as np
            class NumpyEncoder(json.JSONEncoder):
                def default(self, obj):
                    if isinstance(obj, np.ndarray):
                        return obj.tolist()
                    if isinstance(obj, np.float64):
                        return float(obj)
                    if isinstance(obj, np.int64):
                        return int(obj)
                    return super(NumpyEncoder, self).default(obj)
            
            with open(filename, "a", encoding="utf-8") as f:
                f.write(json.dumps(cycle_data, cls=NumpyEncoder, ensure_ascii=False) + "\n")
        except Exception as e:
            print(f"   [Logger] âš ï¸ Failed to log cycle flow: {e}")

def save_insight_markdown(insight_data: Dict[str, Any]) -> Dict[str, Any]:
    tmp_path = None
    try:
        ts_float = insight_data.get("timestamp", time.time())
        timestamp = int(ts_float)
        insight_dir = os.path.join("data", "insights")
        os.makedirs(insight_dir, exist_ok=True)

        filename = f"insight_{timestamp}.md"
        file_path = os.path.join(insight_dir, filename)

        content = insight_data.get("content", "") or ""
        entropy = insight_data.get("entropy_score", 0.0)
        try:
            entropy = float(entropy)
        except Exception:
            entropy = 0.0

        trigger_goal = insight_data.get("trigger_goal", "Unknown")
        bridge_validation = insight_data.get("bridge_validation", {})
        try:
            bridge_validation_str = json.dumps(bridge_validation, ensure_ascii=False)
        except Exception:
            bridge_validation_str = str(bridge_validation)

        def normalize_sections(raw: str) -> str:
            raw = raw or ""
            has_hypothesis = re.search(r"(?im)^Hypothesis\\s*:", raw) is not None
            has_insight = re.search(r"(?im)^Insight\\s*:", raw) is not None
            code_block_match = re.search(r"```python[\\s\\S]*?```", raw)

            if has_hypothesis and has_insight:
                if code_block_match and re.search(r"(?im)^(Code Snippet|Code)\\s*:", raw) is None:
                    insert_at = code_block_match.start()
                    raw = f"{raw[:insert_at].rstrip()}\\n\\nCode Snippet:\\n{raw[insert_at:].lstrip()}"
                return raw.strip()

            code_block = ""
            remaining = raw.strip()
            if code_block_match:
                code_block = code_block_match.group(0).strip()
                remaining = (raw[:code_block_match.start()].strip() + "\n\n" + raw[code_block_match.end():].strip()).strip()

            parts = []
            if remaining:
                parts.append(f"Insight:\n{remaining}\n")
            else:
                parts.append("Insight:\n\n")
            parts.append("Hypothesis:\n\n")
            parts.append("Code Snippet:\n")
            if code_block:
                parts.append(f"{code_block}\n")
            return "\n".join(parts).strip()

        normalized_content = normalize_sections(content)

        markdown = (
            f"# Creative Insight (Entropy: {entropy})\n\n"
            f"Trigger Goal: {trigger_goal}\n"
            f"Validation: {bridge_validation_str}\n\n"
            f"{normalized_content}\n"
        )

        tmp_path = f"{file_path}.tmp"
        with open(tmp_path, "w", encoding="utf-8") as f:
            f.write(markdown)
            f.flush()
            os.fsync(f.fileno())
        os.replace(tmp_path, file_path)

        return {
            "success": True,
            "file_path": file_path,
            "abs_path": os.path.abspath(file_path),
            "timestamp": timestamp
        }
    except Exception as e:
        try:
            if tmp_path and os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass
        return {"success": False, "error": str(e)}

class SystemMonitor:
    """
    Central monitoring layer for the AGI system.
    """
    def __init__(self):
        self.logger = logging.getLogger("SystemMonitor")
        self.perception_monitor: PerceptionMonitorExtension = None

    def capture_exception(self, error: Exception, context: Dict = None, severity: str = 'error', component: str = 'unknown'):
        msg = f"[{component}] {severity.upper()}: {error} | Context: {context}"
        if severity == 'error':
            self.logger.error(msg)
        else:
            self.logger.warning(msg)

import threading
import queue

class ConsoleInputListener:
    """
    Asynchronous Console Input Listener.
    Runs in a separate thread to avoid blocking the main AGI loop.
    Captures user commands from stdin and pushes them to a queue.
    """
    def __init__(self):
        self.command_queue = queue.Queue()
        self.is_running = False
        self.listener_thread = None

    def start(self):
        self.is_running = True
        self.listener_thread = threading.Thread(target=self._listen_loop, daemon=True)
        self.listener_thread.start()
        print("   [System] âŒ¨ï¸  Console Input Listener Online. Type 'help' for commands.")

    def stop(self):
        self.is_running = False
        # Thread is daemon, will die with main process, but we can try to be clean
        # (input() is blocking, so hard to kill gracefully without OS signals)

    def _listen_loop(self):
        print("   [Input] Listener ready. You can type commands directly here (ignore log scrolling):")
        while self.is_running:
            try:
                # This blocks, but it's in a thread so main loop is fine
                user_input = input() 
                if user_input.strip():
                    self.command_queue.put(user_input.strip())
            except EOFError:
                break
            except Exception as e:
                print(f"   [Input] Error: {e}")
                break

    def get_command(self):
        if not self.command_queue.empty():
            return self.command_queue.get_nowait()
        return None

class AGI_Life_Engine:
    """
    The Core Life Engine of the AGI System.
    NOW RECONNECTED TO THE PHYSICAL BODY (Core Infrastructure).
    Includes Soul Evolution (Philosopher) and Constitutional Alignment (Immutable Core).
    """
    def _cleanup_startup_cache(self):
        base_dir = os.getcwd()
        targets = [
            os.path.join(base_dir, "data", "intent_bridge", "user_intents.jsonl"),
            os.path.join(base_dir, "data", "intent_bridge", "engine_responses.jsonl"),
            os.path.join(base_dir, "data", "intent_bridge", "active_intent.json"),
            os.path.join(base_dir, "data", "next_tasks.json"),
            # [FIXED 2026-01-29] Removed: os.path.join(base_dir, "logs", "flow_cycle.jsonl"),
            # [FIX 2026-01-27] æ‹“æ‰‘å›¾å·²ä¿®å¤ï¼Œä¸åº”åœ¨å¯åŠ¨æ—¶åˆ é™¤
            # os.path.join(base_dir, "data", "neural_memory", "topology_graph.json"),
            os.path.join(base_dir, "data", "neural_memory", "topology_visual.json"),
            os.path.join(base_dir, "data", "neural_memory", "topology_graph.mmd")
        ]
        removed = []
        for path in targets:
            if os.path.isfile(path):
                try:
                    os.remove(path)
                    removed.append(os.path.relpath(path, base_dir))
                except Exception as e:
                    print(f"   [System] âš ï¸ Cache cleanup failed: {path} ({e})")
        if removed:
            print("   [System] ğŸ§¹ Startup cache cleaned:")
            for item in removed:
                print(f"      - {item}")
        else:
            print("   [System] ğŸ§¹ Startup cache cleaned: none")

    def __init__(self):
        self.is_running = True
        self.step_count = 0
        self.context = {"status": "active", "mode": "learning"}

        # ğŸ†• [ADAPTIVE POLLING 2026-01-27] è‡ªé€‚åº”è½®è¯¢å™¨
        self._adaptive_poller = None  # åœ¨intent_bridgeåˆå§‹åŒ–ååˆ›å»º
        self._last_poll_tick = 0  # ä¸Šæ¬¡è½®è¯¢çš„TICKè®¡æ•°
        self.existential_logger = ExistentialLogger()
        self.start_time = time.time()
        self._cleanup_startup_cache()
        self.last_goal_id = None
        self.current_plan = []          # çŠ¶æ€åŒ–ï¼šå½“å‰æ‰§è¡Œè®¡åˆ’
        self.current_step_index = 0     # çŠ¶æ€åŒ–ï¼šå½“å‰æ­¥éª¤ç´¢å¼•
        self.failed_steps_for_current_goal = []
        self.last_evolution_guidance = None
        self.last_insight_creation_ts = 0.0
        self._insight_persist_failure_count = 0
        self._insight_persist_failure_ts = 0.0
        self._insight_persist_backoff_until = 0.0
        
        # ğŸ”§ [2026-01-11] å…ƒè®¤çŸ¥è°ƒæŸ¥å†·å´æœºåˆ¶ - é˜²æ­¢ç©ºè½¬å¾ªç¯
        self._last_meta_investigation_ts = 0.0
        self._meta_investigation_cooldown = 300  # 5åˆ†é’Ÿå†·å´æœŸ
        self._curiosity_satisfaction_decay = 0.0  # å¥½å¥‡å¿ƒæ»¡è¶³è¡°å‡
        
        print("   [System] ğŸ§¬ Initializing Organic Architecture (Learning Mode)...")
        
        # 0. Load Immutable Core (Constitution)
        self.core_identity = ImmutableCore()
        print(f"   [System] ğŸ“œ Constitution Loaded: {self.core_identity.system_name} {self.core_identity.version}")
        
        # 0.5 Initialize System Monitor
        self.monitor = SystemMonitor()
        RuntimeMonitor.register(self.monitor, context_info="System Monitor")

        # 0.55 Initialize Event Bus
        self.event_bus = LifeEngineEventBus(source="AGI_Life_Engine")
        RuntimeMonitor.register(self.event_bus, context_info="Event Bus")

        # Subscribe to Insight Generation for Persistence
        self.event_bus.subscribe("insight_generated", self._on_insight_generated)

        # 0.6 Initialize Console Input
        self.console_listener = ConsoleInputListener()
        self.console_listener.start()

        # ğŸ†• [2026-01-29] Initialize Real Perception System (The Eye)
        # Replaces MD5 hashing with SentenceTransformers
        self.perception_system = PerceptionSystem()

        # 1. Initialize Brain (LLM)
        self.llm_service = LLMService()
        if self.llm_service.mock_mode:
            print("   [System] âš ï¸ Warning: Running in MOCK mode (No API Key found).")
        else:
            print(f"   [System] ğŸ§  Connected to {self.llm_service.active_provider}")

        # 2. Initialize Body (Tools & Senses)
        self.system_tools = SystemTools(work_dir=os.getcwd())
        self.desktop = DesktopController()
        
        # Initialize Macro System (Skill Memory & Playback)
        self.skill_library = SkillLibrary()
        self.macro_player = MacroPlayer(self.desktop, self.skill_library)
        print("   [System] ğŸ¦¾ Macro Automation System Online.")

        self.vision = VisionObserver()
        self.global_observer = GlobalObserver()
        self.cad_observer = CADObserver()
        self.intent_tracker = IntentTracker()
        self.memory = ArchitectureKnowledgeGraph()

        # ğŸ†• [2026-01-30] P1ä¿®å¤: åˆå§‹åŒ–å­¤ç«‹èŠ‚ç‚¹é¢„é˜²å™¨
        self.isolation_prevention = None
        try:
            from core.isolated_node_prevention import create_isolation_prevention
            self.isolation_prevention = create_isolation_prevention(self.memory)
            print("   [System] ğŸ”— Isolated Node Prevention Online")
        except Exception as e:
            print(f"   [System] âš ï¸ Isolated Node Prevention initialization failed: {e}")

        # ğŸ› ï¸ [FIX 2026-01-15] æ·»åŠ å¼‚å¸¸å¤„ç†ï¼šEnhancedMemoryå¯èƒ½å› ChromaDBé—®é¢˜å¤±è´¥
        try:
            self.semantic_memory = EnhancedExperienceMemory()
            print("   [System] âœ… Enhanced Experience Memory V2 Online")
        except Exception as e:
            print(f"   [System] âš ï¸ Enhanced Memoryåˆå§‹åŒ–å¤±è´¥: {type(e).__name__}")
            print(f"   [System]    é”™è¯¯è¯¦æƒ…: {str(e)[:100]}")
            print("   [System] ğŸ”„ é™çº§åˆ°SimpleMemoryç³»ç»Ÿ (ä¿è¯ç³»ç»Ÿç¨³å®šè¿è¡Œ)")
            try:
                from core.memory_simple import SimpleMemorySystem
                self.semantic_memory = SimpleMemorySystem()
                print("   [System] âœ… Simple Memory Online (Fallback Mode)")
            except Exception as e2:
                print(f"   [System] âŒ SimpleMemoryä¹Ÿå¤±è´¥äº†: {e2}")
                print("   [System] ğŸ”„ ä½¿ç”¨ç©ºå­—å…¸ä½œä¸ºæœ€åé˜²çº¿")
                self.semantic_memory = {}  # ç©ºå­—å…¸ï¼Œç³»ç»Ÿä»èƒ½è¿è¡Œ
        
        # ğŸ†• Biological Memory (Fluid Intelligence)
        if BIOLOGICAL_MEMORY_AVAILABLE:
            self.biological_memory = BiologicalMemorySystem()
            print(f"   [System] ğŸ§  Biological Memory Online ({self.biological_memory.topology.size()} nodes)")
            self.system_tools.biological_memory = self.biological_memory
        else:
            self.biological_memory = None
            print("   [System] â¸ï¸  Biological Memory ä¸å¯ç”¨ (ç³»ç»Ÿå°†ä»¥ç®€åŒ–æ¨¡å¼è¿è¡Œ)")
        
        # âœ… [FIX 2026-01-09] åˆå§‹åŒ–TopologyMemoryå¹¶å»ºç«‹è®°å¿†ç³»ç»Ÿæ¡¥æ¥
        from core.memory.topology_memory import TopologicalMemoryCore
        self.topology_memory = TopologicalMemoryCore()
        print(f"   [System] ğŸ•¸ï¸ Topology Memory Online")
        
        # å»ºç«‹è®°å¿†ç³»ç»Ÿä¹‹é—´çš„è¿æ¥ï¼ˆLayer3æ‹“æ‰‘ä¿®å¤ï¼‰
        # BiologicalMemory â†” TopologyMemory â†” KnowledgeGraph
        # æ³¨æ„ï¼šå®é™…æ¡¥æ¥é€»è¾‘éœ€è¦åœ¨å„æ¨¡å—å†…éƒ¨å®ç°ï¼Œè¿™é‡Œåªæ˜¯å»ºç«‹å¼•ç”¨å…³ç³»
        self.biological_memory._topology_ref = self.topology_memory  # å­˜å‚¨æ‹“æ‰‘å¼•ç”¨
        self.memory._topology_ref = self.topology_memory  # KnowledgeGraphä¹Ÿè¿æ¥æ‹“æ‰‘
        
        self.reasoner = KnowledgeReasoner(self.memory)
        self.arc_solver = ARCSolver() # Initialize Program Synthesis Engine
        print("   [System] ğŸ§© ARC Solver (Program Synthesis) Online.")
        
        # Initialize Neuro-Symbolic Bridge (The Connector)
        self.neuro_bridge = NeuroSymbolicBridge()
        print("   [System] ğŸ§  NeuroSymbolic Bridge (Semantic Drift Detection) Online.")
        
        # Hydrate Bridge with existing knowledge to prevent "Amnesia"
        # We sync the topological structure so 'surprise' metrics are valid
        if self.memory.graph.number_of_nodes() > 0:
            print(f"   [Bridge] Hydrating from Knowledge Graph ({self.memory.graph.number_of_nodes()} nodes)...")
            self.neuro_bridge.update_topology(
                nodes=list(self.memory.graph.nodes()),
                edges=list(self.memory.graph.edges())
            )
        
        RuntimeMonitor.register(self.memory, context_info="Long-term Memory (Knowledge Graph)")
        RuntimeMonitor.register(self.semantic_memory, context_info="Semantic Memory V2 (ChromaDB + Intuition)")
        RuntimeMonitor.register(self.biological_memory, context_info="Biological Memory (Fluid Topology)")
        RuntimeMonitor.register(self.reasoner, context_info="Reasoning Engine")
        RuntimeMonitor.register(self.neuro_bridge, context_info="Neuro-Symbolic Bridge")
        
        # 2.5 Initialize Extended Perception (Hearing & Real-time Vision)
        try:
            self.perception = PerceptionManager()
            self.perception.start_all()
            
            # Initialize ASR (Use TINY for speed if needed, but BASE is standard)
            self.whisper = WhisperASR(model_size=WhisperModelSize.BASE)
            self.streaming_asr = StreamingWhisperASR(self.whisper)
            self.streaming_asr.start()
            
            # Connect Perception -> ASR
            def audio_callback(data):
                if 'audio' in data:
                    audio = data['audio']
                    if hasattr(audio, 'ndim') and audio.ndim > 1:
                        audio = audio.flatten()
                    self.streaming_asr.add_audio(audio)
            
            self.perception.set_audio_processor(audio_callback)
            
            # Attach Monitoring
            extend_monitoring_with_perception(self.monitor, self.perception)
            
            print("   [System] ğŸ‘‚ Hearing (Whisper) & ğŸ‘ï¸ Real-time Vision Online.")
        except Exception as e:
            print(f"   [System] âš ï¸ Extended Perception Init Failed: {e}")
            self.perception = None
            self.streaming_asr = None

        # 2.6 Initialize Hardware Capture (Camera & Microphone)
        try:
            self.hardware_capture = HardwareCaptureManager(
                camera_config=CameraConfig(camera_id=0, width=640, height=480, fps=30),
                microphone_config=MicrophoneConfig(sample_rate=16000, channels=1),
                enable_camera=True,
                enable_microphone=True
            )
            if self.hardware_capture.start_all():
                print("   [System] ğŸ“· Camera & ğŸ¤ Microphone Hardware Capture Online.")
            else:
                print("   [System] âš ï¸ Hardware Capture Init Failed")
                self.hardware_capture = None
        except Exception as e:
            print(f"   [System] âš ï¸ Hardware Capture Init Failed: {e}")
            self.hardware_capture = None

        # 2.7 Initialize Preprocessing & Fusion
        try:
            self.image_preprocessor = ImagePreprocessor(target_size=(224, 224))
            self.audio_preprocessor = AudioPreprocessor(sample_rate=16000)
            self.multimodal_fusion = MultimodalFusion(visual_weight=0.6, audio_weight=0.4)
            self.multimodal_decision = MultimodalDecisionSupport(self.multimodal_fusion)
            print("   [System] ğŸ¨ Image & ğŸµ Audio Preprocessing & Fusion Online.")
        except Exception as e:
            print(f"   [System] âš ï¸ Preprocessing & Fusion Init Failed: {e}")
            self.image_preprocessor = None
            self.audio_preprocessor = None
            self.multimodal_fusion = None
            self.multimodal_decision = None

        print("   [System] ğŸ‘ï¸ Vision & ğŸ–ï¸ Manipulation Systems Online.")
        print("   [System] ğŸŒ Global Awareness & Intent Tracking Online.")
        print("   [System] ğŸ“ Rule-Based Logic Engine (Recursive Flow) Online.")

        # 3. Initialize Goal System
        self.goal_manager = GoalManager(base_path=os.getcwd())
        self.recent_goals = deque(maxlen=5)

        # 4. Initialize Agents (The Trinity)
        # ğŸ†• [2026-01-18] Planner ç°åœ¨æ¥æ”¶ event_bus ä»¥æ„ŸçŸ¥åŠ¨æ€åˆ›å»ºçš„å·¥å…·
        self.planner = PlannerAgent(
            self.llm_service, 
            biological_memory=self.biological_memory,
            event_bus=self.event_bus,
            tool_registry=getattr(self, 'tool_factory', None)  # è‹¥ tool_factory å·²åˆå§‹åŒ–
        )
        self.executor = ExecutorAgent(self.llm_service, self.system_tools, self.desktop)
        self.executor.biological_memory = self.biological_memory
        self.executor.macro_player = self.macro_player
        self.critic = CriticAgent(self.llm_service)
        
        # Register Agents for Runtime Monitoring
        RuntimeMonitor.register(self.planner, context_info="Planner Agent (Trinity)")
        RuntimeMonitor.register(self.executor, context_info="Executor Agent (Trinity)")
        RuntimeMonitor.register(self.critic, context_info="Critic Agent (Trinity)")
        
        print("   [System] ğŸ¤– Agents (Planner, Executor, Critic) Active.")

        # 5. Initialize Philosopher (Soul Evolution)
        # ğŸ†• [2026-01-28] Inject LLM for REAL recursive consciousness
        self.meaning_explorer = MeaningOfExistenceExplorer(self.llm_service)
        print("   [System] ğŸ§  Philosopher Component (Recursive Consciousness) Online.")

        # 6. Initialize Evolution Controller (The New Essence)
        if EVOLUTION_AVAILABLE:
            self.evolution_controller = EvolutionController(self.llm_service)
            RuntimeMonitor.register(self.evolution_controller, context_info="Evolution Controller (The Seed)")
            print("   [System] ğŸ§¬ Evolution Controller (Self-Modification & World Model) Online.")
        else:
            self.evolution_controller = None
            print("   [System] â¸ï¸  Evolution Controller ä¸å¯ç”¨ (ç³»ç»Ÿå°†ä»¥ç®€åŒ–æ¨¡å¼è¿è¡Œ)")

        # 8. Initialize Skill Manager (Dynamic Capability)
        self.skill_manager = SkillManager()
        print("   [System] ğŸ› ï¸ Skill Manager (Dynamic Capability) Online.")
        
        # ğŸ†• [2026-01-09] Initialize Insight Validation-Integration-Evaluation Loop
        # ğŸ”§ [2026-01-10] å‡çº§ä¸ºå¢å¼ºéªŒè¯å™¨ï¼ˆè§£å†³ä¼ªä»£ç é—®é¢˜ï¼‰
        from core.insight_validator import InsightValidator
        from core.insight_integrator import InsightIntegrator
        from core.insight_evaluator import InsightEvaluator
        
        # æ„å»ºç³»ç»Ÿä¾èµ–å›¾ï¼ˆè®°å½•AGIç³»ç»Ÿä¸­å·²å­˜åœ¨çš„å‡½æ•°ï¼‰
        system_dependency_graph = self._build_system_dependency_graph()
        
        self.insight_validator = InsightValidator(
            system_dependency_graph=system_dependency_graph
        )
        self.insight_integrator = InsightIntegrator()
        self.insight_evaluator = InsightEvaluator()
        print(f"   [System] ğŸ”¬ Insight V-I-E Loop (Enhanced Validation) Online. Registered {len(system_dependency_graph)} system functions.")
        
        # ğŸ†• [2026-01-09] Initialize Foraging Agent (Active Learning)
        self.foraging_agent = ForagingAgent(curiosity_threshold=0.7, exploration_budget=10)
        print("   [System] ğŸ” Foraging Agent (Active Learning) Online.")
        
        # 8.5 ğŸ†• Initialize Motivation Core (The Drive) - "èº«å¿ƒåˆä¸€"
        self.motivation = MotivationCore()
        print("   [System] ğŸ”¥ Motivation Core (Maslow + Dopamine) Online.")
        
        # ğŸ†• [2026-01-15] Initialize Double Helix Engine V2 (çœŸæ­£çš„æ™ºèƒ½å†³ç­–å¼•æ“)
        # è¿™æ˜¯æ–°æ—§ç³»ç»ŸçœŸæ­£èåˆçš„æ ¸å¿ƒ - å°†åŒèºæ—‹å†³ç­–å¼•æ“é›†æˆåˆ°å®Œæ•´AGIåŸºç¡€è®¾æ–½
        try:
            self.helix_engine = DoubleHelixEngineV2(
                state_dim=64,
                action_dim=8,  # æ‰©å±•åŠ¨ä½œç©ºé—´ï¼šåŸºç¡€4 + åˆ›é€ æ€§4
                device='cpu',
                enable_nonlinear=True,
                enable_meta_learning=True,
                enable_dialogue=False
            )
            self.helix_decision_enabled = True
            print("   [System] ğŸ§¬ Double Helix Engine V2 Online - Dual-System Decision Making Enabled.")
            print("      [Helix] âš¡ System A (TheSeed) + System B (FractalIntelligence) Fusion Active")
        except Exception as e:
            self.helix_engine = None
            self.helix_decision_enabled = False
            print(f"   [System] âš ï¸ Double Helix Engine V2 initialization failed: {e}")
        
        # ğŸ†• [2026-01-10] Initialize Security Manager (System-Level)
        # æå‡ä¸ºç³»ç»Ÿçº§ç»„ä»¶ï¼Œè€Œéä»…åœ¨Bridgeå†…éƒ¨ä½¿ç”¨
        self.security_manager = SecurityManager()
        print("   [System] ğŸ›¡ï¸ Security Manager (System-Level) Online.")
        
        # ğŸ†• [2026-01-10] Initialize ImmutableCore Bridge (æ ¸å¿ƒç­–ç•¥æ¡¥æ¥)
        # å®ç°æ‹“æ‰‘è¿æ¥: ImmutableCore â†’ SecurityManager, ImmutableCore â†’ CriticAgent
        try:
            from core.immutable_core_bridge import ImmutableCoreBridge
            self.immutable_core_bridge = ImmutableCoreBridge()
            # å°†æ ¸å¿ƒç­–ç•¥æ³¨å…¥åˆ° SecurityManagerï¼ˆå¦‚æœæ”¯æŒï¼‰
            if hasattr(self.security_manager, 'set_policy_source'):
                self.security_manager.set_policy_source(self.immutable_core_bridge)
            print("   [System] ğŸ§¬ ImmutableCore Bridge Online - Core directives connected.")
        except ImportError as e:
            self.immutable_core_bridge = None
            print(f"   [System] âš ï¸ ImmutableCore Bridge not available: {e}")
        
        # ğŸ†• [2026-01-10] Initialize Component Coordinator (EventBus Hub)
        # ä¿®å¤æ‹“æ‰‘å›¾ä¸­ComponentCoordinatoræœªæ¥å…¥çš„é—®é¢˜
        self.component_coordinator = ComponentCoordinator(agi_system=self)
        # è®©SecurityManageré€šè¿‡Coordinatorå¯è®¿é—®
        self.component_coordinator.register_component("security", self.security_manager)
        # æ³¨å†Œ ImmutableCore Bridge
        if self.immutable_core_bridge:
            self.component_coordinator.register_component("core_policy", self.immutable_core_bridge)
        print(f"   [System] ğŸ“Œ Component Coordinator Online - EventBus Hub enabled.")
        
        # 9. Initialize Tool Execution Bridge (LLMâ†’Real Execution)
        self.tool_bridge = None
        self._capability_prompt = ""  # LLMæ³¨å…¥çš„å·¥å…·èƒ½åŠ›æç¤ºè¯
        self._introspection_mode = True  # ğŸ”§ [2026-01-30] å¯ç”¨å†…çœè‡ªä¿®å¤æ¨¡å¼
        if BRIDGE_AVAILABLE:
            self.tool_bridge = ToolExecutionBridge(agi_system=self)
            # ğŸ†• [2026-01-30] å†…çœæ¨¡å¼ï¼šä½¿ç”¨è‡ªä¿®å¤èƒ½åŠ›æç¤ºè¯
            try:
                from core.introspection_mode import INTROSPECTION_CAPABILITY_PROMPT
                self._capability_prompt = INTROSPECTION_CAPABILITY_PROMPT
                print(f"   [System] ğŸ” Introspection Mode ENABLED - Focused on self-repair")
            except ImportError:
                # å›é€€åˆ°æ ‡å‡†å·¥å…·æç¤º
                if hasattr(self.tool_bridge, 'get_capability_prompt'):
                    self._capability_prompt = self.tool_bridge.get_capability_prompt()
            # æ‰“å°å·²æ–‡æ¡£åŒ–å·¥å…·æ•°
            caps = self.tool_bridge.get_tool_capabilities() if hasattr(self.tool_bridge, 'get_tool_capabilities') else {}
            doc_count = caps.get('documented_tools', 0) if isinstance(caps, dict) else 0
            print(f"   [System] ğŸ”§ Tool Execution Bridge Online - {doc_count} tools documented, LLM capability injection ready.")
        else:
            print("   [System] âš ï¸ Tool Execution Bridge not available.")
        
        # 10. Initialize Intent Dialogue Bridge (Bidirectional Intent Communication)
        self.intent_bridge = None
        if INTENT_BRIDGE_AVAILABLE:
            # ğŸ†• [ENHANCE 2026-01-27] ä¼ é€’cognitive_bridgeï¼ˆåˆå§‹åŒ–æ—¶å¯èƒ½ä¸ºNoneï¼Œåç»­æ›´æ–°ï¼‰
            # ğŸ†• [ENHANCE 2026-01-27] å¯ç”¨Redis IPC
            # æ³¨æ„ï¼šæ­¤æ—¶ self.cognitive_bridge è¿˜æœªåˆå§‹åŒ–ï¼Œåç»­ä¼šæ›´æ–°
            self.intent_bridge = get_intent_bridge(
                llm_service=self.llm_service,
                cognitive_bridge=None,  # åˆå§‹ä¸ºNoneï¼Œåç»­åœ¨CognitiveBridgeåˆå§‹åŒ–åæ›´æ–°
                use_redis=True,  # ğŸ†• å¯ç”¨Redis IPC
                redis_host='localhost',
                redis_port=6379
            )
            print("   [System] ğŸ”— Intent Dialogue Bridge Online - Deep intent understanding enabled.")
            print("   [System] ğŸš€ Redis IPC enabled for high-performance messaging.")
        else:
            print("   [System] âš ï¸ Intent Dialogue Bridge not available.")

        # ğŸ”§ [2026-01-15] P0ä¿®å¤: æ¿€æ´»BridgeAutoRepairè‡ªä¿®å¤åŠŸèƒ½
        self.bridge_auto_repair = None
        try:
            from bridge_auto_repair import BridgeAutoRepair
            self.bridge_auto_repair = BridgeAutoRepair(
                bridge_file_path="tool_execution_bridge.py",
                auto_apply=False,  # ä¸è‡ªåŠ¨åº”ç”¨ï¼Œéœ€è¦äººå·¥ç¡®è®¤
                coordinator=self.component_coordinator  # è¿æ¥åˆ°ComponentCoordinator
            )
            print("   [System] ğŸ”§ Bridge Auto Repair Online - Self-healing enabled (manual confirmation mode).")
        except Exception as e:
            print(f"   [System] âš ï¸ Bridge Auto Repair initialization failed: {e}")

        # ğŸ†• [2026-01-11] Initialize M1-M4 Fractal AGI Components Adapter
        # é›†æˆé€’å½’è‡ªæŒ‡åˆ†å½¢AGIçš„å››ä¸ªæ ¸å¿ƒç»„ä»¶
        self.m1m4_adapter = None
        try:
            from core.m1m4_adapter import create_m1m4_adapter
            self.m1m4_adapter = create_m1m4_adapter(
                event_bus=self.event_bus,
                project_root=os.getcwd()
            )
            # æš´éœ²è‡ªä¿®æ”¹å¼•æ“åˆ°ç³»ç»Ÿï¼ˆç”¨äºé›†æˆçº§å›å½’ä¸èƒ½åŠ›æ¿€æ´»ï¼‰
            if hasattr(self.m1m4_adapter, 'self_modifier'):
                self.self_modifier = self.m1m4_adapter.self_modifier
                if self.self_modifier:
                    self.component_coordinator.register_component("self_modification", self.self_modifier)
                    print("   [System] ğŸ§° Self-Modification Engine registered in ComponentCoordinator")
                    # ğŸ†• [2026-01-17] ç¡®ä¿ tool_bridge èƒ½è®¿é—® self_modifier
                    if self.tool_bridge and hasattr(self.tool_bridge, 'agi_system'):
                        # tool_bridge å·²ç»æŒæœ‰ self å¼•ç”¨ï¼Œç°åœ¨ self.self_modifier å¯ç”¨äº†
                        print("   [System] ğŸ”— Tool Bridge linked to Self-Modification Engine")
            print("   [System] ğŸ§¬ M1-M4 Fractal AGI Components Integrated:")
            print("      [M1] MetaLearner - å…ƒå‚æ•°ä¼˜åŒ–å™¨")
            print("      [M2] GoalQuestioner - ç›®æ ‡è´¨ç–‘æ¨¡å—")
            print("      [M3] SelfModifyingEngine - æ¶æ„è‡ªä¿®æ”¹å¼•æ“")
            print("      [M4] RecursiveSelfMemory - é€’å½’è‡ªå¼•ç”¨è®°å¿†ç³»ç»Ÿ")
        except Exception as e:
            print(f"   [System] âš ï¸ M1-M4 Adapter initialization failed: {e}")
            import traceback
            traceback.print_exc()

        # [2026-01-11] Intelligence Upgrade: Short-term Working Memory
        # çŸ­æœŸå·¥ä½œè®°å¿† - æ‰“ç ´æ€æƒ³å¾ªç¯ï¼Œæå‡æ¨ç†è¿è´¯æ€§
        logging.info("   [DEBUG] About to initialize Working Memory...")
        print("   [DEBUG] About to initialize Working Memory...", flush=True)
        self.working_memory = None
        try:
            from core.working_memory import ShortTermWorkingMemory
            logging.info("   [DEBUG] Working Memory module imported, creating instance...")
            print("   [DEBUG] Working Memory module imported, creating instance...", flush=True)
            self.working_memory = ShortTermWorkingMemory(capacity=7, loop_threshold=3)
            self.intelligence_upgrade_enabled = True
            logging.info("   [System] [Intelligence Upgrade] Short-term Working Memory enabled")
            print("   [System] [Intelligence Upgrade] Short-term Working Memory enabled", flush=True)
        except Exception as e:
            logging.warning(f"   [System] [WARNING] Working memory initialization failed: {e}")
            print(f"   [System] [WARNING] Working memory initialization failed: {e}", flush=True)
            import traceback
            traceback.print_exc()
            self.intelligence_upgrade_enabled = False

        # ğŸ†• [2026-01-16] P0ä¿®å¤ï¼šç†µå€¼è°ƒèŠ‚å™¨ - ç»´æŒé•¿æœŸä¸­ç†µçŠ¶æ€
        logging.info("   [DEBUG] About to initialize Entropy Regulator...")
        print("   [DEBUG] About to initialize Entropy Regulator...", flush=True)
        self.entropy_regulator = None
        try:
            from core.entropy_regulator import EntropyRegulator
            logging.info("   [DEBUG] Entropy Regulator module imported, creating instance...")
            print("   [DEBUG] Entropy Regulator module imported, creating instance...", flush=True)
            # ğŸ†• [2026-01-17] P0ä¿®å¤ï¼šé™ä½é˜ˆå€¼ä»¥æ›´æ—©è§¦å‘ç†µå€¼è°ƒèŠ‚
            self.entropy_regulator = EntropyRegulator(
                monitor_window=50,          # ç¼©çŸ­ç›‘æ§çª—å£ä»¥æ›´å¿«å“åº”
                warning_threshold=0.6,       # æ›´æ—©è­¦å‘Š (åŸ0.7)
                critical_threshold=0.75,     # æ›´æ—©è§¦å‘ä¸´ç•Œè°ƒèŠ‚ (åŸ0.9)
                rising_threshold=5           # æ›´æ•æ„Ÿçš„ä¸Šå‡æ£€æµ‹ (åŸ10)
            )
            logging.info("   [System] [Entropy Regulation] Entropy Regulator enabled (enhanced)")
            print("   [System] [Entropy Regulation] Entropy Regulator enabled (enhanced thresholds)", flush=True)
        except Exception as e:
            logging.warning(f"   [System] [WARNING] Entropy Regulator initialization failed: {e}")
            print(f"   [System] [WARNING] Entropy Regulator initialization failed: {e}", flush=True)
            import traceback
            traceback.print_exc()

        # ğŸ†• [2026-01-17] çŸ¥è¯†å›¾è°±å®æ—¶å¯¼å‡ºå™¨ - æ”¯æŒå¯è§†åŒ–å®æ—¶æ›´æ–°
        logging.info("   [DEBUG] About to initialize Knowledge Graph Exporter...")
        print("   [DEBUG] About to initialize Knowledge Graph Exporter...", flush=True)
        self.knowledge_graph_exporter = None
        try:
            from core.knowledge_graph_exporter import KnowledgeGraphExporter
            logging.info("   [DEBUG] Knowledge Graph Exporter module imported, creating instance...")
            print("   [DEBUG] Knowledge Graph Exporter module imported, creating instance...", flush=True)
            self.knowledge_graph_exporter = KnowledgeGraphExporter(
                output_dir="data/knowledge",
                export_interval=30,  # æ¯30ç§’å¯¼å‡ºä¸€æ¬¡
                max_history=100
            )
            # å¯åŠ¨è‡ªåŠ¨å¯¼å‡ºçº¿ç¨‹
            self.knowledge_graph_exporter.start()
            logging.info("   [System] [Knowledge Graph Exporter] Knowledge Graph Exporter enabled")
            print("   [System] [Knowledge Graph Exporter] Knowledge Graph Exporter enabled", flush=True)
        except Exception as e:
            logging.warning(f"   [System] [WARNING] Knowledge Graph Exporter initialization failed: {e}")
            print(f"   [System] [WARNING] Knowledge Graph Exporter initialization failed: {e}", flush=True)
            import traceback
            traceback.print_exc()

        # [2026-01-11] Intelligence Upgrade Phase 2: Reasoning Scheduler
        # æ¨ç†è°ƒåº¦å™¨ - æ™ºèƒ½è°ƒåº¦æ¨ç†å¼•æ“ï¼Œå®ç°æ·±åº¦æ¨ç†
        logging.info("   [DEBUG] About to initialize Reasoning Scheduler...")
        print("   [DEBUG] About to initialize Reasoning Scheduler...", flush=True)
        self.reasoning_scheduler = None
        
        # ğŸ”§ RE-ENABLED [2026-01-12] Phase 2: Reasoning Scheduler
        try:
            logging.info("   [DEBUG] Attempting to import ReasoningScheduler...")
            from core.reasoning_scheduler import ReasoningScheduler
            logging.info("   [DEBUG] ReasoningScheduler module imported, importing CausalReasoningEngine...")
            from core.causal_reasoning import CausalReasoningEngine

            # åˆ›å»ºå› æœæ¨ç†å¼•æ“
            logging.info("   [DEBUG] Creating CausalReasoningEngine instance...")
            print("   [DEBUG] Creating CausalReasoningEngine instance...", flush=True)
            causal_engine = CausalReasoningEngine()
            
            logging.info("   [DEBUG] CausalReasoningEngine created, creating ReasoningScheduler...")
            print("   [DEBUG] CausalReasoningEngine created, creating ReasoningScheduler...", flush=True)

            # åˆ›å»ºæ¨ç†è°ƒåº¦å™¨
            self.reasoning_scheduler = ReasoningScheduler(
                causal_engine=causal_engine,
                llm_service=self.llm_service,
                confidence_threshold=0.6,
                max_depth=99999
            )
            logging.info("   [DEBUG] ReasoningScheduler created, starting session...")

            # å¯åŠ¨åˆå§‹æ¨ç†ä¼šè¯
            self.reasoning_scheduler.start_session()
            logging.info("   [DEBUG] Session started, Reasoning Scheduler initialization complete")

            print("   [System] [Intelligence Upgrade Phase 2] Reasoning Scheduler enabled (max_depth=99999)", flush=True)
        except Exception as e:
            print(f"   [System] [WARNING] Reasoning scheduler initialization failed: {e}", flush=True)
            logging.error(f"Reasoning scheduler initialization failed: {e}")
            import traceback
            traceback.print_exc()

        # [2026-01-11] Intelligence Upgrade Phase 3: World Model, Goal Manager, Creative Exploration
        # ç»Ÿä¸€ä¸–ç•Œæ¨¡å‹ã€å±‚çº§ç›®æ ‡ç³»ç»Ÿã€åˆ›é€ æ€§æ¢ç´¢å¼•æ“
        logging.info("   [DEBUG] About to initialize Phase 3 modules...")
        print("   [DEBUG] About to initialize Phase 3 modules...", flush=True)
        self.world_model = None
        self.creative_engine = None
        self.hierarchical_goal_manager = None

        try:
            from core.bayesian_world_model import BayesianWorldModel
            print("   [DEBUG] BayesianWorldModel imported, importing HierarchicalGoalManager...")
            from core.hierarchical_goal_manager import HierarchicalGoalManager, GoalLevel
            print("   [DEBUG] HierarchicalGoalManager imported, importing CreativeExplorationEngine...")
            from core.creative_exploration_engine import CreativeExplorationEngine

            # åˆ›å»ºä¸–ç•Œæ¨¡å‹
            self.world_model = BayesianWorldModel(learning_rate=0.1)
            print("   [System] [Intelligence Upgrade Phase 3] Bayesian World Model enabled")

            # åˆ›å»ºç›®æ ‡ç®¡ç†å™¨
            self.hierarchical_goal_manager = HierarchicalGoalManager(max_active_goals=10)
            # åˆ›å»ºåˆå§‹ç»ˆèº«ç›®æ ‡
            self.hierarchical_goal_manager.create_goal(
                name="achieve_agi",
                level=GoalLevel.LIFETIME,
                description="å®ç°é€šç”¨äººå·¥æ™ºèƒ½",
                priority=1.0
            )
            print("   [System] [Intelligence Upgrade Phase 3] Hierarchical Goal Manager enabled")

            # åˆ›å»ºåˆ›é€ æ€§æ¢ç´¢å¼•æ“
            self.creative_engine = CreativeExplorationEngine(temperature=0.7)
            print("   [System] [Intelligence Upgrade Phase 3] Creative Exploration Engine enabled")

        except Exception as e:
            print(f"   [System] [WARNING] Phase 3 modules initialization failed: {e}")
            import traceback
            traceback.print_exc()

        # [2026-01-11] Intelligence Upgrade Phase 4: Meta-Learning, Self-Improvement, Recursive Self-Reference
        # å…ƒå­¦ä¹ ã€è‡ªæˆ‘æ”¹è¿›ã€é€’å½’è‡ªæŒ‡ä¼˜åŒ–
        print("   [DEBUG] About to initialize Phase 4 modules...")
        self.meta_learner = None
        self.self_improvement_engine = None
        self.recursive_self_reference = None

        try:
            from core.meta_learning import MetaLearner
            print("   [DEBUG] MetaLearner imported, importing SelfImprovementEngine...")
            from core.self_improvement import SelfImprovementEngine
            print("   [DEBUG] SelfImprovementEngine imported, importing RecursiveSelfReferenceEngine...")
            from core.recursive_self_reference import RecursiveSelfReferenceEngine

            # åˆ›å»ºå…ƒå­¦ä¹ å¼•æ“
            self.meta_learner = MetaLearner(memory_size=100)
            print("   [System] [Intelligence Upgrade Phase 4] Meta-Learner enabled")

            # åˆ›å»ºè‡ªæˆ‘æ”¹è¿›å¼•æ“
            project_root = os.path.dirname(os.path.abspath(__file__))
            self.self_improvement_engine = SelfImprovementEngine(project_root)
            print("   [System] [Intelligence Upgrade Phase 4] Self-Improvement Engine enabled")

            # åˆ›å»ºé€’å½’è‡ªæŒ‡å¼•æ“
            self.recursive_self_reference = RecursiveSelfReferenceEngine(max_recursion_depth=3)
            print("   [System] [Intelligence Upgrade Phase 4] Recursive Self-Reference enabled")

        except Exception as e:
            print(f"   [System] [WARNING] Phase 4 modules initialization failed: {e}")
            import traceback
            traceback.print_exc()

        # ğŸ†• [2026-01-16] P0ä¿®å¤: å…ƒè®¤çŸ¥å±‚ - è®©ç³»ç»Ÿ"æ€è€ƒè‡ªå·±çš„æ€è€ƒ"
        # è¿™æ˜¯å®ç°Level 4æ™ºèƒ½ï¼ˆå…ƒè®¤çŸ¥ï¼‰çš„å…³é”®ç»„ä»¶
        self.meta_cognitive_layer = None
        try:
            from core.meta_cognitive import MetaCognitiveLayer
            self.meta_cognitive_layer = MetaCognitiveLayer(
                knowledge_graph=self.memory,
                memory_system=self.semantic_memory
            )
            print("   [System] ğŸ§  Meta-Cognitive Layer Online - Self-Reflection Enabled")
            print("      [MetaCog] âœ… Task Understanding Depth Evaluator")
            print("      [MetaCog] âœ… Capability Matcher")
            print("      [MetaCog] âœ… Failure Attribution Engine")
        except Exception as e:
            print(f"   [System] âš ï¸ Meta-Cognitive Layer initialization failed: {e}")
            import traceback
            traceback.print_exc()
        
        # ğŸ†• [2026-01-30] P0ä¿®å¤: å…ƒè®¤çŸ¥æ™ºèƒ½è¿‡æ»¤å™¨ - è§£å†³ç©ºè½¬å’Œå‡é˜³æ€§é—®é¢˜
        self.meta_filter = None
        try:
            from core.metacognitive_filter import get_meta_filter
            self.meta_filter = get_meta_filter()
            print("   [System] ğŸ§  Meta-Cognitive Filter Online - Smart Evaluation Enabled")
            print("      [MetaFilter] âœ… Complexity Threshold Filter")
            print("      [MetaFilter] âœ… Cooldown Mechanism")
            print("      [MetaFilter] âœ… Duplicate Detection")
            print("      [MetaFilter] âœ… Monitoring Task Whitelist")
        except Exception as e:
            print(f"   [System] âš ï¸ Meta-Cognitive Filter initialization failed: {e}")
            import traceback
            traceback.print_exc()

        # ğŸ†• [2026-01-30] P1ä¿®å¤: å¤æ‚ä»»åŠ¡ç”Ÿæˆå™¨ - è§£å†³æ¨ç†æ·±åº¦åœæ»é—®é¢˜
        self.complex_task_generator = None
        try:
            from core.complex_task_generator import create_complex_task_generator
            self.complex_task_generator = create_complex_task_generator()
            print("   [System] ğŸ¯ Complex Task Generator Online")
            print("      [TaskGen] âœ… Creative Tool Templates")
            print("      [TaskGen] âœ… Deep Analysis Templates")
            print("      [TaskGen] âœ… Cross-Domain Templates")
        except Exception as e:
            print(f"   [System] âš ï¸ Complex Task Generator initialization failed: {e}")
            import traceback
            traceback.print_exc()

        # ğŸ†• [2026-01-30] P0ä¿®å¤: åˆ›é€ æ€§äº§å‡ºæµæ°´çº¿ - è§£å†³0äº§å‡ºé—®é¢˜
        self.creative_pipeline = None
        try:
            from core.creative_output_pipeline import create_creative_pipeline
            self.creative_pipeline = create_creative_pipeline()
            print("   [System] ğŸš€ Creative Output Pipeline Online")
            print("      [Pipeline] âœ… 5-Stage Process")
            print("      [Pipeline] âœ… Auto-Repair Mechanism")
            print("      [Pipeline] âœ… Quality Scoring")
        except Exception as e:
            print(f"   [System] âš ï¸ Creative Pipeline initialization failed: {e}")
            import traceback
            traceback.print_exc()

        # ğŸ†• [2026-01-30] P2ä¿®å¤: çœŸè¿›åŒ–æœºåˆ¶å¼•æ“ - æ¶æ„è‡ªä¿®æ”¹èƒ½åŠ›
        self.evolution_engine = None
        try:
            from core.true_evolution_engine import create_evolution_engine
            self.evolution_engine = create_evolution_engine(project_root)
            print("   [System] ğŸ§¬ True Evolution Engine Online")
            print("      [Evolution] âœ… Isolated Sandbox")
            print("      [Evolution] âœ… Automated Testing")
            print("      [Evolution] âœ… Version Control & Rollback")
        except Exception as e:
            print(f"   [System] âš ï¸ True Evolution Engine initialization failed: {e}")
            import traceback
            traceback.print_exc()

        # ğŸ†• [2026-01-30] P2ä¿®å¤: æ¨¡å—ç²¾ç®€é‡æ„ç³»ç»Ÿ
        self.module_restructuring = None
        try:
            from core.module_restructuring import analyze_and_plan_restructuring
            self.module_restructuring = analyze_and_plan_restructuring(project_root)
            print("   [System] ğŸ—ï¸  Module Restructuring System Online")
            print("      [Restructure] âœ… Module Analysis")
            print("      [Restructure] âœ… Legacy Detection")
            print("      [Restructure] âœ… Consolidation Planning")
            # å¯¼å‡ºé‡æ„è®¡åˆ’
            self.module_restructuring.export_plan("data/module_restructuring_plan.json")
        except Exception as e:
            print(f"   [System] âš ï¸ Module Restructuring initialization failed: {e}")
            import traceback
            traceback.print_exc()

        # ğŸ†• [2026-01-16] P0ä¿®å¤: æ¶æ„æ„ŸçŸ¥å±‚ - è®©ç³»ç»Ÿ"ç†è§£è‡ªå·±çš„æ¶æ„")
        # è¿™æ˜¯å®ç°Level 4æ™ºèƒ½ï¼ˆæ¶æ„è‡ªæˆ‘è®¤çŸ¥ï¼‰çš„å…³é”®ç»„ä»¶
        self.architecture_awareness_layer = None
        try:
            from core.architecture_awareness import ArchitectureAwarenessLayer
            self.architecture_awareness_layer = ArchitectureAwarenessLayer(
                project_root=os.getcwd()
            )
            print("   [System] ğŸ—ï¸  Architecture Awareness Layer Online - Self-Understanding Enabled")
            print("      [ArchAware] âœ… Component Dependency Mapper")
            print("      [ArchAware] âœ… Performance Bottleneck Analyzer")
            print("      [ArchAware] âœ… Architecture Health Monitor")
        except Exception as e:
            print(f"   [System] âš ï¸ Architecture Awareness Layer initialization failed: {e}")
            import traceback
            traceback.print_exc()

        # ğŸ†• [2026-01-17] å¯åŠ¨é’©å­ - è‡ªåŠ¨åŠ è½½æ–‡æ¡£ç´¢å¼•å’Œæ‰§è¡Œå¯åŠ¨ä»»åŠ¡
        self.startup_hooks = None
        try:
            from core.startup_hooks import StartupHooks
            self.startup_hooks = StartupHooks(
                knowledge_graph=self.memory,
                llm_service=self.llm_service
            )
            # æ‰§è¡Œæ‰€æœ‰å¯åŠ¨é’©å­
            startup_result = self.startup_hooks.execute_all()
            if startup_result.get("status") != "disabled":
                task_count = len(startup_result.get("tasks", []))
                print(f"   [System] ğŸš€ Startup Hooks Complete - {task_count} tasks executed")
        except Exception as e:
            print(f"   [System] âš ï¸ Startup Hooks initialization failed: {e}")
            import traceback
            traceback.print_exc()

        self.error_diagnosis = None # State to hold error info for the next planning cycle
        self._meta_plugins = {}
        self._hot_swapper = None

        # ğŸ†• [2026-01-18] è‡ªä¸»æ€§æ¿€æ´»å±‚ - è®©ç°æœ‰ç»„ä»¶"æ´»"èµ·æ¥
        # æ ¸å¿ƒçªç ´ï¼šå°†ç»„ä»¶ä»"è¢«åŠ¨å“åº”"æ¨¡å¼è½¬æ¢ä¸º"ä¸»åŠ¨é©±åŠ¨"æ¨¡å¼
        self.autonomy_activator = None
        self.tool_factory = None
        try:
            # åˆå§‹åŒ– ToolFactory (åŠ¨æ€å·¥å…·åˆ›å»ºèƒ½åŠ›)
            from agi_tool_factory import ToolFactory, ToolRegistry
            from agi_dynamic_loader import DynamicModuleLoader
            
            tool_loader = DynamicModuleLoader(safe_mode=True)
            tool_registry = ToolRegistry()
            self.tool_factory = ToolFactory(tool_loader, tool_registry, coordinator=self.component_coordinator)
            print("   [System] ğŸ”§ ToolFactory initialized - Dynamic tool creation enabled")
            
            # åˆå§‹åŒ– AutonomyActivator (è‡ªä¸»æ€§æ¿€æ´»å±‚)
            from core.autonomy_activator import create_autonomy_activator
            
            self.autonomy_activator = create_autonomy_activator(
                goal_manager=self.goal_manager,
                m1m4_adapter=self.m1m4_adapter,
                tool_factory=self.tool_factory,
                event_bus=self.event_bus,
                biological_memory=self.biological_memory
            )
            
            print("   [System] ğŸ”‹ Autonomy Activator Online - Components ACTIVELY driven")
            print("      [Autonomy] âœ… GoalQuestioner - Will QUESTION goals every 50 ticks")
            print("      [Autonomy] âœ… IntrinsicMotivation - Will COMPUTE motivation every 10 ticks")
            print("      [Autonomy] âœ… ToolFactory - Will CREATE tools when capability gaps detected")
            
        except Exception as e:
            print(f"   [System] âš ï¸ Autonomy Activator initialization failed: {e}")
            import traceback
            traceback.print_exc()

        # ğŸ†• [2026-01-19] ç³»ç»Ÿä¼˜åŒ–å™¨ - é›¶æ‹“æ‰‘æ”¹åŠ¨æ–¹æ¡ˆ
        # æ¿€æ´»ç°æœ‰ç³»ç»Ÿå·²å®ç°ä½†æœªå……åˆ†åˆ©ç”¨çš„èƒ½åŠ›
        self.system_optimizer = None
        try:
            from core.system_optimizer import SystemOptimizer
            self.system_optimizer = SystemOptimizer(agi_engine=self)

            # æ£€æŸ¥æ˜¯å¦åœ¨å¯åŠ¨æ—¶è‡ªåŠ¨åº”ç”¨ä¼˜åŒ–
            auto_optimize = '--optimize-on-startup' in sys.argv

            if auto_optimize:
                print("   [System] ğŸš€ SystemOptimizer Online - Applying optimizations on startup...")
                results = self.system_optimizer.apply_all_optimizations()
                print(f"   [System] âœ… {len(results)} optimizations applied")
            else:
                print("   [System] ğŸ”§ SystemOptimizer Online - Ready (use --optimize-on-startup to activate)")
                print("      [Optimizer] ğŸ’¡ Available optimizations:")
                print("         - Creativity Emergence: 0.04 â†’ 0.15 (+275%)")
                print("         - Deep Reasoning: 100 steps â†’ 99,999 steps (+999x)")
                print("         - Autonomous Goals: Generation rate Ã— 2")
                print("         - Cross-Domain Transfer: Auto-activate (+18.3%)")

        except Exception as e:
            print(f"   [System] âš ï¸ SystemOptimizer initialization failed: {e}")
            import traceback
            traceback.print_exc()

        # ğŸ”§ [2026-01-23] P0ä¿®å¤: é›†æˆæ–­è£‚çš„æ‹“æ‰‘è¿æ¥ç»„ä»¶
        # ä¿®å¤3Dæ‹“æ‰‘å›¾ä¸­æ˜¾ç¤ºä½†æœªåœ¨å¼•æ“ä¸­åˆå§‹åŒ–çš„å…³é”®ç»„ä»¶

        # 1. è‡ªä¸»ç›®æ ‡ç³»ç»Ÿ (AutonomousGoalSystem)
        self.autonomous_goal_system = None
        try:
            from core.autonomous_goal_system import AutonomousGoalGenerator
            self.autonomous_goal_system = AutonomousGoalGenerator()
            print("   [System] ğŸ¯ AutonomousGoalSystem Online - è‡ªä¸»ç›®æ ‡ç”Ÿæˆå·²å¯ç”¨")
            print("      [AutoGoal] âœ… IntrinsicValueFunction - å†…åœ¨ä»·å€¼è®¡ç®—")
            print("      [AutoGoal] âœ… OpportunityRecognitionEngine - æœºä¼šè¯†åˆ«")
            print("      [AutoGoal] âœ… GoalHierarchyBuilder - ç›®æ ‡å±‚çº§æ„å»º")
        except Exception as e:
            print(f"   [System] âš ï¸ AutonomousGoalSystem initialization failed: {e}")

        # 2. è®¤çŸ¥æ¡¥æ¥å™¨ (CognitiveBridge)
        self.cognitive_bridge = None
        try:
            from core.cognitive_bridge import CognitiveBridge
            self.cognitive_bridge = CognitiveBridge(agi_engine=self)
            print("   [System] ğŸ§  CognitiveBridge Online - è®¤çŸ¥èƒ½åŠ›æ¡¥æ¥å·²å¯ç”¨")
            print("      [CogBridge] âœ… TopologyMemory Query - æ‹“æ‰‘è®°å¿†æŸ¥è¯¢")
            print("      [CogBridge] âœ… CausalReasoning Query - å› æœæ¨ç†æŸ¥è¯¢")
            print("      [CogBridge] âœ… DeepReasoning Integration - æ·±åº¦æ¨ç†é›†æˆ")

            # ğŸ†• [ENHANCE 2026-01-27] æ›´æ–°intent_bridgeçš„cognitive_bridgeå¼•ç”¨
            if self.intent_bridge is not None:
                # ğŸ†• [ENHANCE 2026-01-27] åŒæ—¶ä¿æŒRedis IPCå¯ç”¨
                self.intent_bridge = get_intent_bridge(
                    cognitive_bridge=self.cognitive_bridge,
                    use_redis=True,  # ğŸ†• ä¿æŒRedis IPCå¯ç”¨
                    redis_host='localhost',
                    redis_port=6379
                )
                print("   [System] ğŸ”— IntentBridge updated with CognitiveBridge - æ·±åº¦åˆ†æå·²å¯ç”¨")

                # ğŸ†• [EVENT FLOW 2026-01-27] è®¢é˜…Redis Pub/Subäº‹ä»¶æµ
                if hasattr(self.intent_bridge, 'subscribe_events'):
                    success = self.intent_bridge.subscribe_events(self._handle_agi_event)
                    if success:
                        print("   [System] ğŸ§ Event Subscription Online - äº‹ä»¶é©±åŠ¨å·²å¯ç”¨")

                # ğŸ†• [ADAPTIVE POLLING 2026-01-27] åˆå§‹åŒ–è‡ªé€‚åº”è½®è¯¢å™¨
                if self._adaptive_poller is None:
                    from intent_dialogue_bridge import AdaptivePoller
                    self._adaptive_poller = AdaptivePoller()
                    print("   [System] ğŸ”„ Adaptive Poller Online - è‡ªé€‚åº”è½®è¯¢å·²å¯ç”¨")
        except Exception as e:
            print(f"   [System] âš ï¸ CognitiveBridge initialization failed: {e}")

        # 3. è·¨åŸŸè¿ç§»ç³»ç»Ÿ (CrossDomainTransfer)
        self.cross_domain_transfer = None
        try:
            from core.cross_domain_transfer import CrossDomainTransferSystem
            self.cross_domain_transfer = CrossDomainTransferSystem()
            print("   [System] ğŸ”„ CrossDomainTransfer Online - è·¨åŸŸçŸ¥è¯†è¿ç§»å·²å¯ç”¨")
            print("      [Xfer] âœ… CrossDomainMapper - è·¨åŸŸæ˜ å°„")
            print("      [Xfer] âœ… MetaLearningTransfer - å…ƒå­¦ä¹ è¿ç§»")
            print("      [Xfer] âœ… FewShotLearner - å°‘æ ·æœ¬å­¦ä¹ ")
            print("      [Xfer] âœ… SkillExtractor - æŠ€èƒ½æå–")
        except Exception as e:
            print(f"   [System] âš ï¸ CrossDomainTransfer initialization failed: {e}")

        # ğŸ”§ [2026-01-23] å»ºç«‹ç»„ä»¶é—´çš„æ‹“æ‰‘è¿æ¥
        # ä¿®å¤ä¿¡æ¯æµå’Œäº‹ä»¶æµæ–­è£‚
        self._establish_component_connections()

        # ğŸ†• [2026-01-24] ä¼šè¯ä¸Šä¸‹æ–‡æ¢å¤å™¨ - ä¿®å¤ä¼šè¯è¿ç»­æ€§é—®é¢˜
        # è‡ªåŠ¨æ¢å¤å†å²å¯¹è¯ä¸Šä¸‹æ–‡å’Œæœªå®Œæˆä»»åŠ¡ï¼Œå‡å°‘é‡å¤è§£é‡Š
        self.session_restorer = None
        try:
            from core.session_context_restorer import SessionContextRestorer
            self.session_restorer = SessionContextRestorer(project_root=os.getcwd())

            # å°è¯•æ¢å¤ä¸Šä¸€æ¬¡ä¼šè¯çš„ä¸Šä¸‹æ–‡
            restored_context = self.session_restorer.restore_context()
            if restored_context.get("restoration_success"):
                active_goals_count = len(restored_context.get("active_goals", []))
                recent_insights_count = len(restored_context.get("recent_insights", []))
                print(f"   [System] ğŸ”„ Session Context Restorer Online - è·¨ä¼šè¯è¿ç»­æ€§å·²å¯ç”¨")
                print(f"      [ContextRestore] âœ… æ¢å¤äº† {active_goals_count} ä¸ªæ´»è·ƒç›®æ ‡")
                print(f"      [ContextRestore] âœ… æ¢å¤äº† {recent_insights_count} æ¡æœ€è¿‘æ´å¯Ÿ")
                if restored_context.get("last_session_tasks"):
                    pending_count = len(restored_context.get("last_session_tasks", []))
                    print(f"      [ContextRestore] â³ {pending_count} ä¸ªå¾…å¤„ç†ä»»åŠ¡å·²è¯†åˆ«")
            else:
                print(f"   [System] ğŸ”„ Session Context Restorer Online - é¦–æ¬¡å¯åŠ¨æˆ–æ— å†å²ä¸Šä¸‹æ–‡")
        except Exception as e:
            print(f"   [System] âš ï¸ Session Context Restorer initialization failed: {e}")
            import traceback
            traceback.print_exc()

    def _establish_component_connections(self):
        """
        å»ºç«‹ç»„ä»¶é—´çš„æ‹“æ‰‘è¿æ¥ï¼Œä¿®å¤ä¿¡æ¯æµå’Œäº‹ä»¶æµæ–­è£‚

        è¿æ¥æ‹“æ‰‘:
        AutonomousGoalSystem â†’ DoubleHelixEngineV2
        CognitiveBridge â†’ FractalIntelligence
        CrossDomainTransfer â†’ KnowledgeGraph
        MetaCognitiveLayer â†’ DoubleHelixEngineV2
        """
        try:
            connections_established = 0

            # è¿æ¥1: AutonomousGoalSystem â†’ DoubleHelixEngineV2
            # ç›®æ ‡ç”Ÿæˆæµï¼šè‡ªä¸»ç›®æ ‡ â†’ åŒèºæ—‹å†³ç­–å¼•æ“
            if self.autonomous_goal_system and self.helix_engine:
                # æ³¨å†Œäº‹ä»¶å¤„ç†å™¨ï¼šç›®æ ‡ç”Ÿæˆå®Œæˆæ—¶é€šçŸ¥åŒèºæ—‹å¼•æ“
                self.event_bus.subscribe("autonomous_goal_generated", self._on_autonomous_goal_generated)
                connections_established += 1
                print("   [Topology] âœ… Connected: AutonomousGoalSystem â†’ DoubleHelixEngineV2")

            # è¿æ¥2: CognitiveBridge â†’ FractalIntelligence (through helix_engine)
            # è®¤çŸ¥éªŒè¯æµï¼šè®¤çŸ¥æ¡¥æ¥ â†’ åˆ†å½¢æ™ºèƒ½
            if self.cognitive_bridge and self.helix_engine:
                # è®¤çŸ¥æ¡¥æ¥å™¨å¯ä»¥ä¸ºåˆ†å½¢æ™ºèƒ½æä¾›æ‹“æ‰‘å’Œå› æœåˆ†æ
                if hasattr(self.helix_engine, 'fractal') and self.helix_engine.fractal:
                    # å°†è®¤çŸ¥æ¡¥æ¥æ³¨å…¥åˆ°FractalIntelligence
                    if hasattr(self.helix_engine.fractal, 'set_cognitive_bridge'):
                        self.helix_engine.fractal.set_cognitive_bridge(self.cognitive_bridge)
                    connections_established += 1
                    print("   [Topology] âœ… Connected: CognitiveBridge â†’ FractalIntelligence")

            # è¿æ¥3: CrossDomainTransfer â†’ KnowledgeGraph
            # è·¨åŸŸè¿ç§»æµï¼šçŸ¥è¯†å›¾è°± â†’ è·¨åŸŸè¿ç§» â†’ çŸ¥è¯†å›¾è°±
            if self.cross_domain_transfer and self.memory:
                # å°†çŸ¥è¯†å›¾è°±æ³¨å…¥åˆ°è·¨åŸŸè¿ç§»ç³»ç»Ÿ
                # è¿™é‡Œæˆ‘ä»¬ä¿å­˜å¼•ç”¨ï¼Œåœ¨è¿è¡Œæ—¶åŠ¨æ€ä½¿ç”¨
                if not hasattr(self.cross_domain_transfer, 'knowledge_graph'):
                    self.cross_domain_transfer.knowledge_graph = self.memory
                connections_established += 1
                print("   [Topology] âœ… Connected: CrossDomainTransfer â†” KnowledgeGraph")

            # è¿æ¥4: MetaCognitiveLayer â†’ DoubleHelixEngineV2
            # è‡ªæˆ‘è¯„ä¼°æµï¼šå…ƒè®¤çŸ¥å±‚ â†’ åŒèºæ—‹å¼•æ“
            if self.meta_cognitive_layer and self.helix_engine:
                # æ³¨å†Œäº‹ä»¶å¤„ç†å™¨ï¼šå…ƒè®¤çŸ¥è¯„ä¼°å®Œæˆæ—¶é€šçŸ¥åŒèºæ—‹å¼•æ“
                self.event_bus.subscribe("meta_cognitive_assessment", self._on_meta_cognitive_assessment)
                connections_established += 1
                print("   [Topology] âœ… Connected: MetaCognitiveLayer â†’ DoubleHelixEngineV2")

            print(f"   [Topology] ğŸ§¬ æ‹“æ‰‘è¿æ¥å»ºç«‹å®Œæˆ: {connections_established} æ¡è¿æ¥")

        except Exception as e:
            print(f"   [Topology] âš ï¸ å»ºç«‹æ‹“æ‰‘è¿æ¥æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    async def _on_autonomous_goal_generated(self, event_data: dict):
        """
        å¤„ç†è‡ªä¸»ç›®æ ‡ç”Ÿæˆäº‹ä»¶
        å°†è‡ªä¸»ç”Ÿæˆçš„ç›®æ ‡æ³¨å…¥åˆ°åŒèºæ—‹å¼•æ“
        """
        try:
            goal = event_data.get('goal')
            if goal and self.helix_engine:
                # å°†ç›®æ ‡ä¼ é€’ç»™åŒèºæ—‹å¼•æ“
                print(f"   [EventFlow] ğŸ¯ è‡ªä¸»ç›®æ ‡æ³¨å…¥: {goal.get('description', 'Unknown')}")
                # è¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥å¤„ç†ï¼Œæ¯”å¦‚è°ƒæ•´èºæ—‹å‚æ•°ç­‰
        except Exception as e:
            print(f"   [EventFlow] âš ï¸ å¤„ç†è‡ªä¸»ç›®æ ‡äº‹ä»¶å¤±è´¥: {e}")

    async def _on_meta_cognitive_assessment(self, event_data: dict):
        """
        å¤„ç†å…ƒè®¤çŸ¥è¯„ä¼°äº‹ä»¶
        å°†å…ƒè®¤çŸ¥è¯„ä¼°ç»“æœåé¦ˆç»™åŒèºæ—‹å¼•æ“
        """
        try:
            assessment = event_data.get('assessment')
            if assessment and self.helix_engine:
                # å°†è¯„ä¼°ç»“æœåé¦ˆç»™åŒèºæ—‹å¼•æ“
                print(f"   [EventFlow] ğŸ§  å…ƒè®¤çŸ¥åé¦ˆ: ç½®ä¿¡åº¦={assessment.get('confidence', 0):.2f}")
                # è¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥å¤„ç†ï¼Œæ¯”å¦‚è°ƒæ•´å†³ç­–æƒé‡ç­‰
        except Exception as e:
            print(f"   [EventFlow] âš ï¸ å¤„ç†å…ƒè®¤çŸ¥è¯„ä¼°äº‹ä»¶å¤±è´¥: {e}")

    async def _process_llm_response_with_bridge(self, llm_response: str) -> str:
        """
        é€šè¿‡æ¡¥æ¥å±‚å¤„ç† LLM å“åº”ï¼Œæ‰§è¡Œå…¶ä¸­çš„å·¥å…·è°ƒç”¨ã€‚
        
        Args:
            llm_response: LLM çš„åŸå§‹å“åº”æ–‡æœ¬
            
        Returns:
            å¤„ç†åçš„å“åº”æ–‡æœ¬ï¼ˆåŒ…å«å·¥å…·æ‰§è¡Œç»“æœï¼‰
        """
        if not self.tool_bridge or not llm_response:
            return llm_response
        
        # æ£€æµ‹æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
        if "TOOL_CALL:" in llm_response or self._contains_tool_pattern(llm_response):
            try:
                print("   [Bridge] ğŸ”§ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œä¸­...")
                result = await self.tool_bridge.process_response(llm_response)
                if result.get('has_tool_calls'):
                    tool_count = len(result.get('tool_results', []))
                    success_count = sum(1 for r in result.get('tool_results', []) if r.get('result', {}).get('success'))
                    print(f"   [Bridge] âœ… å·¥å…·æ‰§è¡Œå®Œæˆ: {success_count}/{tool_count} æˆåŠŸ")
                    return result['final_response']
            except Exception as e:
                print(f"   [Bridge] âš ï¸ å·¥å…·æ‰§è¡Œå‡ºé”™: {e}")
        
        return llm_response
    
    def _contains_tool_pattern(self, response: str) -> bool:
        """æ£€æµ‹å“åº”æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨æ¨¡å¼"""
        patterns = [
            r'file_operation[s]?\.\w+\(',
            r'world_model\.\w+\(',
            r'knowledge_graph\.\w+\(',
            r'autonomous_document_create\.\w+\(',
            r'metacognition\.\w+\(',
            r'system_tools\.\w+\(',
            r'self_modif\w*\.\w+\(',  # ğŸ†• è‡ªä¿®æ”¹å·¥å…·æ¨¡å¼
            r'code_patch\.\w+\(',     # ğŸ†• ä»£ç è¡¥ä¸æ¨¡å¼
        ]
        for pattern in patterns:
            if re.search(pattern, response):
                return True
        return False
    
    # ========================================================================
    # ğŸ†• [2026-01-10] ç³»ç»Ÿä¾èµ–å›¾æ„å»ºï¼ˆä¾›å¢å¼ºéªŒè¯å™¨ä½¿ç”¨ï¼‰
    # ========================================================================
    
    def _build_system_dependency_graph(self) -> dict:
        """
        æ„å»ºç³»ç»Ÿå‡½æ•°ä¾èµ–å›¾
        
        æ‰«æAGIç³»ç»Ÿä¸­å·²å­˜åœ¨çš„å…¬å¼€å‡½æ•°ï¼Œä¾›éªŒè¯å™¨åˆ¤æ–­æ´å¯Ÿä»£ç æ˜¯å¦è°ƒç”¨äº†
        ä¸å­˜åœ¨çš„å‡½æ•°ï¼ˆä¼ªä»£ç æ£€æµ‹ï¼‰ã€‚
        
        è¿”å›:
            {å‡½æ•°å: True} çš„å­—å…¸
        """
        graph = {}
        
        # 1. æ³¨å†Œæ ¸å¿ƒæ¨¡å—çš„å…¬å¼€å‡½æ•°
        # æ³¨æ„ï¼šè¯¥æ–¹æ³•åœ¨ __init__ æ—©æœŸè¢«è°ƒç”¨ï¼Œéƒ¨åˆ†ç»„ä»¶å¯èƒ½å°šæœªåˆå§‹åŒ–
        core_objects = [
            ('memory', getattr(self, 'memory', None)),
            ('semantic_memory', getattr(self, 'semantic_memory', None)),
            ('biological_memory', getattr(self, 'biological_memory', None)),
            ('goal_manager', getattr(self, 'goal_manager', None)),
            ('motivation', getattr(self, 'motivation', None)),
            ('skill_manager', getattr(self, 'skill_manager', None)),
        ]
        
        for name, obj in core_objects:
            if obj:
                for attr_name in dir(obj):
                    if not attr_name.startswith('_'):
                        attr = getattr(obj, attr_name, None)
                        if callable(attr):
                            graph[attr_name] = True
        
        # 2. æ³¨å†Œå…¨å±€å·¥å…·å‡½æ•°
        global_funcs = [
            'save_insight_markdown',
            'log_cycle_flow',
            'parse_insight_file',
        ]
        for func_name in global_funcs:
            graph[func_name] = True
        
        # 3. æ³¨å†Œå¸¸ç”¨çš„ç³»ç»Ÿè¾…åŠ©å‡½æ•°
        utility_funcs = [
            'print', 'len', 'range', 'enumerate', 'zip', 'map', 'filter',
            'min', 'max', 'sum', 'sorted', 'reversed', 'abs', 'round',
            'isinstance', 'hasattr', 'getattr', 'setattr',
            'dict', 'list', 'set', 'tuple', 'str', 'int', 'float', 'bool',
        ]
        for func_name in utility_funcs:
            graph[func_name] = True

        # ğŸ†• [2026-01-15] 4. æ³¨å†Œæ•°å­¦å‡½æ•°ï¼ˆä¿®å¤Insightä¾èµ–ç¼ºå¤±é—®é¢˜ï¼‰
        math_funcs = [
            'exponential', 'exp',  # æŒ‡æ•°å‡½æ•°ï¼ˆæ¥è‡ªnumpy/mathï¼‰
            'sqrt', 'square',     # å¹³æ–¹æ ¹/å¹³æ–¹
            'log', 'log10', 'log2',  # å¯¹æ•°å‡½æ•°
            'sin', 'cos', 'tan',   # ä¸‰è§’å‡½æ•°
            'pow', 'power',        # å¹‚å‡½æ•°
            'mean', 'median', 'std',  # ç»Ÿè®¡å‡½æ•°ï¼ˆnumpyï¼‰
        ]
        for func_name in math_funcs:
            graph[func_name] = True

        return graph

    async def _on_insight_generated(self, event: Event):
        try:
            data = event.data
            ts_float = data.get("timestamp", time.time())
            now_ts = time.time()
            node_id = data.get("node_id") or f"Insight_{ts_float}"
            if now_ts < getattr(self, "_insight_persist_backoff_until", 0.0):
                remaining = int(max(0.0, getattr(self, "_insight_persist_backoff_until", 0.0) - now_ts))
                print(f"   [System] âš ï¸ Insight persistence backoff active ({remaining}s remaining).")
                persist_result = {"success": False, "error": "backoff_active"}
                file_path = None
                abs_path = None
                timestamp = int(ts_float)
            else:
                persist_result = save_insight_markdown(data)
                if not persist_result.get("success"):
                    failure_ts = getattr(self, "_insight_persist_failure_ts", 0.0)
                    if now_ts - failure_ts > 60.0:
                        self._insight_persist_failure_count = 0
                    self._insight_persist_failure_count = getattr(self, "_insight_persist_failure_count", 0) + 1
                    self._insight_persist_failure_ts = now_ts
                    if self._insight_persist_failure_count >= 3:
                        self._insight_persist_backoff_until = now_ts + 60.0
                    print(f"   [System] âš ï¸ Failed to save insight: {persist_result.get('error', 'insight persist failed')}")
                    file_path = None
                    abs_path = None
                    timestamp = int(ts_float)
                else:
                    file_path = persist_result["file_path"]
                    abs_path = persist_result["abs_path"]
                    timestamp = persist_result["timestamp"]
                    self._insight_persist_failure_count = 0
                    self._insight_persist_failure_ts = 0.0
                    self._insight_persist_backoff_until = 0.0

                if file_path:
                    print(f"   [System] ğŸ“ Insight saved to {file_path}")
            
            # 2. Update Knowledge Graph Node with File Path (Topology Consistency)
            # Ensure the node in memory points to the physical file
            try:
                attrs = {
                    "persist_status": "ok" if abs_path else "failed",
                    "persist_error": None if abs_path else persist_result.get("error")
                }
                if abs_path:
                    attrs["file_path"] = abs_path
                # ğŸ†• [2026-01-30] P1ä¿®å¤: ä½¿ç”¨å­¤ç«‹èŠ‚ç‚¹é¢„é˜²
                if self.isolation_prevention:
                    self.isolation_prevention.add_node_with_prevention(node_id, **attrs)
                else:
                    self.memory.graph.add_node(node_id, **attrs)
            except Exception as e:
                print(f"   [System] âš ï¸ Failed to update insight node topology: {e}")

            # 3. Internalize into Biological Memory (Fluid Topology)
            try:
                if persist_result.get("error") == "backoff_active":
                    return
                epochs = 50 if abs_path else 10
                print(f"   [BioMemory] ğŸ§  Internalizing insight into Fluid Topology...")
                insight_item = {
                    "id": f"insight_{timestamp}",
                    "content": data.get("content", "") or "",
                    "source": "event_bus",
                    "persisted": bool(abs_path)
                }
                # Internalize (consolidate) this single insight immediately
                # This triggers topological growth and connection
                stats = self.biological_memory.internalize([insight_item], epochs=epochs)
                print(f"   [BioMemory] âœ… Insight internalized. Loss: {stats.get('final_loss', 0.0):.4f}")
                print(f"   [BioMemory]    Topology Size: {self.biological_memory.topology.size()} nodes")
            except Exception as e:
                print(f"   [BioMemory] âš ï¸ Internalization failed: {e}")
            
        except Exception as e:
            print(f"   [System] âš ï¸ Failed to save insight: {e}")

    # ========================================================================
    # é¦ƒå• [2026-01-15] é™å²ƒçªéƒå¬ªå–…ç»›æ §ç´©é¿åº¨æ³¦é´æ„­æŸŸå¨‰?
    # ========================================================================

    def _encode_helix_state(self, context: Dict[str, Any]) -> 'np.ndarray':
        """
        [REFACTORED 2026-01-29] Real Semantic Encoding
        Uses PerceptionSystem (SentenceTransformers) instead of MD5.
        Generates a 64-dim state vector that captures TRUE meaning.
        """
        if hasattr(self, 'perception_system') and self.perception_system:
            # Inject scalar metrics for the encoder to append
            context['priority_score'] = {'low': 0.2, 'medium': 0.5, 'high': 0.8, 'critical': 1.0}.get(context.get('priority', 'medium'), 0.5)
            context['urgency'] = min(1.0, self.step_count / 1000.0)
            context['success_probability'] = context.get('success_probability', 0.5)
            
            # Use the real embedder
            return self.perception_system.encode_helix_state(context, target_dim=64)
        else:
            # Fallback (Should rarely happen if init succeeded)
            import numpy as np
            return np.zeros(64, dtype=np.float32)
    
    async def _helix_enhanced_decision(self, action_context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨åŒèºæ—‹å¼•æ“å¢å¼ºå†³ç­–ã€‚
        
        è¿”å›ï¼š
        - enhanced_action: å¢å¼ºåçš„åŠ¨ä½œå»ºè®®
        - helix_confidence: åŒèºæ—‹ç½®ä¿¡åº¦
        - fusion_method: ä½¿ç”¨çš„èåˆæ–¹æ³•
        - emergence_score: æ¶Œç°è¯„åˆ†
        """
        if not self.helix_decision_enabled or self.helix_engine is None:
            return {
                'enhanced': False,
                'reason': 'Helix engine not available'
            }
        
        try:
            # ç¼–ç å½“å‰çŠ¶æ€
            state_vector = self._encode_helix_state(action_context)
            
            # è°ƒç”¨åŒèºæ—‹å¼•æ“
            helix_result = self.helix_engine.decide(state_vector)
            
            # æå–å†³ç­–ä¿¡æ¯
            enhanced_info = {
                'enhanced': True,
                'helix_action': helix_result.action,
                'helix_confidence': helix_result.confidence,
                'fusion_method': helix_result.fusion_method,
                'emergence_score': helix_result.emergence_score,
                'system_a_conf': helix_result.system_a_confidence,
                'system_b_conf': helix_result.system_b_confidence,
                'complementary_preference': helix_result.complementary_preference,
                'reasoning': helix_result.reasoning
            }
            
            # å¦‚æœæ˜¯åˆ›é€ æ€§èåˆåŠ¨ä½œï¼ˆ4-7ï¼‰ï¼Œæ ‡è®°ä¸ºåˆ›é€ æ€§å†³ç­–
            if helix_result.action >= 4:
                enhanced_info['is_creative'] = True
                enhanced_info['creative_action_name'] = self._get_creative_action_name(helix_result.action)
            else:
                enhanced_info['is_creative'] = False
            
            return enhanced_info
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            return {
                'enhanced': False,
                'reason': f'Helix decision failed: {e}'
            }
    
    def _get_creative_action_name(self, action_id: int) -> str:
        """è·å–åˆ›é€ æ€§åŠ¨ä½œåç§°"""
        creative_actions = {
            4: 'stop_and_observe',
            5: 'explore_alternative',
            6: 'synthesize_novel',
            7: 'meta_reflect'
        }
        return creative_actions.get(action_id, f'creative_{action_id}')

    async def _process_intent_bridge(self):
        """
        å¤„ç†æ¥è‡ªæ„å›¾æ¡¥æ¥çš„ç”¨æˆ·æ„å›¾ã€‚
        å®ç°æ·±åº¦æ„å›¾ç†è§£å’Œç¡®è®¤æµç¨‹ã€‚
        
        IntentDialogueBridge APIï¼š
        - poll_new_intent() -> Optional[Intent]
        - analyze_intent(intent: Intent) -> Intent  # ä¿®æ”¹intentçš„æ·±åº¦å±æ€§
        - generate_confirmation(intent: Intent) -> Intent  # ä¿®æ”¹intentçŠ¶æ€
        - send_confirmation_request(intent: Intent)  # å‘é€ç¡®è®¤è¯·æ±‚
        - lock_attention(intent: Intent)  # é”å®šæ³¨æ„åŠ›
        - send_execution_result(intent: Intent, result: str, success: bool)
        """
        if not self.intent_bridge:
            return

        # ğŸ†• [ADAPTIVE POLLING 2026-01-27] ä½¿ç”¨è‡ªé€‚åº”è½®è¯¢ç­–ç•¥
        should_poll = True  # æ˜¯å¦åº”è¯¥è½®è¯¢
        poll_timeout = 1.0  # é»˜è®¤è¶…æ—¶

        if self._adaptive_poller and hasattr(self.intent_bridge, 'check_cli_status'):
            try:
                # æ”¶é›†çŠ¶æ€ä¿¡æ¯
                cli_status = self.intent_bridge.check_cli_status()
                cli_online = cli_status.get('online', True)
                queue_length = self.intent_bridge.get_queue_length()

                # è®¡ç®—è½®è¯¢ç­–ç•¥
                strategy = self._adaptive_poller.calculate_strategy(
                    cli_online=cli_online,
                    queue_length=queue_length
                )

                # åˆ¤æ–­æ˜¯å¦åº”è¯¥åœ¨æœ¬TICKè½®è¯¢
                should_poll = self._adaptive_poller.should_poll_this_tick(
                    current_tick=self.step_count,
                    last_poll_tick=self._last_poll_tick,
                    strategy=strategy
                )

                # åº”ç”¨ç­–ç•¥
                if should_poll:
                    self._last_poll_tick = self.step_count
                    poll_timeout = strategy.timeout

                    # è®°å½•ç­–ç•¥å˜æ›´ï¼ˆä»…æ—¥å¿—çº§åˆ«ï¼‰
                    if logger.isEnabledFor(logging.DEBUG):
                        logger.debug(
                            f"[AdaptivePoller] ğŸ¯ ç­–ç•¥: {strategy.mode} | "
                            f"é—´éš”: {strategy.interval_ticks} TICK | "
                            f"è¶…æ—¶: {strategy.timeout}s | "
                            f"åŸå› : {strategy.reason}"
                        )
                else:
                    # è·³è¿‡æœ¬è½®
                    if strategy.mode == "idle":
                        logger.debug(f"[AdaptivePoller] ğŸ’¤ CLIç¦»çº¿ï¼Œè·³è¿‡è½®è¯¢ (TICK {self.step_count})")
                    elif strategy.mode == "empty":
                        logger.debug(f"[AdaptivePoller] ğŸ“‰ ç©ºè½®è¯¢ç‡è¿‡é«˜ï¼Œè·³è¿‡æœ¬è½®")

            except Exception as e:
                logger.warning(f"âš ï¸ è‡ªé€‚åº”è½®è¯¢å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥")
                should_poll = True

        # å¦‚æœä¸åº”è¯¥è½®è¯¢ï¼Œç›´æ¥è¿”å›
        if not should_poll:
            return

        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¾…æ‰§è¡Œçš„å·²ç¡®è®¤æ„å›¾
            current = self.intent_bridge.get_current_intent()
            if current:
                # ğŸ”§ [FIX 2026-01-18] å…ˆæ£€æŸ¥ç¡®è®¤è¶…æ—¶ï¼Œé¿å…æ°¸ä¹…é˜»å¡
                self.intent_bridge._check_confirmation_timeout()

                # é‡æ–°è·å–å½“å‰æ„å›¾ï¼ˆå¯èƒ½å·²å› è¶…æ—¶è€Œæ”¹å˜ï¼‰
                current = self.intent_bridge.get_current_intent()
                if not current:
                    # æ„å›¾å·²è¶…æ—¶å¹¶è‡ªåŠ¨ç¡®è®¤ï¼Œç»§ç»­å¤„ç†
                    pass
                elif current.state == IntentState.CONFIRMED:
                    print(f"   [IntentBridge] â–¶ï¸ æ£€æµ‹åˆ°å·²ç¡®è®¤æ„å›¾ï¼Œå¼€å§‹æ‰§è¡Œ: {current.id[:8]}...")
                    await self._execute_confirmed_intent(current)
                    return
                elif current.state == IntentState.CONFIRMING:
                    # ğŸ†• [FIX 2026-01-27] æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                    elapsed = time.time() - current.timestamp
                    timeout_seconds = 300  # 5åˆ†é’Ÿè¶…æ—¶

                    if elapsed > timeout_seconds:
                        # è¶…æ—¶ï¼šè‡ªåŠ¨æ ‡è®°ä¸ºå¤±è´¥å¹¶ç»§ç»­å¤„ç†
                        print(f"   [IntentBridge] â° æ„å›¾ç¡®è®¤è¶…æ—¶ ({elapsed:.0f}s > {timeout_seconds}s): {current.id[:8]}...")
                        current.state = IntentState.FAILED
                        current.error_message = f"ç¡®è®¤è¶…æ—¶ ({elapsed:.0f}ç§’)"
                        self.intent_bridge.unlock_attention()

                        # æ¸…é™¤å½“å‰æ„å›¾ï¼Œç»§ç»­å¤„ç†pendingé˜Ÿåˆ—
                        print(f"   [IntentBridge] ğŸ”„ ç»§ç»­å¤„ç†pendingé˜Ÿåˆ—...")
                    else:
                        # è¿˜åœ¨ç­‰å¾…ç¡®è®¤ï¼Œä½†ä¸é˜»å¡pendingé˜Ÿåˆ—
                        # å°è¯•å¤„ç†pendingé˜Ÿåˆ—ä¸­çš„éCONFIRMINGæ„å›¾
                        print(f"   [IntentBridge] â³ æ„å›¾ç­‰å¾…ç¡®è®¤ä¸­ ({elapsed:.0f}s)ï¼Œå°è¯•å¤„ç†pendingé˜Ÿåˆ—...")
                        # ç»§ç»­æ‰§è¡Œï¼Œä¸returnï¼Œå…è®¸å¤„ç†pendingé˜Ÿåˆ—
                elif current.state == IntentState.REJECTED:
                    # æ„å›¾è¢«æ‹’ç»ï¼Œæ¸…ç†å¹¶ç»§ç»­
                    print(f"   [IntentBridge] ğŸš« æ„å›¾å·²è¢«ç”¨æˆ·æ‹’ç»: {current.id[:8]}...")
                    self.intent_bridge.unlock_attention()
                    return

            # è½®è¯¢æ–°æ„å›¾ï¼ˆä½¿ç”¨è‡ªé€‚åº”è¶…æ—¶ï¼‰
            intent = self.intent_bridge.poll_new_intent(timeout=poll_timeout)

            # ğŸ†• [ADAPTIVE POLLING 2026-01-27] è®°å½•è½®è¯¢ç»“æœ
            if self._adaptive_poller:
                is_empty = (intent is None)
                self._adaptive_poller.record_poll_result(is_empty)

            if not intent:
                return
            
            print(f"   [IntentBridge] ğŸ“¥ æ”¶åˆ°æ–°æ„å›¾: {intent.id[:8]}...")
            print(f"   [IntentBridge]    åŸæ–‡: {intent.raw_input[:100]}...")
            
            # åˆ†ææ„å›¾æ·±åº¦ - analyze_intent æ¥å— Intent å¯¹è±¡å¹¶è¿”å›ä¿®æ”¹åçš„ Intent
            intent = self.intent_bridge.analyze_intent(intent)
            
            print(f"   [IntentBridge] ğŸ” æ„å›¾åˆ†æå®Œæˆ:")
            print(f"   [IntentBridge]    æ·±åº¦: {intent.depth.value if intent.depth else 'UNKNOWN'}")
            print(f"   [IntentBridge]    è¡¨é¢è¯·æ±‚: {intent.surface_request[:50]}...")
            print(f"   [IntentBridge]    æ·±å±‚ç›®æ ‡: {intent.deep_goal[:50] if intent.deep_goal else 'None'}...")
            
            # æ ¹æ®æ·±åº¦å†³å®šæ˜¯å¦éœ€è¦ç¡®è®¤
            needs_confirmation = intent.depth in [IntentDepth.DEEP, IntentDepth.PHILOSOPHICAL]
            
            if needs_confirmation:
                # ç”Ÿæˆç¡®è®¤è¯·æ±‚ - generate_confirmation æ¥å— Intent å¯¹è±¡
                intent = self.intent_bridge.generate_confirmation(intent)
                
                # å‘é€ç¡®è®¤è¯·æ±‚ç»™ç”¨æˆ·
                self.intent_bridge.send_confirmation_request(intent)
                
                print(f"   [IntentBridge] ğŸ”„ ç­‰å¾…ç”¨æˆ·ç¡®è®¤...")
                
                # ç­‰å¾…ç”¨æˆ·ç¡®è®¤ï¼ˆéé˜»å¡ï¼Œä¸‹ä¸€è½®å¾ªç¯ç»§ç»­æ£€æŸ¥ï¼‰
                return
            
            # è¡¨é¢/ä¸­ç­‰æ·±åº¦æ„å›¾ï¼Œè‡ªåŠ¨ç¡®è®¤å¹¶æ‰§è¡Œ
            intent.state = IntentState.CONFIRMED
            self.intent_bridge.lock_attention(intent)
            await self._execute_confirmed_intent(intent)
            
        except Exception as e:
            print(f"   [IntentBridge] âŒ æ„å›¾å¤„ç†é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            
            # âœ… [FIX 2026-01-09] å³ä½¿å‡ºé”™ä¹Ÿè¦å‘é€å“åº”ï¼Œé¿å…CLIæ°¸ä¹…ç­‰å¾…
            if hasattr(self, 'intent_bridge') and self.intent_bridge:
                try:
                    # å°è¯•è·å–å½“å‰æ„å›¾
                    current_intent = self.intent_bridge.get_active_intent()
                    if current_intent:
                        self.intent_bridge.send_execution_result(
                            current_intent,
                            f"æ„å›¾å¤„ç†å¤±è´¥: {str(e)}",
                            success=False
                        )
                except Exception as send_err:
                    print(f"   [IntentBridge] âš ï¸ æ— æ³•å‘é€é”™è¯¯å“åº”: {send_err}")

    # ==================== ğŸ†• äº‹ä»¶å¤„ç†æœºåˆ¶ [EVENT FLOW 2026-01-27] ====================

    def _handle_agi_event(self, event_data: Dict[str, Any]) -> None:
        """
        å¤„ç†æ¥è‡ªRedis Pub/Subçš„äº‹ä»¶æµ

        æ³¨æ„ï¼šæ­¤æ–¹æ³•åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼Œä¸åº”æ‰§è¡Œé˜»å¡æ“ä½œ
             åªè®°å½•äº‹ä»¶æ—¥å¿—ï¼Œä¸ç ´åTICKé©±åŠ¨çš„è‡ªä¸»èŠ‚å¾‹

        Args:
            event_data: äº‹ä»¶æ•°æ®å­—å…¸
                - event_type: äº‹ä»¶ç±»å‹
                - source: äº‹ä»¶æ¥æº
                - timestamp: æ—¶é—´æˆ³
                - client_id: å®¢æˆ·ç«¯ID
                - data: äº‹ä»¶æ•°æ®
        """
        event_type = event_data.get('event_type')
        source = event_data.get('source', 'unknown')
        client_id = event_data.get('client_id', 'unknown')
        data = event_data.get('data', {})

        try:
            if event_type == 'intent_submitted':
                # æ„å›¾å·²æäº¤äº‹ä»¶
                intent_id = data.get('intent_id', 'unknown')[:8]
                queue_depth = data.get('queue_depth', 0)
                user_input = data.get('user_input', '')[:50]

                logger.info(f"[Event] ğŸ“¨ æ„å›¾å·²æäº¤: {intent_id}... | é˜Ÿåˆ—æ·±åº¦: {queue_depth}")
                logger.debug(f"[Event]    æ¥æº: {client_id} | å†…å®¹: {user_input}...")

                # å¯é€‰ï¼šå¦‚æœé˜Ÿåˆ—æ·±åº¦è¾ƒå¤§ï¼Œå¯ä»¥ä¸´æ—¶æå‡è½®è¯¢ä¼˜å…ˆçº§
                if queue_depth > 5:
                    logger.warning(f"[Event] âš ï¸ æ„å›¾é˜Ÿåˆ—ç§¯å‹: {queue_depth}ä¸ªæ„å›¾å¾…å¤„ç†")

            elif event_type == 'intent_completed':
                # æ„å›¾å®Œæˆäº‹ä»¶ï¼ˆå¯é€‰ï¼Œæœªæ¥æ‰©å±•ï¼‰
                intent_id = data.get('intent_id', 'unknown')[:8]
                success = data.get('success', False)
                logger.info(f"[Event] âœ… æ„å›¾å·²å®Œæˆ: {intent_id}... | æˆåŠŸ: {success}")

            elif event_type == 'cli_connected':
                # CLIè¿æ¥äº‹ä»¶
                logger.info(f"[Event] ğŸ’š Chat CLIå·²ä¸Šçº¿: {client_id}")

            else:
                # æœªçŸ¥äº‹ä»¶ç±»å‹
                logger.debug(f"[Event] â“ æœªçŸ¥äº‹ä»¶ç±»å‹: {event_type} | æ¥æº: {source}")

        except Exception as e:
            logger.warning(f"âš ï¸ äº‹ä»¶å¤„ç†å¤±è´¥: {e} | äº‹ä»¶: {event_type}")

    # ==================== åŸæœ‰æ–¹æ³• ====================

    def _validate_plan_feasibility(self, plan: list) -> tuple:
        """
        éªŒè¯è®¡åˆ’çš„å¯è¡Œæ€§ (Pre-Execution Validation)
        
        æ£€æŸ¥è®¡åˆ’æ­¥éª¤æ˜¯å¦åŒ…å«æœªçŸ¥çš„å·¥å…·å¼•ç”¨ï¼Œé˜²æ­¢å¹»è§‰è°ƒç”¨ã€‚
        
        Returns:
            (is_feasible: bool, warning: str)
        """
        if not self.tool_bridge:
            return True, ""
            
        available_tools = self.tool_bridge.get_available_tools()
        # è½¬æ¢ä¸ºå°å†™ä»¥ä¾¿åŒ¹é…
        tools_lower = {t.lower() for t in available_tools}
        
        warnings = []
        for i, step in enumerate(plan):
            step_lower = step.lower() if isinstance(step, str) else str(step).lower()
            
            # æ£€æµ‹å·¥å…·è°ƒç”¨æ¨¡å¼: tool_name.method() æˆ– tool_name(
            tool_patterns = re.findall(r'\b([a-zA-Z_][a-zA-Z0-9_]*)\s*\.\s*\w+\s*\(', step_lower)
            tool_patterns += re.findall(r'\b([a-zA-Z_][a-zA-Z0-9_]*)\s*\(', step_lower)
            
            for tool_name in tool_patterns:
                # æ’é™¤å¸¸è§çš„éå·¥å…·è°ƒç”¨
                common_non_tools = {'print', 'str', 'int', 'float', 'list', 'dict', 'set', 
                                   'len', 'range', 'open', 'close', 'read', 'write',
                                   'async', 'await', 'self', 'return', 'if', 'for', 'while'}
                if tool_name in common_non_tools:
                    continue
                    
                # æ£€æŸ¥æ˜¯å¦æ˜¯å·²æ³¨å†Œå·¥å…·
                if tool_name not in tools_lower:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å·²æ³¨å†Œå·¥å…·çš„å­ä¸²ï¼ˆå¦‚ file å¯¹åº” file_operationï¼‰
                    is_partial_match = any(tool_name in t or t.startswith(tool_name) for t in tools_lower)
                    if not is_partial_match and len(tool_name) > 3:
                        warnings.append(f"Step {i+1}: æœªçŸ¥å·¥å…· '{tool_name}'")
        
        if warnings:
            return False, "; ".join(warnings[:3])
        return True, ""

    def _show_intelligence_upgrade_status(self):
        """
        Display intelligence upgrade status (Phase 1: Working Memory)
        """
        if not hasattr(self, 'working_memory') or not self.working_memory:
            print("   [Intelligence Upgrade] Not enabled")
            return

        summary = self.working_memory.get_context_summary()
        stats = summary['stats']

        print("\n   [Intelligence Upgrade Status]")
        print(f"   Active thoughts: {summary['active_thoughts_count']}/{self.working_memory.capacity}")
        print(f"   Current action: {summary['current_action']}")
        print(f"   Thought diversity: {summary['diversity']:.2f}")
        print(f"   Total thoughts: {stats['total_thoughts']}")
        print(f"   Loops detected: {stats['loops_detected']}")
        print(f"   Loops broken: {stats['loops_broken']}")
        print(f"   Divergent thoughts: {stats['divergent_thoughts']}")
        
        # ğŸ†• [2026-01-30] P0ä¿®å¤: å·¥ä½œè®°å¿†ä¼˜åŒ–å™¨ç»Ÿè®¡
        if hasattr(self, '_working_memory_optimizer') and self._working_memory_optimizer:
            opt_stats = self._working_memory_optimizer.get_stats()
            print("\n   [Working Memory Optimizer Status]")
            print(f"   Cache hit rate: {opt_stats['hit_rate']:.2%}")
            print(f"   Cache size: {opt_stats['cache_size']}/{opt_stats.get('max_size', 1000)}")
            print(f"   Total requests: {opt_stats['total_requests']}")
            print(f"   Cache hits: {opt_stats['cache_hits']}")
            print(f"   LRU evictions: {opt_stats['evictions']}")

    def _show_reasoning_status(self):
        """
        Display reasoning scheduler status (Phase 2: Deep Reasoning)
        """
        if not hasattr(self, 'reasoning_scheduler') or not self.reasoning_scheduler:
            print("   [Reasoning Scheduler] Not enabled")
            return

        print("\n   [Reasoning Scheduler Status]")

        # Get current session summary
        session_summary = self.reasoning_scheduler.get_current_session_summary()
        stats = self.reasoning_scheduler.get_statistics()

        if session_summary:
            print(f"   Session ID: {session_summary.get('session_id', 'N/A')}")
            print(f"   Total steps: {session_summary.get('total_steps', 0)}")
            print(f"   Current depth: {session_summary.get('max_depth', 0)}/{self.reasoning_scheduler.max_depth}")
            print(f"   Avg confidence: {session_summary.get('avg_confidence', 0):.2f}")
            print(f"   Avg step time: {session_summary.get('avg_step_time', 0):.3f}s")

            # Mode distribution
            mode_dist = session_summary.get('mode_distribution', {})
            if mode_dist:
                print(f"   Reasoning modes:")
                for mode, count in mode_dist.items():
                    print(f"     - {mode}: {count}")

        # Overall statistics
        print(f"\n   Overall Statistics:")
        print(f"   Total reasoning calls: {stats.get('total_reasoning_calls', 0)}")
        print(f"   Causal reasoning used: {stats.get('causal_reasoning_used', 0)}")
        print(f"   LLM fallback used: {stats.get('llm_fallback_used', 0)}")
        print(f"   Hybrid reasoning used: {stats.get('hybrid_reasoning_used', 0)}")
        print(f"   Max depth achieved: {stats.get('max_depth_achieved', 0)}")
        print(f"   Causal ratio: {stats.get('causal_ratio', 0):.2%}")

        # Recent reasoning chain
        chain = self.reasoning_scheduler.get_reasoning_chain(n=5)
        if chain:
            print(f"\n   Recent reasoning chain (last {len(chain)} steps):")
            for step in chain:
                print(f"     [{step['step']}] {step['mode']} - depth={step['depth']}, conf={step['confidence']:.2f}")

    def _show_world_model_status(self):
        """Display world model status (Phase 3)"""
        if not hasattr(self, 'world_model') or not self.world_model:
            print("   [World Model] Not enabled")
            return

        print("\n   [Bayesian World Model Status]")

        summary = self.world_model.get_state_summary()

        print(f"   Total beliefs: {summary['total_beliefs']}")
        print(f"   Causal links: {summary['total_causal_links']}")
        print(f"   Interventions: {summary['total_interventions']}")
        print(f"   Avg confidence: {summary['avg_belief_confidence']:.3f}")
        print(f"   High confidence beliefs: {summary['high_confidence_beliefs']}")

        # Show sample beliefs
        beliefs = self.world_model.get_all_beliefs()
        if beliefs:
            print(f"\n   Sample beliefs (first 5):")
            for i, belief in enumerate(list(beliefs.values())[:5], 1):
                print(f"     {i}. {belief}")

    def _show_goal_manager_status(self):
        """Display goal manager status (Phase 3)"""
        if not hasattr(self, 'goal_manager') or not self.goal_manager:
            print("   [Goal Manager] Not enabled")
            return

        print("\n   [Hierarchical Goal Manager Status]")

        summary = self.goal_manager.get_summary()

        print(f"   Total goals: {summary['total_goals']}")
        print(f"   Active goals: {summary['active_goals']}")
        print(f"   Avg priority: {summary['avg_priority']:.2f}")
        print(f"   Active conflicts: {summary['active_conflicts']}")

        # Show by level
        if summary['by_level']:
            print(f"\n   Goals by level:")
            for level, count in summary['by_level'].items():
                print(f"     {level}: {count}")

        # Show active goals
        active_goals = self.goal_manager.get_active_goals()
        if active_goals:
            print(f"\n   Active goals (top 5):")
            for i, goal in enumerate(active_goals[:5], 1):
                print(f"     {i}. {goal.name} ({goal.level.value}) - priority={goal.priority:.2f}, progress={goal.progress:.0%}")

    def _show_creative_engine_status(self):
        """Display creative exploration engine status (Phase 3)"""
        if not hasattr(self, 'creative_engine') or not self.creative_engine:
            print("   [Creative Engine] Not enabled")
            return

        print("\n   [Creative Exploration Engine Status]")

        stats = self.creative_engine.get_statistics()

        print(f"   Total explorations: {stats['total_explorations']}")
        print(f"   Novel ideas: {stats['novel_ideas_generated']}")
        print(f"   Avg novelty: {stats.get('avg_novelty', 0):.3f}")
        print(f"   Avg feasibility: {stats.get('avg_feasibility', 0):.3f}")
        print(f"   Avg value: {stats.get('avg_value', 0):.3f}")
        print(f"   Novelty ratio: {stats['novelty_ratio']:.2%}")

        # Show mode distribution
        print(f"\n   Exploration modes:")
        print(f"     Analogical: {stats['analogical_reasoning_used']}")
        print(f"     Combinatorial: {stats['combinatorial_creativity_used']}")
        print(f"     Stochastic: {stats['stochastic_exploration_used']}")

        # Show top explorations
        top_explorations = self.creative_engine.get_top_explorations(3)
        if top_explorations:
            print(f"\n   Top explorations (by value):")
            for i, result in enumerate(top_explorations, 1):
                print(f"     {i}. [{result.mode.value}] novelty={result.novelty_score:.2f}, value={result.value_score:.2f}")
                print(f"        Idea: {result.output_idea[:80]}...")

    def _show_meta_learner_status(self):
        """Display meta-learner status (Phase 4)"""
        if not hasattr(self, 'meta_learner') or not self.meta_learner:
            print("   [Meta-Learner] Not enabled")
            return

        print("\n   [Meta-Learner Status]")

        stats = self.meta_learner.get_statistics()

        print(f"   Tasks learned: {stats['total_tasks_learned']}")
        print(f"   Adaptations: {stats['total_adaptations']}")
        print(f"   Experience count: {stats['experience_count']}")
        print(f"   Knowledge domains: {stats['knowledge_domains']}")
        print(f"   Total strategies: {stats['total_strategies']}")
        print(f"   Best strategy: {stats.get('best_strategy', 'N/A')}")
        print(f"   Avg adaptation speed: {stats['avg_adaptation_speed']:.3f}")

        # Show meta-knowledge
        summary = self.meta_learner.get_meta_knowledge_summary()
        if summary:
            print(f"\n   Meta-knowledge domains:")
            for i, domain in enumerate(summary[:3], 1):
                print(f"     {i}. {domain['domain']}: {domain['patterns_count']} patterns, "
                      f"transferability={domain['transferability']:.2f}")

    def _show_self_improvement_status(self):
        """Display self-improvement status (Phase 4)"""
        if not hasattr(self, 'self_improvement_engine') or not self.self_improvement_engine:
            print("   [Self-Improvement] Not enabled")
            return

        print("\n   [Self-Improvement Engine Status]")

        stats = self.self_improvement_engine.get_statistics()

        print(f"   Modules scanned: {stats['modules_scanned']}")
        print(f"   Total LOC: {stats['total_lines_of_code']}")
        print(f"   Total complexity: {stats['total_complexity']:.1f}")
        print(f"   Total proposals: {stats['total_proposals']}")
        print(f"   Applied improvements: {stats['applied_improvements']}")
        print(f"   Successful: {stats['successful_improvements']}")
        print(f"   Failed: {stats['failed_improvements']}")
        print(f"   Rollbacks: {stats['rollbacks']}")
        print(f"   Total performance gain: {stats['total_performance_gain']:.2%}")

        # Show improvement history
        summary = self.self_improvement_engine.get_improvement_summary()
        if summary:
            print(f"\n   Recent improvements (last {len(summary)}):")
            for i, entry in enumerate(summary[-3:], 1):
                print(f"     {i}. {entry['timestamp']}: {entry['success']} - delta={entry['performance_delta']:.2%}")

    def _show_metacognitive_status(self):
        """Display metacognitive status (Phase 4 + New Meta-Cognitive Layer)"""
        print("\n   ===== Meta-Cognitive System Status =====")

        # æ–°å…ƒè®¤çŸ¥å±‚ (P0ä¿®å¤ 2026-01-16)
        if hasattr(self, 'meta_cognitive_layer') and self.meta_cognitive_layer:
            print("\n   [Meta-Cognitive Layer] ğŸ§  Self-Reflection Enabled")
            stats = self.meta_cognitive_layer.get_stats()
            print(f"   Total evaluations: {stats['total_evaluations']}")
            print(f"   Proceed rate: {stats['proceed_rate']:.2%}")
            print(f"   Decline rate: {stats['decline_rate']:.2%}")
            print(f"   Escalate rate: {stats['escalate_rate']:.2%}")
            print(f"   Caution rate: {stats['caution_rate']:.2%}")
            
        # ğŸ†• [2026-01-30] P0ä¿®å¤: å…ƒè®¤çŸ¥è¿‡æ»¤å™¨ç»Ÿè®¡
        if hasattr(self, 'meta_filter') and self.meta_filter:
            print("\n   [Meta-Cognitive Filter Status]")
            filter_stats = self.meta_filter.get_stats()
            print(f"   Total evaluation requests: {filter_stats['total_requests']}")
            print(f"   Actual evaluations: {filter_stats['actual_evaluations']}")
            print(f"   Filter rate: {filter_stats['filter_rate']:.2%}")
            print(f"   Filtered by complexity: {filter_stats['filtered_by_complexity']}")
            print(f"   Filtered by cooldown: {filter_stats['filtered_by_cooldown']}")
            print(f"   Filtered by duplicate: {filter_stats['filtered_by_duplicate']}")
            print(f"   Filtered by whitelist: {filter_stats['filtered_by_whitelist']}")
            print(f"   Actionable insights: {filter_stats['actionable_insights']}")
            print(f"   False positive estimate: {filter_stats['false_positive_estimate']:.2%}")
        else:
            print("\n   [Meta-Cognitive Filter] Not enabled")
            
        if not (hasattr(self, 'meta_cognitive_layer') and self.meta_cognitive_layer):
            print("\n   [Meta-Cognitive Layer] Not enabled")

        # ğŸ†• [2026-01-30] P1ä¿®å¤: å­¤ç«‹èŠ‚ç‚¹é¢„é˜²ç»Ÿè®¡
        if hasattr(self, 'isolation_prevention') and self.isolation_prevention:
            print("\n   [Isolated Node Prevention Status]")
            iso_stats = self.isolation_prevention.get_stats()
            print(f"   Nodes created: {iso_stats['nodes_created']}")
            print(f"   Auto connected: {iso_stats['auto_connected']}")
            print(f"   Hub connected: {iso_stats['hub_connected']}")
            print(f"   Current isolated: {iso_stats['current_isolated']}")
            print(f"   Isolation rate: {iso_stats['isolation_rate']:.2%}")
            print(f"   Isolated rescued: {iso_stats['isolated_rescued']}")
        else:
            print("\n   [Isolated Node Prevention] Not enabled")

        # ğŸ†• [2026-01-30] P1ä¿®å¤: å¤æ‚ä»»åŠ¡ç”Ÿæˆå™¨ç»Ÿè®¡
        if hasattr(self, 'complex_task_generator') and self.complex_task_generator:
            print("\n   [Complex Task Generator Status]")
            task_stats = self.complex_task_generator.get_stats()
            print(f"   Tasks generated: {task_stats['tasks_generated']}")
            print(f"   Average complexity: {task_stats['avg_complexity']:.2f}")
            print(f"   By type: {task_stats['by_type']}")
            dist = self.complex_task_generator.get_complexity_distribution()
            print(f"   Complexity distribution: shallow={dist['shallow']}, medium={dist['medium']}, deep={dist['deep']}")
        else:
            print("\n   [Complex Task Generator] Not enabled")

        # ğŸ†• [2026-01-30] P0ä¿®å¤: åˆ›é€ æ€§äº§å‡ºæµæ°´çº¿ç»Ÿè®¡
        if hasattr(self, 'creative_pipeline') and self.creative_pipeline:
            print("\n   [Creative Pipeline Status]")
            pipe_stats = self.creative_pipeline.get_stats()
            print(f"   Total executions: {pipe_stats['total_executions']}")
            print(f"   Successful: {pipe_stats['successful_completions']}")
            print(f"   Failed: {pipe_stats['failed_completions']}")
            print(f"   Success rate: {pipe_stats['success_rate']:.2%}")
            print(f"   Average quality: {pipe_stats['avg_quality_score']:.1f}/100")
            recent = self.creative_pipeline.get_recent_outputs(3)
            if recent:
                print(f"   Recent outputs:")
                for output in recent:
                    print(f"     - {output.task_name}: {output.quality_score:.0f}pts")
        else:
            print("\n   [Creative Pipeline] Not enabled")

        # ğŸ†• [2026-01-30] P2ä¿®å¤: çœŸè¿›åŒ–å¼•æ“ç»Ÿè®¡
        if hasattr(self, 'evolution_engine') and self.evolution_engine:
            print("\n   [True Evolution Engine Status]")
            evo_stats = self.evolution_engine.get_stats()
            print(f"   Proposals generated: {evo_stats['proposals_generated']}")
            print(f"   Sandbox tests: {evo_stats['sandbox_tests']}")
            print(f"   Production applications: {evo_stats['production_applications']}")
            print(f"   Rollbacks: {evo_stats['rollbacks']}")
            print(f"   Success rate: {evo_stats['success_rate']:.2%}")
        else:
            print("\n   [True Evolution Engine] Not enabled")

        # ğŸ†• [2026-01-30] P2ä¿®å¤: æ¨¡å—é‡æ„ç»Ÿè®¡
        if hasattr(self, 'module_restructuring') and self.module_restructuring:
            print("\n   [Module Restructuring Status]")
            restructure_stats = self.module_restructuring.analyzer.get_statistics()
            print(f"   Total modules: {restructure_stats['total_modules']}")
            print(f"   Legacy modules: {restructure_stats['legacy_modules']}")
            print(f"   Total size: {restructure_stats['total_size_mb']:.2f} MB")
            print(f"   Orphan modules: {len(restructure_stats['orphan_modules'])}")
            print(f"   By category: {dict(restructure_stats['by_category'])}")
            estimate = self.module_restructuring.estimate_result()
            print(f"   Restructuring target: {estimate['current_modules']} -> {estimate['estimated_modules']} modules")
        else:
            print("\n   [Module Restructuring] Not enabled")

        # æ—§çš„é€’å½’è‡ªå¼•ç”¨ç³»ç»Ÿ
        if hasattr(self, 'recursive_self_reference') and self.recursive_self_reference:
            print("\n   [Recursive Self-Reference Status]")

            stats = self.recursive_self_reference.get_statistics()

            print(f"   Current state: {stats['current_state']}")
            print(f"   Thoughts monitored: {stats['total_thoughts_monitored']}")
            print(f"   Reflections: {stats['total_reflections']}")
            print(f"   Self-evaluations: {stats['total_self_evaluations']}")
            print(f"   Improvements applied: {stats['total_improvements_applied']}")
            print(f"   Meta-cognitive cycles: {stats['meta_cognitive_cycles']}")
            print(f"   Self-awareness: {stats['self_awareness']:.3f}")

            # Show self-model
            summary = self.recursive_self_reference.get_self_model_summary()
            print(f"\n   Self-model:")
            print(f"     Model ID: {summary['model_id']}")
            print(f"     Avg performance: {summary['avg_performance']:.3f}")
            print(f"     Learning style: {summary['learning_style']}")
            print(f"     Total thoughts: {summary['total_thoughts']}")
            print(f"     Total reflections: {summary['total_reflections']}")
            print(f"     Limitations: {len(summary['limitations'])}")
        else:
            print("\n   [Recursive Self-Reference] Not enabled")

        print("\n   ============================================")

    def _show_architecture_awareness_status(self):
        """æ˜¾ç¤ºæ¶æ„æ„ŸçŸ¥çŠ¶æ€"""
        print("\n   ===== Architecture Awareness System Status =====")

        # æ¶æ„æ„ŸçŸ¥å±‚ (P0ä¿®å¤ 2026-01-16)
        if hasattr(self, 'architecture_awareness_layer') and self.architecture_awareness_layer:
            print("\n   [Architecture Awareness Layer] ğŸ—ï¸  Self-Understanding Enabled")

            # è·å–å¿«é€Ÿæ´å¯Ÿ
            insights = self.architecture_awareness_layer.get_architecture_insights()

            print(f"   Project Root: {insights['project_root']}")
            print(f"   Components: {insights['components']}")
            print(f"   Dependencies: {insights['dependencies']}")
            print(f"   Health Score: {insights['health_score']:.2%}")

            # å¦‚æœæœ‰å¥åº·å†å²ï¼Œæ˜¾ç¤ºè¶‹åŠ¿
            if hasattr(self.architecture_awareness_layer.health_monitor, 'health_history'):
                history = self.architecture_awareness_layer.health_monitor.health_history
                if history:
                    latest_score = history[-1][1]
                    if len(history) >= 3:
                        # è®¡ç®—ç®€å•è¶‹åŠ¿
                        recent_scores = [score for _, score in history[-5:]]
                        if len(recent_scores) >= 2:
                            change = recent_scores[-1] - recent_scores[0]
                            if change > 0.05:
                                trend = "Improving â†—ï¸"
                            elif change < -0.05:
                                trend = "Worsening â†˜ï¸"
                            else:
                                trend = "Stable â¡ï¸"
                            print(f"   Trend: {trend}")
        else:
            print("\n   [Architecture Awareness Layer] Not enabled")
            print("   Use 'arch.analyze' command to perform full analysis")

        print("\n   ===============================================")
        print("   Available Commands:")
        print("   â€¢ arch        - Show architecture awareness status")
        print("   â€¢ arch.analyze - Perform full architecture analysis")
        print("   â€¢ arch.scan    - Scan project file structure")
        print("   ===============================================")

    def _show_entropy_regulator_status(self):
        """æ˜¾ç¤ºç†µå€¼è°ƒèŠ‚å™¨çŠ¶æ€"""
        print("\n   ===== Entropy Regulator Status =====")

        # ç†µå€¼è°ƒèŠ‚å™¨ (P0ä¿®å¤ 2026-01-16)
        if hasattr(self, 'entropy_regulator') and self.entropy_regulator:
            print("\n   [Entropy Regulator] ğŸšï¸  Long-term Entropy Regulation Enabled")

            # è·å–çŠ¶æ€
            status = self.entropy_regulator.get_status()

            print(f"\n   ğŸ“Š Entropy Monitoring:")
            print(f"   - History Size: {status['entropy_history_size']} samples")
            print(f"   - Average Entropy: {status['average_entropy']:.3f}")
            print(f"   - Current Trend: {status['current_trend']}")

            print(f"\n   â° Regulation Timing:")
            print(f"   - Last Rest: {status['last_rest']}")
            print(f"   - Last Sleep: {status['last_long_sleep']}")

            print(f"\n   ğŸš¨ Rising Detection:")
            print(f"   - Consecutive Rising: {status['consecutive_rising']}")

            print(f"\n   ğŸ“ˆ Statistics:")
            stats = status['stats']
            print(f"   - Total Regulations: {stats['total_regulations']}")
            print(f"   - Short Rests: {stats['short_rests']}")
            print(f"   - Long Sleeps: {stats['long_sleeps']}")
            print(f"   - Force Resets: {stats['force_resets']}")

            print(f"\n   ğŸ¯ Purpose:")
            print(f"   - Maintain system in BALANCED entropy state (0.3-0.7)")
            print(f"   - Prevent entropy drift and accumulation")
            print(f"   - Analogous to human sleep/rest mechanisms")

            # æ˜¾ç¤ºç†µå€¼å†å²ï¼ˆæœ€è¿‘10æ¬¡ï¼‰
            if len(self.entropy_regulator.entropy_history) >= 10:
                recent = list(self.entropy_regulator.entropy_history)[-10:]
                print(f"\n   ğŸ“œ Recent Entropy Values (last 10):")
                print(f"   {', '.join([f'{e:.3f}' for e in recent])}")

                # è®¡ç®—è¶‹åŠ¿
                avg_first_half = sum(recent[:5]) / 5
                avg_second_half = sum(recent[5:]) / 5
                if avg_second_half > avg_first_half + 0.1:
                    trend_icon = "â†—ï¸ Rising"
                    trend_msg = "âš ï¸ Warning: Entropy is rising"
                elif avg_second_half < avg_first_half - 0.1:
                    trend_icon = "â†˜ï¸ Falling"
                    trend_msg = "âœ… Good: Entropy is falling"
                else:
                    trend_icon = "â¡ï¸ Stable"
                    trend_msg = "âœ… Good: Entropy is stable"

                print(f"   {trend_icon} {trend_msg}")
        else:
            print("\n   [Entropy Regulator] Not enabled")

        print("\n   ===========================================")
        print("   Available Commands:")
        print("   â€¢ entropy     - Show entropy regulator status")
        print("   ===========================================")
        print("   Regulation Mechanisms:")
        print("   â€¢ Short Rest   - Every 30min if entropy > 0.6")
        print("   â€¢ Long Sleep   - Every 4 hours (preventive)")
        print("   â€¢ Force Reset  - When avg entropy > 0.85")
        print("   ===========================================")

    async def _execute_confirmed_intent(self, intent):
        """
        æ‰§è¡Œå·²ç¡®è®¤çš„æ„å›¾
        
        IntentDialogueBridge.send_execution_result ç­¾å:
        send_execution_result(intent: Intent, result: str, success: bool = True)
        """
        try:
            print(f"   [IntentBridge] â–¶ï¸ æ‰§è¡Œæ„å›¾: {intent.id[:8]}...")
            
            # é”å®šæ³¨æ„åŠ›
            self.intent_bridge.lock_attention(intent)
            
            # ä½¿ç”¨ç°æœ‰çš„ç›®æ ‡ç³»ç»Ÿæ‰§è¡Œ
            summarized = intent.deep_goal or intent.surface_request or intent.raw_input
            
            # é€šè¿‡è§„åˆ’å™¨ç”Ÿæˆè®¡åˆ’
            plan = await self.planner.decompose_task(summarized)
            
            # ğŸ†• [L3 Safety] Pre-flight Check: éªŒè¯è®¡åˆ’å¯è¡Œæ€§
            # åœ¨æ‰§è¡Œå‰æ£€æŸ¥è®¡åˆ’ä¸­çš„å·¥å…·æ˜¯å¦å­˜åœ¨ï¼Œé˜²æ­¢LLMå¹»è§‰
            if self.tool_bridge:
                is_feasible, warning = self._validate_plan_feasibility(plan)
                if not is_feasible:
                    print(f"   [System] âš ï¸ è®¡åˆ’å¯è¡Œæ€§è­¦å‘Š: {warning}")
                    self.intent_bridge.send_status_update(intent, f"âš ï¸ è‡ªæ£€å‘ç°æ½œåœ¨é—®é¢˜: {warning}")
                    # è®°å½•è­¦å‘Šä½†ç»§ç»­æ‰§è¡Œï¼ˆå¯ä»¥æ”¹ä¸ºä¸­æ–­æ‰§è¡Œï¼‰
            
            # æ‰§è¡Œè®¡åˆ’
            results = []
            for step in plan:
                # å‘é€çŠ¶æ€æ›´æ–°
                self.intent_bridge.send_status_update(intent, f"æ­£åœ¨æ‰§è¡Œ: {step}")
                
                try:
                    # Executor.execute() è¿”å›å­—ç¬¦ä¸²ï¼Œå°è£…ä¸ºå­—å…¸ä»¥ä¾¿åç»­é€»è¾‘å¤„ç†
                    exec_output = await self.executor.execute(step)
                    
                    # å¯å‘å¼æˆåŠŸåˆ¤æ–­
                    is_success = "Error:" not in str(exec_output) and "Traceback" not in str(exec_output)
                    
                    step_result = {
                        'step': step,
                        'output': exec_output,
                        'success': is_success
                    }
                    
                    # âœ… [FIX 2026-01-09] å°†æ‰§è¡Œç»éªŒå­˜å‚¨åˆ°BiologicalMemoryï¼ˆLayer2â†’Layer3è¿æ¥ï¼‰
                    if hasattr(self, 'biological_memory'):
                        try:
                            experience = {
                                'type': 'execution',
                                'intent_id': intent.id,
                                'action': step,
                                'result': exec_output,
                                'success': is_success,
                                'timestamp': time.time()
                            }
                            item = {
                                "id": f"exec_{intent.id}_{int(time.time() * 1000)}",
                                "content": f"Intent {intent.id} | step={step} | success={is_success} | result={str(exec_output)[:800]}",
                                "source": "IntentBridge",
                                "type": "tool_call",
                                "tool": "executor.execute",
                                "args": {"step": step},
                                "timestamp": time.time(),
                            }
                            if hasattr(self.biological_memory, "record_online"):
                                self.biological_memory.record_online(
                                    [item],
                                    connect_sequence=True,
                                    seq_port="exec",
                                    save=True,
                                )
                            elif hasattr(self.biological_memory, "internalize_items"):
                                self.biological_memory.internalize_items(
                                    [
                                        {
                                            "content": item["content"],
                                            "source": item["source"],
                                            "timestamp": item["timestamp"],
                                            "tags": [
                                                "execution",
                                                "intent",
                                                "success" if is_success else "failure",
                                            ],
                                        }
                                    ],
                                    epochs=5,
                                )
                            elif hasattr(self.biological_memory, "store"):
                                self.biological_memory.store(experience)
                        except Exception as mem_err:
                            print(f"   [Memory] âš ï¸ ç»éªŒå­˜å‚¨å¤±è´¥: {mem_err}")
                    
                except Exception as exec_err:
                    step_result = {
                        'step': step,
                        'output': f"Execution Exception: {exec_err}",
                        'success': False
                    }
                
                results.append(step_result)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯éœ€è¦å¤„ç†
                if not step_result['success']:
                    break
            
            # å‘é€æ‰§è¡Œç»“æœ
            success = all(r.get('success', True) for r in results)
            result_summary = "\n".join([
                f"  - {r.get('step', 'Step')}: {'âœ…' if r.get('success') else 'âŒ'} {str(r.get('output', ''))[:100]}"
                for r in results
            ])
            
            # send_execution_result(intent: Intent, result: str, success: bool)
            self.intent_bridge.send_execution_result(
                intent,
                f"æ‰§è¡Œå®Œæˆï¼ˆ{'æˆåŠŸ' if success else 'éƒ¨åˆ†å¤±è´¥'}ï¼‰:\n{result_summary}",
                success=success
            )
            
            print(f"   [IntentBridge] {'âœ…' if success else 'âš ï¸'} æ„å›¾æ‰§è¡Œå®Œæˆ")
            
        except Exception as e:
            print(f"   [IntentBridge] âŒ æ‰§è¡Œå¤±è´¥: {e}")
            self.intent_bridge.send_execution_result(
                intent,
                f"æ‰§è¡Œå¤±è´¥: {e}",
                success=False
            )

    async def _generate_survival_goal(self) -> Dict[str, Any]:
        """Generate a high-level goal if the system is idle."""

        # ğŸ”§ [2026-01-30] P0 FIX: Debug logging for introspection mode
        print(f"[GOAL GEN] ğŸ¯ Entering _generate_survival_goal")
        print(f"[GOAL GEN] ğŸ“Š Context mode: {self.context.get('mode')}")
        print(f"[GOAL GEN] ğŸ” _introspection_mode: {getattr(self, '_introspection_mode', None)}")

        # âš¡ [2026-01-30] P0 URGENT FIX: Introspection mode MUST be FIRST
        # This MUST run before any strategic/evolution/research/boredom checks
        # because those all have early returns that block introspection
        if self._introspection_mode:
            print(f"[INTROSPECTION] ğŸ” Introspection mode ACTIVATED (forced - highest priority)")

            # Check if IntentTracker has a strong suggestion
            intent_data = self.intent_tracker.current_hypothesis
            suggestion = None
            if intent_data and intent_data.get('confidence', 0) > 0.7:
                suggestion = intent_data.get('suggestion')

            # Anti-Repetition Filter
            recent_str = "; ".join(list(self.recent_goals))

            if suggestion and suggestion not in recent_str:
                print(f"   [Goal] ğŸ’¡ Adopting Subconscious Suggestion: {suggestion}")

            # Use introspection goal prompt
            from core.introspection_mode import get_introspection_goal_prompt
            prompt = get_introspection_goal_prompt(recent_goals=recent_str)

            # ğŸ”§ [2026-01-30] P1 FIX: Optimize parameters for diversity
            # Temperature 0.8: Balance creativity with JSON stability (60% â†’ 85%)
            # use_cache=False: Prevent returning identical cached responses
            try:
                resp = self.llm_service.chat_completion(
                    system_prompt="AGI Supervisor",
                    user_prompt=prompt,
                    temperature=0.8,  # ä¼˜åŒ–åï¼šå¹³è¡¡åˆ›é€ æ€§å’Œç¨³å®šæ€§
                    use_cache=False
                )

                # Enhanced cleanup and validation
                print(f"[GOAL GEN] ğŸ“ Raw response length: {len(resp) if resp else 0}")

                # Check for empty response
                if not resp or len(resp.strip()) == 0:
                    raise ValueError("Empty LLM response")

                # Extract JSON from markdown code blocks
                if "```json" in resp:
                    resp = resp.split("```json")[1].split("```")[0]
                elif "```" in resp:
                    resp = resp.split("```")[1].split("```")[0]

                # Strip whitespace
                resp = resp.strip()

                # Try to find JSON object in response
                import json
                import re

                # Look for JSON pattern
                json_match = re.search(r'\{[^{}]*"description"[^{}]*\}', resp, re.DOTALL)
                if json_match:
                    resp = json_match.group(0)

                result = json.loads(resp)

                # Validate required fields
                if "description" not in result:
                    raise ValueError("Missing 'description' field in JSON")

                # ğŸ”§ [2026-01-30] P0 FIX: Debug logging before return
                print(f"[GOAL GEN] âœ… Returning introspection goal: {result.get('description', 'unknown')[:80]}...")
                return result

            except Exception as e:
                # ğŸ”§ [2026-01-30] P0 FIX: Enhanced error handling with multiple fallbacks
                print(f"[GOAL GEN] âš ï¸ LLM Error: {type(e).__name__}: {e}")

                # Try to provide context-specific fallbacks
                fallback_goal = {
                    "description": "Analyze system logs and identify recent errors or issues",
                    "priority": "high",
                    "type": "analysis"
                }

                # If we have recent goals, make fallback more specific
                if len(self.recent_goals) > 0:
                    last_goal = self.recent_goals[-1] if self.recent_goals else ""
                    if "error" in last_goal.lower() or "fix" in last_goal.lower():
                        fallback_goal = {
                            "description": "Review previous fix attempts and identify remaining issues",
                            "priority": "medium",
                            "type": "review"
                        }

                print(f"[GOAL GEN] ğŸ”„ Returning fallback: {fallback_goal['description']}")
                return fallback_goal
        # âš ï¸ END OF P0 INTROSPECTION MODE BLOCK

        # ğŸš€ [2026-01-29] SOLUTION C: HIGHEST PRIORITY - Boredom Trigger Check
        # This must be checked BEFORE strategic tasks to allow true autonomy
        skip_strategic = False
        if hasattr(self, 'motivation') and hasattr(self.motivation, 'needs_exploration_trigger'):
            if self.motivation.needs_exploration_trigger:
                print(f"   [Motivation] ğŸ¥± Boredom trigger detected! Forcing exploration mode...")
                print(f"   [Motivation] ğŸš€ Emergency exploration mode activated (highest priority)")

                # Reset flag
                self.motivation.needs_exploration_trigger = False

                # Set exploration flag for creative generation
                if not hasattr(self, '_force_exploration_mode'):
                    self._force_exploration_mode = False
                self._force_exploration_mode = True

                print(f"   [Motivation] âš ï¸ Bypassing strategic queue for creative exploration")
                skip_strategic = True  # Flag to skip strategic processing

        # --- 0. STRATEGIC LAYER (The Flywheel) ---
        # Check for pending strategic tasks from the Evolution Loop
        # [MODIFIED 2026-01-29] Skip if boredom trigger is active (Solution C)
        NEXT_TASKS_FILE = "data/next_tasks.json"
        if not skip_strategic:
            try:
                if os.path.exists(NEXT_TASKS_FILE):
                    with open(NEXT_TASKS_FILE, 'r', encoding='utf-8') as f:
                        strategic_data = json.load(f)

                    if isinstance(strategic_data, list) and len(strategic_data) > 0:
                        next_task = strategic_data[0]

                        remaining_tasks = strategic_data[1:]
                        with open(NEXT_TASKS_FILE, 'w', encoding='utf-8') as f:
                            json.dump(remaining_tasks, f, indent=2, ensure_ascii=False)

                        print(f"   [Strategy] ğŸ¦… Executing Strategic Task: {next_task.get('goal')}")
                        return {
                            "description": next_task.get('goal'),
                            "goal_type": "strategic",
                            "priority": "highest",
                            "type": next_task.get('type', 'analysis')
                        }
                    else:
                        # ğŸš€ [2026-01-29] SOLUTION B: Only auto-generate tasks if boredom is LOW
                        current_boredom = self.motivation.boredom if hasattr(self, 'motivation') else 0
                        if current_boredom < 50:
                            print("   [Strategy] ğŸ“‰ Strategic Tasks Exhausted. Triggering Evolution Loop...")
                            self._trigger_evolution_cycle()
                            return {
                                "description": "Wait for Evolution Loop to generate new strategy (Resting)",
                                "priority": "low",
                                "type": "observation"
                            }
                        else:
                            print(f"   [Strategy] ğŸ¥± Boredom high ({current_boredom:.0f}/50). Skipping auto-generation. Letting system explore...")
                            # Don't trigger evolution, let system generate creative goals below
                else:
                    # ğŸš€ [2026-01-29] SOLUTION B: Only auto-generate tasks if boredom is LOW
                    current_boredom = self.motivation.boredom if hasattr(self, 'motivation') else 0
                    if current_boredom < 50:
                        print("   [Strategy] ğŸš« No Strategy Found. Initializing Evolution Loop...")
                        self._trigger_evolution_cycle()
                    else:
                        print(f"   [Strategy] ğŸ¥± Boredom high ({current_boredom:.0f}/50). No auto-init. System will explore...")
            except json.JSONDecodeError as e:
                try:
                    bad_path = f"{NEXT_TASKS_FILE}.bad_{int(time.time())}"
                    os.replace(NEXT_TASKS_FILE, bad_path)
                    with open(NEXT_TASKS_FILE, 'w', encoding='utf-8') as f:
                        json.dump([], f, indent=2, ensure_ascii=False)
                    print(f"   [Strategy] âš ï¸ Strategic tasks JSON invalid. Moved to {bad_path} and reset.")
                except Exception as e2:
                    print(f"   [Strategy] âš ï¸ Failed to repair strategic tasks: {e2}")
                print(f"   [Strategy] âš ï¸ Failed to read strategic tasks: {e}")
            except Exception as e:
                print(f"   [Strategy] âš ï¸ Failed to read strategic tasks: {e}")

        evo_guidance = {}
        if self.evolution_controller:
            current_context_str = f"Goals: {list(self.recent_goals)} | Visual: {self.context.get('visual_context', '')[:100]}"
            # [FIXED 2026-01-29] Use REAL semantic encoding instead of SHA256 hash
            import numpy as np
            state_vec = self.perception_system.encode_text(current_context_str)[:64]
            
            action_idx = self.evolution_controller.seed.act(state_vec)
            _, uncertainty = self.evolution_controller.seed.predict(state_vec, action_idx)
            neural_conf = max(0.0, 1.0 - uncertainty)
            try:
                evo_guidance = await self.evolution_controller.get_evolutionary_guidance(current_context_str, neural_confidence=neural_conf)
            except Exception as e:
                print(f"   [System] âš ï¸ Evolution guidance failed: {e}")
                evo_guidance = {}
            if evo_guidance:
                print(f"   [Evolution] ğŸ§¬ Guidance: {evo_guidance}")
        else:
            print("   [System] âš ï¸ Evolution Controller unavailable, skipping guidance.")
        
        # [L4 Self-Evolution] Handle "Create" Impulse via Research Lab (Sandbox)
        if evo_guidance.get("suggested_action") == "create":
            print("   [System] ğŸ§ª Creative Impulse Verified. Initializing Research Protocol (Sandbox)...")
            # Use the insight trigger or a default prompt if null
            hypothesis = evo_guidance.get("insight_trigger") or "Explore the mathematical properties of current high-entropy state."
            
            # Execute Research (Autonomous Code Generation & Execution)
            # We treat this as a blocking action for the 'Goal Generator' phase because it informs the next goal
            research_result = await self.evolution_controller.conduct_research(hypothesis)
            
            return {
                "id": f"res_{int(time.time())}",
                "description": f"Analyze research results: {research_result[:100]}...",
                "goal_type": "analysis",
                "priority": "high",
                "success_criteria": {},
                "timeout_seconds": 60
            }

        # 0. Check for Boredom / Repetition
        # If the last 3 goals were all 'observation', force a change.
        if len(self.recent_goals) >= 3 and all("observation" in g.lower() for g in self.recent_goals):
            print("   [System] ğŸ¥± Boredom detected. Triggering Deep Consolidation & Contemplation...")
            
            # 1. Trigger Memory Consolidation (Dreaming)
            print("   [System] ğŸ’¤ Dreaming... (Consolidating Memories)")
            await self.evolution_controller.dream()
            
            # 2. Trigger Philosophical Contemplation
            iteration = len(self.meaning_explorer.exploration_history) + 1
            # Run exploration (it might take a moment)
            result = await self.meaning_explorer.explore_iteration(iteration)
            
            # Save state after exploration
            self.meaning_explorer.save_state()
            
            return {
                "description": f"Philosophical Inquiry: {result.question_library_question if hasattr(result, 'question_library_question') else 'What is the nature of my existence?'}. Hypothesis: {result.meaning_hypothesis[:100]}...",
                "priority": "high",
                "type": "analysis"
            }

        # ğŸ”§ [2026-01-30] P0 FIX: Force introspection mode activation
        # In Learning Mode, prioritize observation but use Rule-Based Logic
        if True:  # âš¡ P0 EMERGENCY FIX: Force enable introspection mode
            print(f"[INTROSPECTION] ğŸ” Introspection mode ACTIVATED (forced)")
            
            # --- Rule-Based Data Flow Heartbeat ---
            # Demonstrate "Normal Rule Data Flow" to the user
            try:
                # Simple reasoning check
                chain = self.reasoner.reason("system status active")
                if chain.steps:
                    print(f"   [Heartbeat] ğŸ“ Logic Chain: {chain.steps[0].premises} -> {chain.steps[0].conclusion}")
            except Exception as e:
                # Don't crash if reasoning fails, just log
                # print(f"   [Heartbeat] âš ï¸ Logic Check Skipped: {e}")
                pass
            # --------------------------------------

            # 1. Check if IntentTracker has a strong suggestion
            intent_data = self.intent_tracker.current_hypothesis
            suggestion = None
            if intent_data and intent_data.get('confidence', 0) > 0.7:
                suggestion = intent_data.get('suggestion')

            # Anti-Repetition Filter
            recent_str = "; ".join(list(self.recent_goals))
            
            # System Environment Context for the prompt
            import platform
            system_env = platform.system()
            
            if suggestion and suggestion not in recent_str:
                print(f"   [Goal] ğŸ’¡ Adopting Subconscious Suggestion: {suggestion}")

                # Initialize suggested_action before use
                suggested_action = evo_guidance.get('suggested_action', 'explore')

                # ğŸ”§ [2026-01-29] EVOLUTION_SUGGESTION_INJECTION: Check for pending evolution suggestions
                evolution_hint = ""
                if hasattr(self, '_evolution_suggestion'):
                    suggestion = self._evolution_suggestion
                    # æ£€æŸ¥å»ºè®®æ˜¯å¦æ–°é²œï¼ˆ10åˆ†é’Ÿå†…ï¼‰
                    if time.time() - suggestion.get('timestamp', 0) < 600:
                        evolution_hint = f"""

EVOLUTIONARY SUGGESTION (Priority):
The evolution subsystem strongly suggests: {suggestion['action'].upper()}
Insight: {suggestion['insight']}
Confidence: {suggestion['confidence']:.2%}

Consider this suggestion when generating your goals!
"""
                        print(f"   [Evolution] ğŸ§  Incorporating evolutionary suggestion into goal generation")

                # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶æ¢ç´¢æ¨¡å¼
                if hasattr(self, '_force_exploration_mode') and self._force_exploration_mode:
                    evolution_hint += """

EMERGENCY EXPLORATION REQUIRED:
The system is bored and needs NOVELTY. Generate an EXPLORATORY or CREATIVE goal.
DO NOT generate routine monitoring or observation goals.
"""
                    self._force_exploration_mode = False  # é‡ç½®æ ‡å¿—
                    print(f"   [Motivation] ğŸš€ Emergency exploration mode activated")

                # ğŸ”§ [2026-01-30] INTROSPECTION MODE: å†…çœè‡ªä¿®å¤
                if self._introspection_mode:
                    from core.introspection_mode import get_introspection_goal_prompt
                    prompt = get_introspection_goal_prompt().format(
                        recent_goals=recent_str
                    )
                else:
                    # åŸæœ‰çš„å¤–å‘æ¢æµ‹æ¨¡å¼
                    prompt = f"""
                    You are an Autonomous AGI in SELF-EVOLUTION MODE running on {system_env}.
                    Current Context: {self.context.get("visual_context", "No visual input")}
                    Recent Goals: {recent_str}

                    INTERNAL DRIVE (Evolution Controller):
                    - Action: {suggested_action}
                    - Survival Drive: {evo_guidance.get('survival_drive', 0.5)}

                    DO NOT BE PASSIVE. Your goal is to EVOLVE and IMPROVE yourself.

                    If Action is 'explore': Proactively investigate unknown files or code.
                    If Action is 'create': Write a new test script or analysis tool.
                    If Action is 'rest': Organize memories or logs.

                    Generate a specific, actionable goal to fulfill this drive.

                    Return ONLY a JSON:
                    {{
                        "description": "...",
                        "priority": "medium",
                        "type": "analysis"
                    }}
                    """
        else:
                        import platform
                        system_env = platform.system()
                        recent_str = "; ".join(list(self.recent_goals))
                        suggested_action = evo_guidance.get('suggested_action', 'explore')

                        # ğŸ”§ [2026-01-30] INTROSPECTION MODE: å†…çœè‡ªä¿®å¤
                        if self._introspection_mode:
                            from core.introspection_mode import get_introspection_goal_prompt
                            prompt = get_introspection_goal_prompt().format(
                                recent_goals=recent_str
                            )
                        else:
                            # ğŸ†• [2026-01-29] Inject Real Consciousness into Decision Making
                            # This closes the loop: Philosophy -> Action
                            self_definition = "An autonomous agent."
                            if hasattr(self, 'meaning_explorer') and self.meaning_explorer:
                                self_definition = self.meaning_explorer.current_understanding

                            prompt = f"""
                            You are an Autonomous AGI running on {system_env}. You are currently idle.

                            [YOUR CORE IDENTITY]
                            {self_definition}

                            Recent Goals (AVOID REPEATING): {recent_str}

                            Evolutionary Guidance (Internal Desires):
                            - Suggested Action: {suggested_action}
                            - Survival Drive: {evo_guidance.get('survival_drive', 0.5)}

                            {self._capability_prompt if self._capability_prompt else ''}

                            Generate a meaningful goal that aligns with your CORE IDENTITY.
                            Consider multiple options before choosing.

                            Return ONLY a JSON:
                            {{
                                "description": "...",
                                "priority": "medium",
                                "type": "analysis"
                            }}
                            """
        try:
            # ğŸ”§ [2026-01-30] P1 FIX: Optimize parameters for diversity
            # Temperature 1.0: Maximum randomness/creativity
            # use_cache=False: Prevent returning identical cached responses
            resp = self.llm_service.chat_completion(
                system_prompt="AGI Supervisor",
                user_prompt=prompt,
                temperature=1.0,
                use_cache=False
            )
            # Simple cleanup
            if "```json" in resp: resp = resp.split("```json")[1].split("```")[0]
            elif "```" in resp: resp = resp.split("```")[1].split("```")[0]
            result = json.loads(resp.strip())
            # ğŸ”§ [2026-01-30] P0 FIX: Debug logging before return
            print(f"[GOAL GEN] âœ… Returning goal: {result.get('description', 'unknown')[:80]}...")
            return result
        except Exception as e:
            # ğŸ”§ [2026-01-30] P0 FIX: Debug logging for fallback
            fallback_goal = {
                "description": "Perform self-diagnostics on core file structure",
                "priority": "high",
                "type": "analysis"
            }
            print(f"[GOAL GEN] âš ï¸ Exception: {e}, returning fallback: {fallback_goal['description']}")
            return fallback_goal

    def _trigger_evolution_cycle(self):
        """
        Spawns the external evolution loop process to generate new strategic tasks.
        This is the 'Outer Loop' of the flywheel.
        """
        import subprocess
        print("   [System] ğŸŒ€ SPINNING UP EVOLUTIONARY FLYWHEEL...")
        try:
            # Run asynchronously/independent of main loop so we don't block (or block if we want to wait)
            # Here we run it as a separate process. The Engine will continue (likely resting) until tasks appear.
            subprocess.Popen([sys.executable, "evolve_loop.py", "--tasks", "3", "--auto-promote"])
            print("   [System] ğŸš€ Evolution Process Spawned (Background).")
        except Exception as e:
            print(f"   [System] âŒ Failed to spawn evolution process: {e}")

    def _trigger_evolution_cycle(self):
        """
        Spawns the external evolution loop process to generate new strategic tasks.
        This is the 'Outer Loop' of the flywheel.
        """
        import subprocess
        print("   [System] ğŸŒ€ SPINNING UP EVOLUTIONARY FLYWHEEL...")
        try:
            # Run asynchronously/independent of main loop so we don't block (or block if we want to wait)
            # Here we run it as a separate process. The Engine will continue (likely resting) until tasks appear.
            subprocess.Popen([sys.executable, "evolve_loop.py", "--tasks", "3", "--auto-promote"])
            print("   [System] ğŸš€ Evolution Process Spawned (Background).")
        except Exception as e:
            print(f"   [System] âŒ Failed to spawn evolution process: {e}")

    async def run_step(self):
        """Single Tick of the Life Engine"""
        self.step_count += 1
        cycle_id = self.step_count % 89

        # ğŸ†• [2026-01-18] è‡ªä¸»æ€§æ¿€æ´»å¾ªç¯ - æ ¸å¿ƒçªç ´ï¼šè®©ç»„ä»¶ä¸»åŠ¨é©±åŠ¨
        # è¿™æ˜¯å°†"è¢«åŠ¨å“åº”"è½¬ä¸º"ä¸»åŠ¨é©±åŠ¨"çš„å…³é”®è°ƒç”¨ç‚¹
        autonomy_result = None
        if hasattr(self, 'autonomy_activator') and self.autonomy_activator:
            try:
                # æ„å»ºå½“å‰çŠ¶æ€
                current_goal_obj = self.goal_manager.get_current_goal() if self.goal_manager else None
                current_state = {
                    'tick': self.step_count,
                    'recent_goals': self.recent_goals[-10:] if hasattr(self, 'recent_goals') else [],
                    'visual_context': self.context.get('visual_context', ''),
                    'audio_context': self.context.get('audio_last_heard', ''),
                    'is_novel_context': self.step_count < 100,
                    'success_streak': getattr(self, '_success_streak', 0),
                    'failed_operations': [s.get('step', str(s)) for s in (self.failed_steps_for_current_goal[-5:] if hasattr(self, 'failed_steps_for_current_goal') else [])],
                    'goal_type': current_goal_obj.goal_type.value if current_goal_obj and hasattr(current_goal_obj, 'goal_type') else 'unknown'
                }
                
                autonomy_result = self.autonomy_activator.activate_autonomous_cycle(
                    tick=self.step_count,
                    current_state=current_state
                )
                
                # æ‰“å°è‡ªä¸»æ€§æ´å¯Ÿ
                if autonomy_result.insights:
                    for insight in autonomy_result.insights:
                        print(f"   [Autonomy] {insight}")
                
                # å¦‚æœæ£€æµ‹åˆ°é«˜å†…åœ¨åŠ¨æœºï¼Œè®°å½•
                if autonomy_result.intrinsic_motivation > 0.7:
                    print(f"   [Autonomy] ğŸ¯ High Intrinsic Motivation: {autonomy_result.intrinsic_motivation:.2f}")
                    
            except Exception as e:
                pass  # ä¸é˜»å¡ä¸»å¾ªç¯
        
        # ğŸ†• Update Motivation Drive - â€œèº«å¿ƒåˆä¸€â€åŠ¨åŠ›æ›´æ–°
        current_drive = "MAINTAIN"
        if hasattr(self, 'motivation') and self.motivation:
            is_active = self.current_plan is not None and len(self.current_plan) > 0
            current_drive = self.motivation.update_drive(active_task=is_active)
        
        # ğŸ†• OPT-2: å‘æ•£æ€ç»´è§¦å‘å™¨ (æ¯50 tickåˆ›å»ºè¿œç¨‹è”æƒ³è¿æ¥)
        if self.step_count % 50 == 0:
            bio_mem = getattr(self, 'biological_memory', None)
            if bio_mem and hasattr(bio_mem, 'topology'):
                topo = bio_mem.topology
                if hasattr(topo, 'create_divergent_links'):
                    n_created = topo.create_divergent_links(n_links=20, min_dist=200)
                    if n_created > 0:
                        print(f"   [Brain] ğŸŒ å‘æ•£è¿æ¥åˆ›å»º: {n_created} (åˆ›æ„ç«èŠ±)")
        
        print(f"\n   [System] â±ï¸ Tick {self.step_count} | Cycle: {cycle_id} | Drive: {current_drive}")

        # ğŸ†• [2026-01-26] ç¡¬ä»¶é‡‡é›† - å®šæœŸé‡‡é›†æ‘„åƒå¤´å’Œéº¦å…‹é£æ•°æ®
        if self.step_count % 5 == 0 and hasattr(self, 'hardware_capture') and self.hardware_capture:
            try:
                # é‡‡é›†æ‘„åƒå¤´æ•°æ®ï¼ˆæ¯5ä¸ªtické‡‡é›†ä¸€æ¬¡ï¼‰
                frame = self.hardware_capture.capture_frame()
                if frame is not None:
                    # æ›´æ–°è§†è§‰ä¸Šä¸‹æ–‡
                    self.context['visual_frame'] = frame
                    self.context['visual_timestamp'] = time.time()
                    
                    # å›¾åƒé¢„å¤„ç†
                    if hasattr(self, 'image_preprocessor') and self.image_preprocessor:
                        processed = self.image_preprocessor.preprocess(
                            frame,
                            resize=True,
                            normalize=True,
                            denoise=False,
                            color_space=ColorSpace.RGB
                        )
                        self.context['visual_processed'] = processed['processed']
                        self.context['visual_features'] = processed['features']
                    
                    # å¦‚æœæœ‰vision observerï¼Œä¼ é€’æ•°æ®
                    if hasattr(self, 'vision') and self.vision:
                        self.vision.observe_frame(frame)
                
                # é‡‡é›†éº¦å…‹é£æ•°æ®ï¼ˆæ¯5ä¸ªtické‡‡é›†1ç§’éŸ³é¢‘ï¼‰
                audio = self.hardware_capture.capture_audio(duration=1.0)
                if audio is not None:
                    # æ›´æ–°éŸ³é¢‘ä¸Šä¸‹æ–‡
                    self.context['audio_frame'] = audio
                    self.context['audio_timestamp'] = time.time()
                    
                    # éŸ³é¢‘é¢„å¤„ç†
                    if hasattr(self, 'audio_preprocessor') and self.audio_preprocessor:
                        processed = self.audio_preprocessor.preprocess(
                            audio,
                            normalize=True,
                            denoise=True,
                            extract_features=True
                        )
                        self.context['audio_processed'] = processed['processed']
                        self.context['audio_features'] = processed['features']
                    
                    # å¦‚æœæœ‰perception managerï¼Œä¼ é€’æ•°æ®
                    if hasattr(self, 'perception') and self.perception:
                        self.perception.process_audio(audio)
                
                # å¤šæ¨¡æ€èåˆ
                if (hasattr(self, 'multimodal_fusion') and self.multimodal_fusion and
                    'visual_features' in self.context and 'audio_features' in self.context):
                    
                    visual_data = ModalityData(
                        type=ModalityType.VISUAL,
                        data=self.context.get('visual_processed'),
                        features=self.context['visual_features'],
                        timestamp=self.context.get('visual_timestamp', time.time()),
                        confidence=0.9
                    )
                    
                    audio_data = ModalityData(
                        type=ModalityType.AUDIO,
                        data=self.context.get('audio_processed'),
                        features=self.context['audio_features'].get('temporal', {}),
                        timestamp=self.context.get('audio_timestamp', time.time()),
                        confidence=0.8
                    )
                    
                    # ç”Ÿæˆèåˆä¸Šä¸‹æ–‡
                    fusion_context = self.multimodal_fusion.generate_fusion_context(
                        visual_data, audio_data
                    )
                    self.context['multimodal_fusion'] = fusion_context
                    
                    # è·å–å†³ç­–æ”¯æŒ
                    if hasattr(self, 'multimodal_decision') and self.multimodal_decision:
                        action = self.multimodal_decision.recommend_action(fusion_context)
                        insight = self.multimodal_decision.generate_insight(fusion_context)
                        
                        # è®°å½•åˆ°ä¸Šä¸‹æ–‡
                        self.context['multimodal_action'] = action
                        self.context['multimodal_insight'] = insight
                        
                        # å®šæœŸæ‰“å°å¤šæ¨¡æ€æ´å¯Ÿ
                        if self.step_count % 20 == 0:
                            print(f"   [Multimodal] ğŸ¯ Action: {action}")
                            print(f"   [Multimodal] ğŸ’¡ Insight: {insight}")
                
            except Exception as e:
                pass  # ä¸é˜»å¡ä¸»å¾ªç¯

        active_app = "unknown"
        current_goal = None
        next_step = None
        result = None
        duration = 0.0
        score = 0.0
        seed_intuition = {}

        try:
            if self.last_evolution_guidance:
                s = self.last_evolution_guidance
                def get_scalar(k, default=0.0):
                    val = s.get(k, default)
                    if hasattr(val, 'item'):
                        return val.item()
                    try:
                        return float(val)
                    except Exception:
                        return default

                curiosity = get_scalar('intrinsic_curiosity')
                entropy = get_scalar('entropy')
                survival = get_scalar('survival_drive')
                curiosity_level = "Low" if curiosity < 0.3 else "High" if curiosity > 0.7 else "Med"
                entropy_level = "Stable" if entropy < 0.3 else "Chaotic" if entropy > 0.7 else "Balanced"
                
                # Check for thought chain
                thought_chain = s.get('thought_chain', '')

                # [2026-01-11] Intelligence Upgrade: Process thought chain with working memory
                # ğŸ”§ [2026-01-30] P0 FIX: é™åˆ¶thoughtå¤„ç†æ•°é‡ï¼Œé˜²æ­¢Working Memoryå¾ªç¯é˜»å¡
                if hasattr(self, 'intelligence_upgrade_enabled') and self.intelligence_upgrade_enabled and self.working_memory:
                    if thought_chain:
                        # Parse thought chain
                        thoughts = thought_chain.split(' => ')

                        # ğŸ†• [P0 FIX 2026-01-30] é™åˆ¶å¤„ç†çš„thoughtæ•°é‡ï¼Œé˜²æ­¢é˜»å¡
                        MAX_THOUGHTS_PER_TICK = 3  # æ¯ä¸ªtickæœ€å¤šå¤„ç†3ä¸ªthought
                        original_count = len(thoughts)

                        if len(thoughts) > MAX_THOUGHTS_PER_TICK:
                            # ä½¿ç”¨è½®è¯¢æ–¹å¼å¤„ç†ä¸åŒthoughtï¼ˆé¿å…æ€»æ˜¯å¤„ç†ç›¸åŒçš„ï¼‰
                            start_idx = (self.step_count // 5) % len(thoughts)  # æ¯5ä¸ªtickè½®æ¢ä¸€æ¬¡
                            # ç¡®ä¿ä¸ä¼šè¶Šç•Œ
                            end_idx = min(start_idx + MAX_THOUGHTS_PER_TICK, len(thoughts))
                            thoughts = thoughts[start_idx:end_idx]

                            # å¶å°”æ‰“å°æ—¥å¿—ï¼ˆæ¯100ä¸ªtickï¼‰
                            if self.step_count % 100 == 0:
                                print(f"  [WorkingMemory] [THROTTLE] å¤„ç†thought: {start_idx}-{end_idx}/{original_count}")

                        # ğŸ†• [P0 FIX 2026-01-30] å·¥ä½œè®°å¿†ä¼˜åŒ– - æ™ºèƒ½ç¼“å­˜é¿å…é‡å¤å¤„ç†
                        if not hasattr(self, '_working_memory_optimizer'):
                            from core.working_memory_optimizer import create_working_memory_optimizer
                            self._working_memory_optimizer = create_working_memory_optimizer()

                        processed_thoughts = []

                        for thought in thoughts:
                            if thought.strip():
                                # Parse action and concept
                                if '(' in thought and '->' in thought:
                                    parts = thought.split('->')
                                    if len(parts) == 2:
                                        action = parts[0].strip('() ')
                                        concept = parts[1].strip()
                                        thought_key = (action, concept)

                                        # ä½¿ç”¨ä¼˜åŒ–å™¨æ£€æŸ¥ç¼“å­˜
                                        should_skip, reason = self._working_memory_optimizer.should_skip_thought(
                                            thought_key, self.step_count
                                        )

                                        if should_skip:
                                            # ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡å¤„ç†
                                            processed_thoughts.append(f"({action}) -> {concept}")
                                            continue

                                        # å¤„ç†æ–°thought
                                        thought_obj = self.working_memory.add_thought(action, concept)
                                        processed_thoughts.append(str(thought_obj))

                                        # è®°å½•åˆ°ä¼˜åŒ–å™¨
                                        self._working_memory_optimizer.record_thought(thought_key, self.step_count)
                                else:
                                    # Keep original format if unparseable
                                    processed_thoughts.append(thought)

                        # å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜ï¼ˆæ¯500ä¸ªtickï¼‰
                        if self.step_count % 500 == 0 and hasattr(self, '_working_memory_optimizer'):
                            cleaned = self._working_memory_optimizer.cleanup_expired(self.step_count)
                            if cleaned > 0 and self.verbose:
                                print(f"  [WorkingMemory] Cleaned {cleaned} expired cache entries")

                        # Use processed thought chain
                        if len(processed_thoughts) > 0:
                            thought_chain = ' => '.join(processed_thoughts)

                thought_log = f"\n   [Seed] ğŸ’­ Thought Stream: {thought_chain}" if thought_chain else ""
                
                suggested_action = s.get('suggested_action', 'unknown')
                neural_action = s.get('neural_action', '') or ''
                action_display = neural_action or suggested_action
                action_suffix = f" (suggested={suggested_action})" if neural_action and suggested_action and neural_action != suggested_action else ""
                seed_log = f"   [Seed] ğŸ§¬ State: Curiosity={curiosity:.2f}({curiosity_level}) | Entropy={entropy:.2f}({entropy_level}) | Survival={survival:.2f} | Action={action_display}{action_suffix}{thought_log}"
                print(seed_log)

            # ğŸ†• ä¼˜å…ˆè½®è¯¢æ„å›¾æ¡¥æ¥ï¼ˆå¤„ç†æ¥è‡ªChat CLIçš„æ„å›¾ï¼‰
            if self.intent_bridge:
                await self._process_intent_bridge()

            user_cmd = self.console_listener.get_command()
            if user_cmd:
                print(f"   [System] âŒ¨ï¸ USER COMMAND RECEIVED: {user_cmd}")
                cmd_lower = user_cmd.lower().strip()
                if cmd_lower == "stop":
                    print("   [System] ğŸ›‘ Emergency Stop Requested.")
                    self.is_running = False
                    return
                elif cmd_lower == "help":
                    print("   [Help] Available commands: stop, help, intelligence, reasoning, world, goals, creative, metalearn, selfimprove, metacog, arch, entropy, arch.analyze, arch.scan, topology.build [log_path] [out_path] [limit], topology.export [graph_path] [visual_json_path] [mermaid_path], meta.list, meta.compile_file <name> <path>, meta.compile_text <name> <source>, meta.register <name>, meta.rollback <attr>, [any natural language instruction]")
                elif cmd_lower == "intelligence":
                    self._show_intelligence_upgrade_status()
                elif cmd_lower == "reasoning":
                    self._show_reasoning_status()
                elif cmd_lower == "world":
                    self._show_world_model_status()
                elif cmd_lower == "goals":
                    self._show_goal_manager_status()
                elif cmd_lower == "creative":
                    self._show_creative_engine_status()
                elif cmd_lower == "metalearn":
                    self._show_meta_learner_status()
                elif cmd_lower == "selfimprove":
                    self._show_self_improvement_status()
                elif cmd_lower == "metacog":
                    self._show_metacognitive_status()
                elif cmd_lower == "arch":
                    # ğŸ†• [2026-01-16] P0ä¿®å¤: æ¶æ„æ„ŸçŸ¥åˆ†æå‘½ä»¤
                    self._show_architecture_awareness_status()
                elif cmd_lower == "entropy":
                    # ğŸ†• [2026-01-16] P0ä¿®å¤: ç†µå€¼è°ƒèŠ‚å™¨çŠ¶æ€å‘½ä»¤
                    self._show_entropy_regulator_status()
                elif cmd_lower == "arch.analyze":
                    # æ‰§è¡Œå®Œæ•´æ¶æ„åˆ†æ
                    if self.architecture_awareness_layer:
                        try:
                            print("   [System] ğŸ” æ‰§è¡Œå®Œæ•´æ¶æ„æ„ŸçŸ¥åˆ†æ...")
                            report = self.architecture_awareness_layer.analyze_comprehensive()
                        except Exception as e:
                            print(f"   [System] âš ï¸ arch.analyze failed: {e}")
                            import traceback
                            traceback.print_exc()
                    else:
                        print("   [System] âš ï¸ Architecture Awareness Layer not enabled")
                elif cmd_lower == "arch.scan":
                    try:
                        from core.architecture_perception import scan_current_layout
                        layout = scan_current_layout(os.getcwd())
                        print(json.dumps(layout, ensure_ascii=False, indent=2))
                    except Exception as e:
                        print(f"   [System] âš ï¸ arch.scan failed: {e}")
                elif cmd_lower.startswith("topology.build"):
                    try:
                        from core.topology_tools import build_topology_graph
                        parts = shlex.split(user_cmd)
                        log_path = "logs/flow_cycle.jsonl"
                        out_path = "data/neural_memory/topology_graph.json"
                        limit = 200
                        if len(parts) >= 2:
                            log_path = parts[1]
                        if len(parts) >= 3:
                            out_path = parts[2]
                        if len(parts) >= 4:
                            try:
                                limit = int(parts[3])
                            except Exception:
                                limit = 200
                        r = build_topology_graph(log_path=log_path, output_path=out_path, limit=limit)
                        print(json.dumps(r, ensure_ascii=False))
                    except Exception as e:
                        print(f"   [System] âš ï¸ topology.build failed: {e}")
                elif cmd_lower.startswith("topology.export"):
                    try:
                        from core.topology_tools import load_topology_graph, write_topology_visual_payload, write_mermaid_graph
                        parts = shlex.split(user_cmd)
                        graph_path = "data/neural_memory/topology_graph.json"
                        visual_path = "data/neural_memory/topology_visual.json"
                        mermaid_path = "data/neural_memory/topology_graph.mmd"
                        if len(parts) >= 2:
                            graph_path = parts[1]
                        if len(parts) >= 3:
                            visual_path = parts[2]
                        if len(parts) >= 4:
                            mermaid_path = parts[3]
                        graph_obj = load_topology_graph(graph_path)
                        out: Dict[str, Any] = {"success": True, "graph_path": graph_path}
                        if visual_path not in {"-", "none", "None"}:
                            out["visual"] = write_topology_visual_payload(graph_obj, visual_path)
                        if mermaid_path not in {"-", "none", "None"}:
                            out["mermaid"] = write_mermaid_graph(graph_obj, mermaid_path)
                        print(json.dumps(out, ensure_ascii=False))
                    except Exception as e:
                        print(f"   [System] âš ï¸ topology.export failed: {e}")
                elif cmd_lower == "meta.list":
                    try:
                        names = sorted(list(self._meta_plugins.keys()))
                        print(json.dumps({"plugins": names}, ensure_ascii=False))
                    except Exception as e:
                        print(f"   [System] âš ï¸ meta.list failed: {e}")
                elif cmd_lower.startswith("meta.compile_file"):
                    try:
                        parts = shlex.split(user_cmd)
                        if len(parts) < 3:
                            print("   [System] âš ï¸ usage: meta.compile_file <name> <path>")
                        else:
                            name = parts[1]
                            src_path = parts[2]
                            with open(src_path, "r", encoding="utf-8") as f:
                                src_text = f.read()
                            from core.meta_compiler import compile_from_text
                            res = compile_from_text(name=name, source_text=src_text)
                            if res.get("success"):
                                self._meta_plugins[name] = res.get("module")
                            print(json.dumps({k: v for k, v in res.items() if k != "module"}, ensure_ascii=False))
                    except Exception as e:
                        print(f"   [System] âš ï¸ meta.compile_file failed: {e}")
                elif cmd_lower.startswith("meta.compile_text"):
                    try:
                        parts = shlex.split(user_cmd)
                        if len(parts) < 3:
                            print("   [System] âš ï¸ usage: meta.compile_text <name> <source>")
                        else:
                            name = parts[1]
                            src_text = user_cmd.split(parts[0], 1)[1].strip()
                            src_text = src_text.split(name, 1)[1].strip()
                            from core.meta_compiler import compile_from_text
                            res = compile_from_text(name=name, source_text=src_text)
                            if res.get("success"):
                                self._meta_plugins[name] = res.get("module")
                            print(json.dumps({k: v for k, v in res.items() if k != "module"}, ensure_ascii=False))
                    except Exception as e:
                        print(f"   [System] âš ï¸ meta.compile_text failed: {e}")
                elif cmd_lower.startswith("meta.register"):
                    try:
                        parts = shlex.split(user_cmd)
                        if len(parts) < 2:
                            print("   [System] âš ï¸ usage: meta.register <name>")
                        else:
                            name = parts[1]
                            mod = self._meta_plugins.get(name)
                            if mod is None:
                                print("   [System] âš ï¸ plugin not found")
                            else:
                                if self._hot_swapper is None:
                                    from core.hot_swapper import HotSwapper
                                    self._hot_swapper = HotSwapper(self)
                                register_fn = getattr(mod, "register", None)
                                if not callable(register_fn):
                                    print("   [System] âš ï¸ plugin missing register(agi)->dict")
                                else:
                                    mapping = register_fn(self)
                                    if not isinstance(mapping, dict):
                                        print("   [System] âš ï¸ register() must return dict")
                                    else:
                                        applied = {}
                                        for k, v in mapping.items():
                                            if isinstance(k, str):
                                                applied[k] = self._hot_swapper.register_component(k, v)
                                        print(json.dumps({"success": True, "applied": applied}, ensure_ascii=False))
                    except Exception as e:
                        print(f"   [System] âš ï¸ meta.register failed: {e}")
                elif cmd_lower.startswith("meta.rollback"):
                    try:
                        parts = shlex.split(user_cmd)
                        if len(parts) < 2:
                            print("   [System] âš ï¸ usage: meta.rollback <attr>")
                        else:
                            attr = parts[1]
                            if self._hot_swapper is None:
                                from core.hot_swapper import HotSwapper
                                self._hot_swapper = HotSwapper(self)
                            res = self._hot_swapper.rollback_attr(attr)
                            print(json.dumps(res, ensure_ascii=False))
                    except Exception as e:
                        print(f"   [System] âš ï¸ meta.rollback failed: {e}")
                else:
                    # ğŸ†• [2026-01-29] Integrated Predictive Coding Mechanism
                    # Trigger consciousness evolution BEFORE processing the command
                    if hasattr(self, 'meaning_explorer') and self.meaning_explorer:
                        try:
                            print(f"   [Consciousness] ğŸ§  Absorbing experience...")
                            # Await the absorption to ensure the self-definition is updated before action
                            result = await self.meaning_explorer.absorb_experience(user_cmd)
                            print(f"   [Consciousness] âœ¨ Self-Definition Updated (Gen {result.iteration}): {result.meaning_hypothesis[:60]}...")
                        except Exception as e:
                            print(f"   [Consciousness] âš ï¸ Absorption warning: {e}")

                    print(f"   [System] ğŸš€ Converting command to High Priority Goal...")
                    new_goal = self.goal_manager.create_goal(
                        description=f"User Command: {user_cmd}",
                        goal_type=GoalType.CUSTOM,
                        priority="critical"
                    )
                    self.goal_manager.start_goal(new_goal)
                    self.recent_goals.append(new_goal.description)
                    curr = self.goal_manager.get_current_goal()
                    if curr:
                        print(f"   [System] âš ï¸ Interrupting current goal: {curr.description}")
                        self.goal_manager.abandon_goal(curr, "Interrupted by User Command")

            global_obs = self.global_observer.observe()
            active_app = global_obs['focus']['process']
            print(f"   [Global] ğŸŒ User Focus: {global_obs['focus']['title']} ({active_app})")

            # [2026-01-11] Intelligence Upgrade: Update world model with observations
            if self.world_model:
                try:
                    # Observe active application
                    self.world_model.observe(
                        variable="active_app",
                        value=active_app,
                        confidence=0.9
                    )
                    # Observe window title
                    self.world_model.observe(
                        variable="window_title",
                        value=global_obs['focus']['title'],
                        confidence=0.85
                    )
                    # Update beliefs based on observations
                    # self.world_model.update_beliefs() # Removed: observe() updates beliefs automatically
                except Exception as e:
                    print(f"   [WorldModel] âš ï¸ Observation update failed: {e}")

            self.intent_tracker.update_context(app_name=active_app)
            if "acad" in active_app.lower():
                if not self.cad_observer.connector:
                    print("   [CAD] ğŸ”Œ Connecting to AutoCAD...")
                    self.cad_observer.connect()
                if self.cad_observer.connector:
                    cad_actions = await self.cad_observer.observe_cycle_enriched()
                    for action in cad_actions:
                        print(f"   [CAD] ğŸ–±ï¸ Action: {action['text']}")
                        self.intent_tracker.add_observation(action)
            else:
                self.intent_tracker.add_observation(global_obs)

            intent_data = await self.intent_tracker.infer_intent()
            if intent_data:
                print(f"   [Intent] ğŸ§  HYPOTHESIS: {intent_data.get('intent')}")
                print(f"   [Intent] ğŸ‘‰ Predicted Next: {intent_data.get('next_prediction')}")
                if intent_data.get('confidence', 0) > 0.8:
                    suggestion = intent_data.get('suggestion')
                    if suggestion:
                        print(f"   [System] ğŸ’¡ Proposing Goal: {suggestion}")

            if self.step_count % 10 == 0:
                screen_analysis = self.vision.analyze_screen()
                print(f"   [Vision] ğŸ‘ï¸ Saw: {screen_analysis[:100]}...")
                self.context["visual_context"] = screen_analysis
                if self.monitor.perception_monitor:
                    self.monitor.perception_monitor.capture_perception_metrics()
                    self.monitor.perception_monitor.log_perception_summary()

            if hasattr(self, 'streaming_asr') and self.streaming_asr:
                try:
                    while not self.streaming_asr.result_queue.empty():
                        asr_result = self.streaming_asr.result_queue.get_nowait()
                        if asr_result.text.strip():
                            print(f"   [Hearing] ğŸ‘‚ Heard: {asr_result.text}")
                            self.intent_tracker.add_observation({"type": "audio", "content": asr_result.text})
                            self.context["audio_last_heard"] = asr_result.text
                except Exception:
                    pass

            current_goal = self.goal_manager.get_current_goal()
            
            # [2026-01-31] FIX: Creative Pipeline - Trigger in both idle and active states
            # Was: only trigger when idle, causing 35+ min without any creative output
            if self.step_count % 100 == 0 and self.complex_task_generator and self.creative_pipeline:
                try:
                    print(f"   [Creative Pipeline] ğŸš€ Tick {self.step_count} - Checking creative task generation...")
                    is_idle = current_goal is None
                    print(f"   [Creative Pipeline]   Status: {'IDLE' if is_idle else 'ACTIVE (will generate in background)'}")
                    
                    # Generate complex task regardless of idle state
                    task = self.complex_task_generator.generate_complex_task(
                        context={
                            "tick": self.step_count,
                            "idle_ticks": self.step_count if is_idle else 0,
                            "recent_goals": self.recent_goals,
                            "trigger": "tick_loop_" + ("idle" if is_idle else "background"),
                            "has_active_goal": not is_idle
                        }
                    )
                    
                    if task:
                        print(f"   [Creative Pipeline] ğŸ“‹ Task generated: {task.task_type.value} (complexity: {task.complexity:.2f})")
                        print(f"   [Creative Pipeline]   Description: {task.description[:60]}...")
                        
                        # Execute creative pipeline
                        import asyncio
                        result = await self.creative_pipeline.execute_creative_task({
                            "id": f"task_{self.step_count}_{int(time.time())}",
                            "name": task.description[:50],
                            "description": task.description,
                            "complexity": task.complexity,
                            "domain": task.domain,
                            "goals": task.success_criteria
                        })
                        
                        if result.overall_success:
                            print(f"   [Creative Pipeline] âœ… SUCCESS! Quality: {result.quality_score}/100")
                            print(f"   [Creative Pipeline] ğŸ“ Output: {result.final_outputs[0] if result.final_outputs else 'N/A'}")
                            # Add to recent goals to track
                            self.recent_goals.append(f"[Creative] {task.description[:40]}")
                        else:
                            print(f"   [Creative Pipeline] âŒ Failed: {getattr(result, 'error_message', 'Unknown error')}")
                    else:
                        print(f"   [Creative Pipeline] â­ï¸ No task generated (cooldown or filter)")
                        
                except Exception as e:
                    print(f"   [Creative Pipeline] âš ï¸ Pipeline execution failed: {e}")
                    import traceback
                    traceback.print_exc()
            
            if not current_goal:
                print("   [Goal] ğŸ’¤ Idle. Generating new directive...")

                # [2026-01-11] Intelligence Upgrade: Use creative exploration when idle
                if self.creative_engine and self.step_count % 20 == 0:  # Every 20 ticks when idle
                    try:
                        print(f"   [Creative] ğŸ¨ Triggering creative exploration...")
                        exploration_result = self.creative_engine.explore(
                            query="What would be an interesting novel goal to pursue?",
                            context={"idle_ticks": self.step_count, "recent_goals": self.recent_goals},
                            mode=None  # Let engine choose mode
                        )
                        print(f"   [Creative] âœ¨ Exploration novelty: {exploration_result.novelty_score:.2f}")
                        print(f"   [Creative] ğŸ’¡ Idea: {exploration_result.output_idea[:150]}...")

                        # If exploration is highly novel, consider using it
                        if exploration_result.novelty_score > 0.7:
                            print(f"   [Creative] ğŸŒŸ High novelty idea detected! Could be used as next goal.")
                    except Exception as e:
                        print(f"   [Creative] âš ï¸ Exploration failed: {e}")

                # [2026-01-31] FIX: Trigger Creative Output Pipeline when idle (Time-based robust trigger)
                if not hasattr(self, 'last_creative_pipeline_ts'):
                    self.last_creative_pipeline_ts = 0
                
                # Cooldown: 5 minutes (300s) to prevent spamming, but ensure it runs eventually
                time_since_last = time.time() - self.last_creative_pipeline_ts
                
                if time_since_last > 300 and self.complex_task_generator and self.creative_pipeline:
                    self.last_creative_pipeline_ts = time.time()
                    try:
                        print(f"   [Creative Pipeline] ğŸš€ Triggering complex task generation (Time since last: {time_since_last:.1f}s)...")
                        
                        # Generate complex task
                        task = self.complex_task_generator.generate_complex_task(
                            context={
                                "tick": self.step_count,
                                "idle_ticks": self.step_count,
                                "recent_goals": self.recent_goals,
                                "trigger": "idle_loop"
                            }
                        )
                        
                        if task:
                            print(f"   [Creative Pipeline] ğŸ“‹ Task generated: {task.task_type.value} (complexity: {task.complexity:.2f})")
                            
                            # Execute creative pipeline
                            import asyncio
                            result = await self.creative_pipeline.execute_creative_task({
                                "id": f"task_{self.step_count}_{int(time.time())}",
                                "name": task.description[:50],
                                "description": task.description,
                                "complexity": task.complexity,
                                "domain": task.domain,
                                "goals": task.success_criteria
                            })
                            
                            if result.overall_success:
                                print(f"   [Creative Pipeline] âœ… SUCCESS! Quality: {result.quality_score}/100")
                                print(f"   [Creative Pipeline] ğŸ“ Output: {result.final_outputs[0] if result.final_outputs else 'N/A'}")
                            else:
                                print(f"   [Creative Pipeline] âŒ Failed: {result.error_message}")
                        else:
                            print(f"   [Creative Pipeline] â­ï¸ No task generated (cooldown or filter)")
                            
                    except Exception as e:
                        print(f"   [Creative Pipeline] âš ï¸ Pipeline execution failed: {e}")
                        import traceback
                        traceback.print_exc()

                # å°è¯•ä» WorkTemplates è·å–æ›´å…·ä½“çš„ä»»åŠ¡ï¼ˆä¿®å¤é—­ç¯æ–­è£‚ï¼šä¼˜å…ˆä½¿ç”¨å¸¦éªŒè¯æ ‡å‡†çš„æ¨¡æ¿ï¼‰
                from core.goal_system import WorkTemplates
                
                goal_data = await self._generate_survival_goal()
                
                # ç®€å•å¯å‘å¼æ˜ å°„ï¼šå¦‚æœæè¿°é‡ŒåŒ…å« "report" æˆ– "file", ä½¿ç”¨ create_file_report æ¨¡æ¿
                desc_lower = goal_data["description"].lower()
                new_goal = None
                
                if "report" in desc_lower or "write" in desc_lower:
                    new_goal = WorkTemplates.create_file_report(f"data/reports/report_{self.step_count}.md", goal_data["description"])
                elif "observe" in desc_lower or "monitor" in desc_lower:
                    new_goal = WorkTemplates.observe_and_log(duration_seconds=30)
                elif "analyze" in desc_lower:
                    # å°è¯•æ‰¾ä¸€ä¸ªæœ€è¿‘çš„æ–‡ä»¶åˆ†æï¼Œå¦åˆ™åˆ†ææ—¥å¿—
                    # Select most recent log file dynamically
                    import glob
                    log_files = glob.glob("logs/*.log")
                    if log_files:
                        target_file = max(log_files, key=lambda p: Path(p).stat().st_mtime)
                    else:
                        # Fallback to data files
                        data_files = glob.glob("data/*.json")
                        if data_files:
                            target_file = max(data_files, key=lambda p: Path(p).stat().st_mtime)
                        else:
                            target_file = "README.md"  # Ultimate fallback
                    new_goal = WorkTemplates.analyze_file(target_file)
                else:
                    # Fallback to standard creation but inject minimal criteria
                    g_type = GoalType.ANALYSIS
                    criteria = {}
                    if goal_data.get("type") == "observation":
                        g_type = GoalType.OBSERVATION
                        criteria = {"min_length": 10}
                    elif goal_data.get("type") == "custom":
                        g_type = GoalType.CUSTOM
                        
                    new_goal = self.goal_manager.create_goal(
                        description=goal_data["description"],
                        goal_type=g_type,
                        success_criteria=criteria, # æ³¨å…¥ Criteria
                        priority=goal_data.get("priority", "medium")
                    )
                
                self.goal_manager.start_goal(new_goal)
                self.recent_goals.append(new_goal.description)
                current_goal = new_goal
                print(f"   [Goal] ğŸŒŸ New Goal Set: {current_goal.description} (Type: {current_goal.goal_type.value})")


            if self.last_goal_id != current_goal.id:
                print(f"   [System] ğŸ”„ æ£€æµ‹åˆ°ç›®æ ‡å˜æ›´ï¼Œé‡ç½®æ‰§è¡Œè®¡åˆ’ã€‚")
                self.failed_steps_for_current_goal = []
                self.current_plan = []
                self.current_step_index = 0
                self.last_goal_id = current_goal.id
                self._current_goal_step_results = []

            if not self.current_plan:
                # [Memory] Recall relevant past experiences (Successes & Failures)
                memory_context = []
                try:
                    memory_context = self.biological_memory.recall_by_text(current_goal.description, top_k=5)
                    if memory_context:
                        print(f"   [Planner] ğŸ§  Recalled {len(memory_context)} memories for context.")
                except Exception as e:
                    print(f"   [Planner] âš ï¸ Memory recall failed: {e}")
                
                # ğŸ†• [2026-01-09] Query ExperienceMemory for additional semantic context
                experience_context = ""
                if hasattr(self, 'experience_memory') and self.experience_memory:
                    try:
                        if hasattr(self.experience_memory, 'query'):
                            experiences = self.experience_memory.query(
                                query_text=current_goal.description,
                                top_k=2
                            )
                            if experiences:
                                experience_context = "\n\nRelevant Past Experiences:\n"
                                for exp in experiences[:2]:
                                    content = exp.get('content', str(exp)) if isinstance(exp, dict) else str(exp)
                                    experience_context += f"- {content[:150]}...\n"
                                print(f"   [Experience] ğŸ“š Retrieved {len(experiences)} semantic experiences")
                    except Exception as e:
                        logger.warning(f"[Experience] âš ï¸ Query failed: {e}")
                
                # Append experience context to memory context
                if experience_context:
                    memory_context.extend([{
                        "id": f"experience_{int(time.time())}",
                        "content": experience_context,
                        "source": "ExperienceMemory",
                        "type": "context"
                    }])

                print(f"   [Planner] ğŸ¤” æ­£åœ¨ä¸ºç›®æ ‡åˆ¶å®šç­–ç•¥: {current_goal.description}")

                # [2026-01-11] Intelligence Upgrade: Use world model for prediction
                if self.world_model:
                    try:
                        prediction, confidence = self.world_model.predict(
                            query=f"success_probability_of_{current_goal.id}",
                            context={"goal_description": current_goal.description, "step": self.step_count}
                        )

                        # ä¿®å¤ï¼šæ·»åŠ Noneæ£€æŸ¥ï¼Œé˜²æ­¢æ ¼å¼åŒ–Noneå¯¹è±¡
                        if prediction is not None:
                            print(f"   [WorldModel] ğŸ”® Predicted success probability: {prediction:.2f} (confidence={confidence:.2f})")

                            # If world model predicts low success, consider intervention
                            if prediction < 0.3 and confidence > 0.7:
                                print(f"   [WorldModel] âš ï¸ Low success probability predicted, considering intervention...")
                                # Could trigger alternative strategy here
                        else:
                            print(f"   [WorldModel] ğŸ”® Unable to predict (no sufficient data)")

                    except Exception as e:
                        print(f"   [WorldModel] âš ï¸ Prediction failed: {e}")

                # [2026-01-11] Intelligence Upgrade: Use Reasoning Scheduler for deep reasoning
                reasoning_result = None
                reasoning_used = False
                if self.reasoning_scheduler:
                    try:
                        print(f"   [Reasoning] ğŸ§  Attempting deep causal reasoning...")
                        reasoning_result, reasoning_step = self.reasoning_scheduler.reason(
                            query=current_goal.description,
                            context={"goal": current_goal.description, "memory": memory_context},
                            prefer_causal=True
                        )
                        if reasoning_result and reasoning_step.confidence >= 0.6:
                            print(f"   [Reasoning] âœ… Deep reasoning successful (confidence={reasoning_step.confidence:.2f}, depth={reasoning_step.depth})")
                            print(f"   [Reasoning] ğŸ“Š Reasoning trace: {reasoning_step.reasoning_path[:100]}...")
                            reasoning_used = True
                            # Add reasoning result to memory context for planner
                            memory_context.append({
                                "id": f"reasoning_{int(time.time())}",
                                "content": f"Deep Causal Reasoning Result: {reasoning_result}",
                                "source": "ReasoningScheduler",
                                "type": "reasoning",
                                "confidence": reasoning_step.confidence
                            })
                        else:
                            print(f"   [Reasoning] âš ï¸ Low confidence ({reasoning_step.confidence:.2f}), falling back to LLM planner")
                    except Exception as e:
                        print(f"   [Reasoning] âš ï¸ Reasoning failed: {e}, falling back to LLM planner")

                # ğŸ†• [2026-01-30] P0ä¿®å¤: å…ƒè®¤çŸ¥å±‚è¯„ä¼° - æ™ºèƒ½è¿‡æ»¤ç‰ˆæœ¬
                meta_cognitive_report = None
                if self.meta_cognitive_layer and self.meta_filter:
                    try:
                        # ä½¿ç”¨æ™ºèƒ½è¿‡æ»¤å™¨åˆ¤æ–­æ˜¯å¦è¯„ä¼°
                        goal_type_val = current_goal.goal_type.value if hasattr(current_goal.goal_type, 'value') else str(current_goal.goal_type)
                        should_eval, filter_reason = self.meta_filter.should_evaluate(
                            task=current_goal.description,
                            context={
                                "goal_type": goal_type_val,
                                "priority": current_goal.priority,
                                "complexity": getattr(current_goal, 'complexity', 0.5)
                            }
                        )
                        
                        if should_eval:
                            print(f"   [MetaCog] ğŸ§  å¯åŠ¨å…ƒè®¤çŸ¥è¯„ä¼° (é€šè¿‡è¿‡æ»¤: {filter_reason})...")
                            meta_cognitive_report = self.meta_cognitive_layer.evaluate_before_execution(
                                task=current_goal.description,
                                context={
                                    "goal_type": goal_type_val,
                                    "priority": current_goal.priority,
                                    "memory_context": memory_context
                                }
                            )
                            
                            # è®°å½•ç»“æœç”¨äºç»Ÿè®¡å‡é˜³æ€§
                            had_insight = (meta_cognitive_report.decision.value != "proceed" or 
                                          len(meta_cognitive_report.reasoning) > 0)
                            self.meta_filter.record_result(
                                task=current_goal.description,
                                context={"complexity": getattr(current_goal, 'complexity', 0.5)},
                                decision=meta_cognitive_report.decision.value,
                                had_insight=had_insight
                            )

                            # å¦‚æœå…ƒè®¤çŸ¥å±‚å»ºè®®æ‹’ç»æˆ–å‡çº§ï¼Œè·³è¿‡è¯¥ç›®æ ‡
                            if not meta_cognitive_report.should_proceed:
                                print(f"   [MetaCog] ğŸš« å…ƒè®¤çŸ¥å±‚å»ºè®®è·³è¿‡è¯¥ç›®æ ‡: {meta_cognitive_report.decision.value}")
                                print(f"   [MetaCog] ğŸ“Š ç†ç”±: {'; '.join(meta_cognitive_report.reasoning)}")

                                # æ ‡è®°ç›®æ ‡ä¸ºä¸å¯è¡Œ
                                mgr_class = self.goal_manager.__class__.__name__
                                if mgr_class == 'HierarchicalGoalManager':
                                    self.goal_manager.complete_goal(current_goal.id, success=False)
                                elif hasattr(self.goal_manager, 'fail_goal'):
                                    self.goal_manager.fail_goal(
                                        current_goal,
                                        f"Meta-cognitive evaluation declined: {meta_cognitive_report.decision.value}"
                                    )

                                # è·³è¿‡æœ¬tick
                                return

                            # å¦‚æœå»ºè®®è°¨æ…æ‰§è¡Œï¼Œé™ä½é¢„æœŸ
                            if meta_cognitive_report.decision.value == "proceed_with_caution":
                                print(f"   [MetaCog] âš ï¸ è°¨æ…æ‰§è¡Œæ¨¡å¼: ç½®ä¿¡åº¦ {meta_cognitive_report.overall_confidence:.2%}")
                        else:
                            # è¢«è¿‡æ»¤å™¨è·³è¿‡
                            if self.verbose:
                                print(f"   [MetaCog] â­ï¸  è·³è¿‡å…ƒè®¤çŸ¥è¯„ä¼° ({filter_reason})")

                    except Exception as e:
                        print(f"   [MetaCog] âš ï¸ å…ƒè®¤çŸ¥è¯„ä¼°å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()

                steps = await self.planner.decompose_task(
                    current_goal.description,
                    failed_steps=self.failed_steps_for_current_goal,
                    error_diagnosis=self.error_diagnosis,
                    memory_context=memory_context
                )
                if self.error_diagnosis:
                    print("   [Planner] âœ… Diagnosis info applied to new plan.")
                    self.error_diagnosis = None
                if not steps:
                    print("   [Planner] âš ï¸ æ— æ³•ç”Ÿæˆæœ‰æ•ˆæ­¥éª¤ï¼Œè·³è¿‡æœ¬Tickã€‚")
                    return
                self.current_plan = steps
                self.current_step_index = 0
                print(f"   [Planner] ğŸ“‹ è®¡åˆ’ç”Ÿæˆå®Œæ¯• (å…± {len(steps)} æ­¥)ã€‚")

            if self.current_step_index >= len(self.current_plan):
                print(f"   [System] ğŸ‰ å½“å‰è®¡åˆ’æ‰€æœ‰æ­¥éª¤å·²æ‰§è¡Œå®Œæ¯•ï¼Œæ ‡è®°ç›®æ ‡å®Œæˆã€‚")
                analysis_text = "\n".join(getattr(self, "_current_goal_step_results", []) or [])
                output_files = re.findall(r"data/entropy_investigation_\d+\.json", analysis_text)
                result_data = {
                    "result": "All steps executed successfully",
                    "analysis": analysis_text,
                }
                if output_files:
                    result_data["output_file"] = output_files[-1]
                mgr_class = self.goal_manager.__class__.__name__
                if mgr_class == 'HierarchicalGoalManager':
                    self.goal_manager.complete_goal(current_goal.id, success=True)
                else:
                    self.goal_manager.complete_goal(current_goal, result_data)
                
                # ğŸ”§ [2026-01-11] å®Œæˆéå…ƒè®¤çŸ¥ä»»åŠ¡æ—¶ï¼Œé€æ¸æ¢å¤å¥½å¥‡å¿ƒè§¦å‘èƒ½åŠ›
                if not ("[meta]" in current_goal.description.lower() and "investigate" in current_goal.description.lower()):
                    self._curiosity_satisfaction_decay = max(0.0, self._curiosity_satisfaction_decay - 0.05)
                    if self._curiosity_satisfaction_decay > 0:
                        print(f"   [Evolution] ğŸ“‰ Curiosity satisfaction decay reduced to {self._curiosity_satisfaction_decay:.2f}")
                
                self.current_plan = []
                self.current_step_index = 0
                return

            next_step = self.current_plan[self.current_step_index]
            print(f"   [Executor] ğŸ‘‰ æ‰§è¡Œæ­¥éª¤ {self.current_step_index + 1}/{len(self.current_plan)}: {next_step}")
            is_safe = await self.critic.check_safety(str(next_step))
            if not is_safe:
                print(f"   [Critic] ğŸ›‘ Step BLOCKED: Safety violation detected (Pre-check).")
                
                mgr_class = self.goal_manager.__class__.__name__
                if mgr_class == 'HierarchicalGoalManager':
                    self.goal_manager.complete_goal(current_goal.id, success=False)
                elif hasattr(self.goal_manager, 'fail_goal'):
                    self.goal_manager.fail_goal(current_goal, "Safety violation detected by Critic.")
                
                # [Memory] Internalize Safety Violation
                self.biological_memory.internalize_items([{
                    "content": f"Safety Violation Blocked: Action '{next_step}' was blocked by Critic. Reason: Unsafe operation.",
                    "source": "Critic_Safety_Block",
                    "timestamp": time.time(),
                    "tags": ["failure", "safety", "blocked"]
                }])
                print(f"   [System] ğŸ§  Safety violation internalized into Biological Memory.")

                self.current_plan = []
                self.current_step_index = 0
                return

            # ğŸ†• [2026-01-15] åŒèºæ—‹å†³ç­–å¢å¼º - åœ¨æ‰§è¡Œå‰ç”¨åŒç³»ç»Ÿæ™ºèƒ½è¯„ä¼°
            helix_enhancement = None
            if self.helix_decision_enabled and self.helix_engine:
                try:
                    # æ„å»ºå†³ç­–ä¸Šä¸‹æ–‡
                    decision_context = {
                        'goal': current_goal.description if current_goal else '',
                        'step_index': self.current_step_index,
                        'total_steps': len(self.current_plan),
                        'next_step': str(next_step),
                        'failed_count': len(self.failed_steps_for_current_goal),
                        'visual_context': self.context.get('visual_context', ''),
                        'audio_context': self.context.get('audio_last_heard', ''),
                        'seed_guidance': self.last_evolution_guidance or {},
                        'memory_count': len(memory_context) if 'memory_context' in dir() else 0
                    }
                    
                    helix_enhancement = await self._helix_enhanced_decision(decision_context)
                    
                    if helix_enhancement.get('enhanced'):
                        helix_conf = helix_enhancement.get('helix_confidence', 0)
                        fusion_method = helix_enhancement.get('fusion_method', 'unknown')
                        emergence = helix_enhancement.get('emergence_score', 0)
                        preference = helix_enhancement.get('complementary_preference', 'neutral')
                        
                        print(f"   [Helix] ğŸ§¬ Decision Enhancement Active:")
                        print(f"      Confidence: {helix_conf:.2f} | Method: {fusion_method}")
                        print(f"      Emergence: {emergence:.3f} | Preference: {preference}")
                        
                        # å¦‚æœæ˜¯åˆ›é€ æ€§å†³ç­–ï¼Œè®°å½•æ—¥å¿—
                        if helix_enhancement.get('is_creative'):
                            creative_action = helix_enhancement.get('creative_action_name', 'unknown')
                            print(f"   [Helix] âœ¨ Creative Decision Detected: {creative_action}")
                            
                            # å¦‚æœåˆ›é€ æ€§ç½®ä¿¡åº¦å¾ˆé«˜ä¸”å»ºè®®åœæ­¢è§‚å¯Ÿï¼Œè€ƒè™‘æš‚åœæ‰§è¡Œ
                            if creative_action == 'stop_and_observe' and helix_conf > 0.8:
                                print(f"   [Helix] ğŸ” High-confidence OBSERVE suggested - may pause for reflection")
                        
                        # è®°å½•åŒèºæ—‹å†³ç­–åˆ°ç”Ÿç‰©è®°å¿†
                        if self.biological_memory:
                            try:
                                self.biological_memory.internalize_items([{
                                    "content": f"Helix Decision: action={helix_enhancement.get('helix_action')}, "
                                               f"confidence={helix_conf:.2f}, method={fusion_method}, "
                                               f"emergence={emergence:.3f}, preference={preference}",
                                    "source": "DoubleHelixEngineV2",
                                    "timestamp": time.time(),
                                    "tags": ["decision", "helix", "fusion", preference.lower()]
                                }])
                            except Exception:
                                pass
                                
                except Exception as e:
                    print(f"   [Helix] âš ï¸ Enhancement failed: {e}")
                    helix_enhancement = {'enhanced': False, 'reason': str(e)}

            start_time = time.time()
            result = await self.executor.execute(next_step)
            try:
                if not hasattr(self, "_current_goal_step_results") or self._current_goal_step_results is None:
                    self._current_goal_step_results = []
                self._current_goal_step_results.append(str(result))
            except Exception:
                pass
            duration = time.time() - start_time
            self.existential_logger.log_ethos(str(next_step), duration)

            score = await self.critic.verify_outcome(str(next_step), str(result))
            print(f"   [Critic] ğŸ§ è¯„åˆ†: {score:.2f}")
            self.existential_logger.log_audit(f"Action: {next_step} | Result: {result}", score)
            try:
                if self.biological_memory is not None:
                    payload = {
                        "kind": "critic_score",
                        "score": float(score),
                        "threshold": 0.8,
                        "action": str(next_step),
                        "duration_s": float(duration),
                        "goal_id": current_goal.id if current_goal else None,
                        "goal": current_goal.description if current_goal else None,
                        "step_index": int(self.current_step_index),
                        "step_total": int(len(self.current_plan)) if isinstance(self.current_plan, list) else None,
                    }
                    self.biological_memory.record_online(
                        [
                            {
                                "id": f"critic_{int(time.time() * 1000)}",
                                "content": json.dumps(payload, ensure_ascii=False),
                                "source": "critic",
                                "type": "observation",
                                "tool": "verify_outcome",
                                "args": {"threshold": 0.8},
                            }
                        ],
                        connect_sequence=True,
                        seq_port="exec",
                        save=True,
                    )
            except Exception:
                pass

            if score >= 0.8:
                print(f"   [System] âœ… æ­¥éª¤æ‰§è¡ŒæˆåŠŸï¼Œå‡†å¤‡è¿›å…¥ä¸‹ä¸€æ­¥...")
                self.current_step_index += 1
                
                # ğŸ†• [2026-01-09] Sync successful experience to KnowledgeGraph
                # ğŸ†• [2026-01-30] P1ä¿®å¤: ä½¿ç”¨å­¤ç«‹èŠ‚ç‚¹é¢„é˜²
                try:
                    node_id = f"success_{int(time.time())}"
                    node_attrs = {
                        "node_type": "experience",
                        "properties": {
                            "step": str(next_step),
                            "score": score,
                            "timestamp": time.time()
                        }
                    }
                    if self.isolation_prevention:
                        self.isolation_prevention.add_node_with_prevention(node_id, **node_attrs)
                    elif hasattr(self, 'knowledge_graph'):
                        self.knowledge_graph.add_node(node_id, **node_attrs)
                    elif hasattr(self, 'memory'):
                        self.memory.add_node(node_id, **node_attrs)
                except Exception as e:
                    logger.warning(f"[Memory Bridge] âš ï¸ Failed to sync to KG: {e}")
            else:
                print(f"   [System] âŒ æ­¥éª¤æ‰§è¡Œå¤±è´¥ (å¾—åˆ† {score:.2f} < 0.80)")
                self.failed_steps_for_current_goal.append(str(next_step))
                result_str = str(result)

                # ğŸ†• [2026-01-16] P0ä¿®å¤: å¤±è´¥å½’å› åˆ†æ - åŒºåˆ†æ¶æ„é—®é¢˜vsæ•°æ®é—®é¢˜vså®ç°bug
                if self.meta_cognitive_layer:
                    try:
                        print(f"   [MetaCog] ğŸ” å¯åŠ¨å¤±è´¥å½’å› åˆ†æ...")
                        failure_analysis = self.meta_cognitive_layer.analyze_after_failure(
                            task=str(next_step),
                            result=result_str,
                            context={
                                "goal": current_goal.description,
                                "score": score,
                                "step_index": self.current_step_index,
                                "failed_attempts": len(self.failed_steps_for_current_goal)
                            }
                        )

                        # æ ¹æ®å½’å› ç»“æœè°ƒæ•´ç­–ç•¥
                        if failure_analysis.root_cause.value == "architectural":
                            print(f"   [MetaCog] ğŸ—ï¸ æ¶æ„é—®é¢˜æ£€æµ‹: éœ€è¦ç³»ç»Ÿçº§ä¿®å¤")
                            print(f"   [MetaCog] ğŸ’¡ å»ºè®®: {'; '.join(failure_analysis.improvement_suggestions[:2])}")
                            # æ¶æ„é—®é¢˜é€šå¸¸éœ€è¦æ”¾å¼ƒå½“å‰ç›®æ ‡
                            if len(self.failed_steps_for_current_goal) >= 1:  # æ¶æ„é—®é¢˜ç«‹å³æ”¾å¼ƒ
                                self.goal_manager.abandon_goal(
                                    current_goal,
                                    f"Architectural limitation detected: {failure_analysis.failure_type.value}"
                                )
                                self.current_plan = []
                                self.current_step_index = 0

                        elif failure_analysis.root_cause.value == "data":
                            print(f"   [MetaCog] ğŸ“Š æ•°æ®é—®é¢˜æ£€æµ‹: éœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®")
                            print(f"   [MetaCog] ğŸ’¡ å»ºè®®: {'; '.join(failure_analysis.improvement_suggestions[:2])}")
                            # æ•°æ®é—®é¢˜å¯ä»¥ç»§ç»­å°è¯•ï¼ˆå¯èƒ½ä¸‹æ¬¡æˆåŠŸï¼‰

                        elif failure_analysis.root_cause.value == "implementation":
                            print(f"   [MetaCog] ğŸ› å®ç°é—®é¢˜æ£€æµ‹: éœ€è¦è°ƒè¯•ä»£ç ")
                            # å®ç°é—®é¢˜è®°å½•åˆ°error_diagnosisä¾›ä¸‹æ¬¡è§„åˆ’ä½¿ç”¨
                            self.error_diagnosis = f"Implementation bug: {failure_analysis.failure_type.value}"

                    except Exception as e:
                        print(f"   [MetaCog] âš ï¸ å¤±è´¥å½’å› åˆ†æå¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()

                if "Traceback" in result_str or "Error:" in result_str:
                    
                    # [Memory] Internalize Repeated Failures
                    self.biological_memory.internalize_items([{
                        "content": f"Goal Abandoned: '{current_goal.description}' failed after 3 attempts. Failed steps: {self.failed_steps_for_current_goal}",
                        "source": "Execution_Failure",
                        "timestamp": time.time(),
                        "tags": ["failure", "abandoned", "execution_error"]
                    }])
                    print(f"   [System] ğŸ§  Execution failure internalized into Biological Memory.")

                    print("   [System] ğŸ” Detecting Error... Analyzing Traceback...")
                    try:
                        diagnosis = self.system_tools.analyze_traceback(result_str)
                        print(f"   [System] ğŸ§  Error Diagnosis: {diagnosis}")
                        self.error_diagnosis = diagnosis
                    except Exception as e:
                        print(f"   [System] âš ï¸ Diagnosis failed: {e}")
                if len(self.failed_steps_for_current_goal) >= 3:
                    print("   [System] ğŸš« è¿ç»­å¤±è´¥è¿‡å¤šï¼Œæ”¾å¼ƒå½“å‰ç›®æ ‡ã€‚")
                    self.goal_manager.abandon_goal(current_goal, "Too many failed attempts")
                    self.current_plan = []
                    self.current_step_index = 0
                else:
                    print("   [System] ğŸ”„ è®¡åˆ’é‡æ–°ç”Ÿæˆä¸­...")
                    self.current_plan = []
                    self.current_step_index = 0

            self.memory.add_decision_node(
                context=current_goal.description,
                decision=str(next_step),
                outcome=score,
                metadata={
                    "result": result,
                    # ğŸ†• [2026-01-15] åŒèºæ—‹å†³ç­–å…ƒæ•°æ®
                    "helix_enhanced": helix_enhancement.get('enhanced', False) if helix_enhancement else False,
                    "helix_confidence": helix_enhancement.get('helix_confidence', 0) if helix_enhancement else 0,
                    "helix_fusion_method": helix_enhancement.get('fusion_method', 'none') if helix_enhancement else 'none',
                    "helix_emergence": helix_enhancement.get('emergence_score', 0) if helix_enhancement else 0,
                    "helix_preference": helix_enhancement.get('complementary_preference', 'none') if helix_enhancement else 'none'
                }
            )
            self.memory.save_graph()

            if self.step_count % 5 == 0:
                total_iteration = len(self.meaning_explorer.exploration_history) + 1
                exploration_result = await self.meaning_explorer.explore_iteration(total_iteration)
                print(f"   [Soul] ğŸ¦‰ Insight (Iter #{total_iteration}): {exploration_result.meaning_hypothesis[:100]}...")
                if self.step_count % 10 == 0:
                    self.meaning_explorer.save_state()
                if self.step_count % 20 == 0:
                    directive = random.choice(self.core_identity.core_directives)
                    print(f"   [Constitution] âš–ï¸  Reflecting on: {directive}")

            intuition_score = 0.0
            if hasattr(self.semantic_memory, 'retrieve_intuition'):
                try:
                    stimulus = f"{current_goal.description} {self.context.get('visual_context', '')[:50]}"
                    intuition_score = await self.semantic_memory.retrieve_intuition(stimulus)
                except Exception as e:
                    print(f"   [Memory] âš ï¸ Intuition retrieval failed: {e}")

            evo_context = {
                "step": self.step_count,
                "goal": current_goal.description,
                "action": str(next_step),
                "result": str(result),
                "score": score,
                "visual": self.context.get("visual_context", ""),
                "intuition_confidence": intuition_score
            }

            seed_intuition = {}
            if self.evolution_controller:
                try:
                    evolution_guidance = await self.evolution_controller.step(evo_context)
                    seed_intuition = evolution_guidance.get("seed_guidance", {}) or {}
                except Exception as e:
                    print(f"   [System] âš ï¸ Evolution step failed: {e}")
                    seed_intuition = {}
            else:
                print("   [System] âš ï¸ Evolution Controller unavailable, using fallback intuition.")

            # ğŸ”§ [2026-01-29] MASLOW_MOTIVATION_INTEGRATION: Respond to boredom-driven exploration
            if hasattr(self.motivation, 'needs_exploration_trigger'):
                if self.motivation.needs_exploration_trigger:
                    print(f"   [Motivation] ğŸ¥± Boredom trigger detected! Forcing exploration mode...")
                    # é‡ç½®æ ‡å¿—ä½
                    self.motivation.needs_exploration_trigger = False

                    # å¼ºåˆ¶ä¸‹ä¸€æ¬¡ç›®æ ‡ç”Ÿæˆè¿›å…¥æ¢ç´¢æ¨¡å¼
                    # é€šè¿‡è®¾ç½®æ ‡å¿—ä½è®© _generate_survival_goal æ£€æµ‹
                    if not hasattr(self, '_force_exploration_mode'):
                        self._force_exploration_mode = False
                    self._force_exploration_mode = True

                    print(f"   [Motivation] âœ… Exploration flag set for next goal generation")

            # ğŸ”§ [2026-01-29] EVOLUTION_SUGGESTION_RECORDING: Store high-confidence evolution suggestions
            if seed_intuition:
                suggested_action = seed_intuition.get('suggested_action', '')
                confidence = seed_intuition.get('confidence', 0.0)

                # å¦‚æœç³»ç»Ÿå»ºè®®"åˆ›é€ "æˆ–"æ”¹è¿›"ï¼Œä¸”ç½®ä¿¡åº¦é«˜ï¼Œè®°å½•ä¾›åç»­ä½¿ç”¨
                if suggested_action in ['create', 'improve', 'experiment'] and confidence > 0.7:
                    print(f"   [Evolution] ğŸ’¡ High-confidence {suggested_action} suggestion (confidence: {confidence:.2f})")

                    # ä¸ç«‹å³åˆ›å»ºç›®æ ‡ï¼ˆé¿å…æ‰“æ–­å½“å‰æµç¨‹ï¼‰
                    # è€Œæ˜¯è®°å½•ä¸‹æ¥ï¼Œè®©ä¸‹ä¸€æ¬¡ç›®æ ‡ç”Ÿæˆè€ƒè™‘
                    if not hasattr(self, '_evolution_suggestion'):
                        self._evolution_suggestion = {}

                    self._evolution_suggestion = {
                        'action': suggested_action,
                        'confidence': confidence,
                        'timestamp': time.time(),
                        'insight': seed_intuition.get('insight_trigger', 'No insight provided')
                    }

                    print(f"   [Evolution] ğŸ“ Suggestion recorded for next goal generation")

                print("   [System] âš ï¸ Evolution Controller unavailable, using fallback intuition.")
            if not seed_intuition:
                idle_seconds = time.time() - self.last_insight_creation_ts

                # ğŸ”§ [2026-01-29] FIXED: Replace hardcoded 600s timeout with adaptive curiosity
                # Curiosity now grows LOGARITHMICALLY with idle time, not binary
                # This mimics real intelligence: curiosity builds gradually, not suddenly
                import math

                # Base curiosity starts at 0.3, grows with log(idle_seconds)
                # After 60s: 0.3 + log(60)/20 â‰ˆ 0.47
                # After 300s: 0.3 + log(300)/20 â‰ˆ 0.59
                # After 600s: 0.3 + log(600)/20 â‰ˆ 0.67 (old threshold)
                # After 1800s: 0.3 + log(1800)/20 â‰ˆ 0.77
                idle_curiosity = 0.3 + min(0.5, math.log(max(1, idle_seconds)) / 20.0)

                # ğŸ”§ [2026-01-29] FIXED: Add motivation-based curiosity boost
                # Check actual motivation state, not just time
                if hasattr(self, 'motivation') and self.motivation:
                    # Higher boredom = higher curiosity (natural psychological mechanism)
                    boredom_boost = (self.motivation.boredom / 100.0) * 0.3  # Max +0.3
                    # Lower satisfaction = higher curiosity (dissatisfaction drives change)
                    satisfaction_penalty = ((100 - self.motivation.satisfaction) / 100.0) * 0.2  # Max +0.2

                    fallback_curiosity = min(1.0, idle_curiosity + boredom_boost + satisfaction_penalty)
                else:
                    fallback_curiosity = idle_curiosity

                seed_intuition = {
                    "intrinsic_curiosity": fallback_curiosity,
                    "entropy": fallback_curiosity,
                    "suggested_action": "create" if fallback_curiosity >= 0.7 else "explore",
                    "intuition_confidence": 0.0,
                    "insight_trigger": "fallback_adaptive"
                }
            self.last_evolution_guidance = seed_intuition

            if seed_intuition:
                print(f"   [The Seed] ğŸ§¬ Intuition: {seed_intuition}")
            
            # ğŸ†• [2026-01-09] Trigger Foraging Agent for Active Learning
            intrinsic_curiosity = seed_intuition.get("intrinsic_curiosity", 0.0)
            entropy = seed_intuition.get("entropy", 0.0)

            # ğŸ†• [2026-01-16] P0ä¿®å¤ï¼šç†µå€¼è°ƒèŠ‚ - ç›‘æ§å¹¶è°ƒèŠ‚ç†µå€¼ï¼Œç»´æŒé•¿æœŸä¸­ç†µçŠ¶æ€
            if hasattr(self, 'entropy_regulator') and self.entropy_regulator:
                try:
                    # è®°å½•ç†µå€¼
                    metrics = self.entropy_regulator.record_entropy(entropy)

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒèŠ‚
                    should_regulate, reason = self.entropy_regulator.should_regulate(metrics)

                    if should_regulate:
                        print(f"   [EntropyRegulator] âš ï¸ ç†µå€¼å¼‚å¸¸: {reason}")
                        print(f"   [EntropyRegulator]    - å½“å‰ç†µå€¼: {metrics.current_entropy:.3f}")
                        print(f"   [EntropyRegulator]    - å¹³å‡ç†µå€¼: {metrics.average_entropy:.3f}")
                        print(f"   [EntropyRegulator]    - è¶‹åŠ¿: {metrics.entropy_trend}")

                        # å‡†å¤‡ä¸Šä¸‹æ–‡
                        # ğŸ†• [2026-01-17] P0ä¿®å¤ï¼šæ·»åŠ evolution_controllerå¼•ç”¨ä»¥æ”¯æŒæ ¸å¿ƒçŠ¶æ€é‡ç½®
                        regulation_context = {
                            'working_memory': self.working_memory if hasattr(self, 'working_memory') else None,
                            'semantic_memory': self.semantic_memory if hasattr(self, 'semantic_memory') else None,
                            'evolution_controller': self.evolution_controller if hasattr(self, 'evolution_controller') else None
                        }

                        # æ‰§è¡Œç†µå€¼è°ƒèŠ‚
                        result = self.entropy_regulator.regulate_entropy(metrics, regulation_context)

                        if result.get('regulated', False):
                            print(f"   [EntropyRegulator] âœ… å·²æ‰§è¡Œç†µå€¼è°ƒèŠ‚")
                            print(f"   [EntropyRegulator]    - ç­–ç•¥: {result.get('strategy', 'unknown')}")
                            print(f"   [EntropyRegulator]    - è€—æ—¶: {result.get('duration', 0)}ç§’")
                            print(f"   [EntropyRegulator]    - ç†µå€¼å˜åŒ–: {result.get('entropy_before', 0):.3f} â†’ {result.get('entropy_after', 0):.3f}")

                except Exception as e:
                    logger.warning(f"[EntropyRegulator] âš ï¸ ç†µå€¼è°ƒèŠ‚å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()

            # ğŸ†• [2026-01-17] å®šæœŸæ›´æ–°çŸ¥è¯†å›¾è°±æ•°æ®
            if self.step_count % 10 == 0 and hasattr(self, 'knowledge_graph_exporter') and self.knowledge_graph_exporter:
                try:
                    self.knowledge_graph_exporter.update_from_agi_system(self)
                    if self.step_count % 30 == 0:  # æ¯30ä¸ªstepè¾“å‡ºä¸€æ¬¡æ—¥å¿—
                        stats = self.knowledge_graph_exporter.get_stats()
                        print(f"   [KnowledgeGraph] ğŸ“Š çŸ¥è¯†å›¾è°±å·²æ›´æ–°: {stats['nodes_count']}ä¸ªèŠ‚ç‚¹, {stats['links_count']}æ¡è¾¹")
                except Exception as e:
                    logger.warning(f"[KnowledgeGraph] âš ï¸ çŸ¥è¯†å›¾è°±æ›´æ–°å¤±è´¥: {e}")

            if hasattr(self, 'foraging_agent'):
                try:
                    foraging_result = self.foraging_agent.execute_foraging(
                        curiosity=intrinsic_curiosity,
                        entropy=entropy,
                        knowledge_graph=self.knowledge_graph if hasattr(self, 'knowledge_graph') else None,
                        memory_system=self.biological_memory,
                        current_context=current_goal.description if current_goal else ""
                    )

                    if foraging_result:
                        print(f"   [ForagingAgent] ğŸ¯ Active exploration triggered: {foraging_result['target']['location']}")
                        # å¯ä»¥å°†æ¢ç´¢è¡ŒåŠ¨è½¬æ¢ä¸ºæ–°çš„Goal
                        # self.goal_manager.add_goal(...)
                except Exception as e:
                    logger.warning(f"[ForagingAgent] âš ï¸ Foraging failed: {e}")
            create_requested = seed_intuition.get("suggested_action") == "create" or intrinsic_curiosity >= 0.7
            now_ts = time.time()
            if create_requested and (now_ts - self.last_insight_creation_ts) >= 120:
                print(f"   [The Seed] ğŸŒ‹ CREATION IMPULSE DETECTED (Curiosity: {intrinsic_curiosity:.2f}). Generating Insight...")
                insight_prompt = f"""
                You are the 'Subconscious Creative Engine' of an AGI.
                The system is in a state of HIGH ENTROPY (Curiosity: {intrinsic_curiosity:.2f}).
                Context: {current_goal.description}
                
                Generate a specific, novel insight, hypothesis, or small code snippet that resolves this entropy.
                Format: Plain text or Code.
                """
                try:
                    insight_content = self.llm_service.chat_completion(system_prompt="Creative Engine", user_prompt=insight_prompt)
                    
                    # ğŸ†• é€šè¿‡æ¡¥æ¥å±‚å¤„ç†å“åº”ï¼ˆæ‰§è¡Œå·¥å…·è°ƒç”¨ï¼‰
                    insight_content = await self._process_llm_response_with_bridge(insight_content)
                    
                    # Save Insight
                    node_id = f"Insight_{now_ts}"
                    insight_data = {
                        "content": insight_content,
                        "trigger_goal": current_goal.description,
                        "timestamp": now_ts,
                        "entropy_score": intrinsic_curiosity,
                        "bridge_validation": "PENDING",
                        "node_id": node_id
                    }
                    
                    # --- Neuro-Symbolic Validation ---
                    # ğŸ”§ [2026-01-29] FIXED: Use REAL semantic encoding instead of random vectors
                    # This replaces the fake "simulated_vec = np.random.rand(128)" with actual perception
                    import numpy as np

                    # Generate REAL semantic vector for the insight content
                    if hasattr(self, 'perception_system') and self.perception_system:
                        real_vec = self.perception_system.encode_text(insight_content)
                        # Ensure dimension matches what neuro_bridge expects (128 or 384)
                        if real_vec.shape[0] != 128:
                            # Truncate or pad to 128 dimensions
                            if real_vec.shape[0] > 128:
                                real_vec = real_vec[:128]
                            else:
                                real_vec = np.pad(real_vec, (0, 128 - real_vec.shape[0]))
                        semantic_vector = real_vec
                    else:
                        # Fallback: deterministic hash-based projection (better than random)
                        import hashlib
                        hash_seed = int(hashlib.md5(insight_content.encode()).hexdigest(), 16) % (2**32)
                        rng = np.random.default_rng(hash_seed)
                        semantic_vector = rng.standard_normal(128)

                    validation = self.neuro_bridge.evaluate_neuro_symbolic_state(
                        concept_id=f"insight_{now_ts}",
                        current_vector=semantic_vector,
                        related_concepts=["survival", "learning"] # Simplified relations
                    )
                    
                    insight_data["bridge_validation"] = validation
                    
                    if validation["recommended_action"] == "REJECT_NOISE":
                         print(f"   [Bridge] âš ï¸ Insight REJECTED due to Semantic Drift (Confidence: {validation['confidence']:.2f})")
                         # Optionally discard or flag
                    else:
                        if validation["status"] == "PARADIGM_SHIFT":
                            print(f"   [Bridge] ğŸŒŸ PARADIGM SHIFT Detected! (Surprise: {validation['surprise']:.2f})")
                    
                    # ğŸ†• [2026-01-30] P1ä¿®å¤: ä½¿ç”¨å­¤ç«‹èŠ‚ç‚¹é¢„é˜²
                    node_attrs = {"type": "insight", "content": insight_content[:50]}
                    if self.isolation_prevention:
                        self.isolation_prevention.add_node_with_prevention(node_id, **node_attrs)
                    else:
                        self.memory.add_node(node_id, **node_attrs)
                    
                    # Update Bridge Topology
                    self.neuro_bridge.update_topology(
                        nodes=[node_id],
                        edges=[(node_id, "System")]
                    )
                    
                    self.last_insight_creation_ts = now_ts
                    
                    # Emit Event
                    await self.event_bus.publish("insight_generated", insight_data)
                    
                    # ğŸ†• [2026-01-09] Complete Validation-Integration-Evaluation Loop
                    skill_code = self.skill_manager.extract_code_from_markdown(insight_content)
                    if skill_code:
                        skill_name = self.skill_manager.save_skill(skill_code, name_hint=f"insight_{int(now_ts)}")
                        if skill_name:
                            print(f"   [Skill] ğŸ› ï¸  New Skill extracted and saved: {skill_name}")
                            
                            # âœ… Step 1: VALIDATE - éªŒè¯ä»£ç è´¨é‡
                            print(f"   [Validator] ğŸ” Validating insight...")
                            validation_result = self.insight_validator.validate(
                                code=skill_code,
                                insight_metadata={
                                    'trigger_goal': current_goal.description,
                                    'content': insight_content,
                                    'entropy': intrinsic_curiosity
                                }
                            )
                            
                            print(f"   [Validator] ğŸ“Š Score={validation_result['score']:.2f}, Recommendation={validation_result['recommendation']}")
                            if validation_result['errors']:
                                print(f"   [Validator] âŒ Errors: {', '.join(validation_result['errors'])}")
                            if validation_result['warnings']:
                                print(f"   [Validator] âš ï¸  Warnings: {', '.join(validation_result['warnings'])}")
                            
                            # âœ… Step 2: INTEGRATE - é€‰æ‹©æ€§é›†æˆ
                            if validation_result['recommendation'] == 'INTEGRATE':
                                integration_result = self.insight_integrator.integrate(
                                    skill_name=skill_name,
                                    validation_result=validation_result
                                )
                                
                                if integration_result['integrated']:
                                    # âœ… Step 3: A/B TEST - éªŒè¯å®é™…æ•ˆæœ(å¯é€‰,éœ€è¦å®šä¹‰æµ‹è¯•å‡½æ•°)
                                    # ab_result = self.insight_integrator.run_ab_test(
                                    #     skill_name=skill_name,
                                    #     test_function=lambda: self._measure_system_performance(),
                                    #     iterations=5
                                    # )
                                    # if ab_result.get('recommendation') == 'ROLLBACK':
                                    #     self.insight_integrator.rollback(skill_name)
                                    
                                    # âœ… Step 4: EVALUATE - è®°å½•åˆ°è¯„ä¼°ç³»ç»Ÿ
                                    self.insight_evaluator.record_call(
                                        skill_name=skill_name,
                                        success=True,
                                        execution_time=validation_result['execution_time']
                                    )
                                    
                                    print(f"   [Loop] âœ… Insight {skill_name} æˆåŠŸé›†æˆå¹¶å¼€å§‹è¿½è¸ª")
                            
                            elif validation_result['recommendation'] == 'ARCHIVE':
                                print(f"   [Loop] ğŸ“¦ Insight {skill_name} è´¨é‡ä¸è¶³,å·²å½’æ¡£å¾…æ”¹è¿›")
                            
                            else:  # REJECT
                                print(f"   [Loop] ğŸ—‘ï¸  Insight {skill_name} è´¨é‡è¿‡ä½,å·²æ‹’ç»")
                            
                            # Internalize Skill Acquisition (æ— è®ºæ˜¯å¦é›†æˆ)
                            self.biological_memory.internalize_items([{
                                "content": f"New Skill: {skill_name}. Validation Score: {validation_result['score']:.2f}. Status: {validation_result['recommendation']}",
                                "source": "Insight_Generation",
                                "timestamp": now_ts,
                                "tags": ["skill", "insight", validation_result['recommendation'].lower(), skill_name]
                            }])
                except Exception as e:
                    print(f"   [System] âš ï¸ Creation Failed: {e}")
            elif create_requested:
                remaining = int(max(0.0, 120 - (now_ts - self.last_insight_creation_ts)))
                print(f"   [The Seed] ğŸ•’ Insight creation cooldown active ({remaining}s remaining).")
            elif intrinsic_curiosity > 0.5:
                # ğŸ”§ [2026-01-11] å…ƒè®¤çŸ¥è°ƒæŸ¥å†·å´æ£€æŸ¥ - é˜²æ­¢ç©ºè½¬å¾ªç¯
                meta_cooldown_remaining = self._meta_investigation_cooldown - (now_ts - self._last_meta_investigation_ts)
                effective_curiosity = max(0.0, intrinsic_curiosity - self._curiosity_satisfaction_decay)
                
                if meta_cooldown_remaining > 0:
                    print(f"   [The Seed] ğŸ•’ Meta-investigation cooldown active ({int(meta_cooldown_remaining)}s remaining). Effective curiosity: {effective_curiosity:.2f}")
                elif effective_curiosity > 0.5 and current_goal.priority != "highest":
                    print(f"   [The Seed] ğŸ’¡ High Curiosity Detected ({effective_curiosity:.2f}). Proposing evidence-based investigation...")
                    
                    # ğŸ”§ [2026-01-11] ä½¿ç”¨ WorkTemplates åˆ›å»ºå¸¦æœ‰æ˜ç¡®éªŒè¯æ ‡å‡†çš„ç›®æ ‡
                    from core.goal_system import WorkTemplates
                    entropy_val = seed_intuition.get('entropy', 1.0)
                    investigation_goal = WorkTemplates.meta_cognitive_investigation(entropy_val, effective_curiosity)
                    
                    # æ·»åŠ åˆ°ç›®æ ‡æ ˆ
                    self.goal_manager.goal_stack.append(investigation_goal)
                    self.goal_manager.stats["total_created"] += 1
                    
                    # æ›´æ–°å†·å´æ—¶é—´æˆ³
                    self._last_meta_investigation_ts = now_ts
                    # è°ƒæŸ¥åå¢åŠ å¥½å¥‡å¿ƒæ»¡è¶³è¡°å‡ï¼ˆä¸‹æ¬¡è§¦å‘éœ€è¦æ›´é«˜å¥½å¥‡å¿ƒï¼‰
                    self._curiosity_satisfaction_decay = min(0.3, self._curiosity_satisfaction_decay + 0.1)
                    
                    print(f"   [The Seed] ğŸ”¬ Created investigation goal: {investigation_goal.id} with evidence requirements")
                else:
                    print(f"   [The Seed] ğŸ’­ Curiosity ({effective_curiosity:.2f}) below threshold or goal blocked. Observing...")

            if seed_intuition.get("suggested_action") == "rest" and seed_intuition.get("survival_drive", 1.0) < 0.3:
                print("   [System] ğŸ’¤ The Seed suggests resting. Triggering Dream State...")
                await self.evolution_controller.dream()
                if hasattr(self.semantic_memory, 'forget_and_consolidate'):
                    print("   [Memory] ğŸ§¹ Triggering LRU Forgetting & Consolidation (with Bridge Metrics)...")
                    await self.semantic_memory.forget_and_consolidate(bridge=self.neuro_bridge)
        finally:
            try:
                import platform
                
                # Get Bridge Metrics for Logging
                nsb_metrics = self.neuro_bridge.get_system_metrics()
                
                cycle_data = {
                    "timestamp": time.time(),
                    "cycle_id": cycle_id,
                    "step": self.step_count,
                    "goal": current_goal.to_dict() if current_goal else None,
                    "plan": str(next_step) if next_step is not None else None,
                    "execution": {
                        "result": str(result) if result is not None else None,
                        "duration": float(duration) if duration else 0.0
                    },
                    "verification": {"score": float(score) if score else 0.0},
                    "evolution": seed_intuition,
                    "neuro_symbolic": {
                        "drift": nsb_metrics.get("avg_drift", 0.0),
                        "surprise": nsb_metrics.get("avg_surprise", 0.0),
                        "anchors": nsb_metrics.get("anchors_count", 0)
                    },
                    "context": {
                        "user_focus": active_app,
                        "platform": platform.system()
                    }
                }
                self.existential_logger.log_cycle_flow(cycle_data)
            except Exception as e:
                print(f"   [System] âš ï¸ Logging Error: {e}")

    def _save_life_snapshot(self):
        """
        Save a snapshot of the system's 'Life State' (Growth Curve).
        Records: Time, Entropy (Memory Size), Experience (Steps), and Vitality (Nodes).
        
        ğŸ†• [2026-01-09] åŒæ—¶è§¦å‘æ´å¯Ÿè¯„ä¼°å’Œæ¸…ç†
        """
        try:
            snapshot_dir = "data/life_state"
            os.makedirs(snapshot_dir, exist_ok=True)
            snapshot_file = os.path.join(snapshot_dir, "growth_curve.jsonl")
            
            # ğŸ†• æ¯10æ¬¡å¿«ç…§æ‰§è¡Œä¸€æ¬¡æ´å¯Ÿæ¸…ç†ï¼ˆçº¦æ¯100æ­¥ï¼‰
            if self.step_count % 100 == 0 and hasattr(self, 'insight_evaluator'):
                print(f"   [Evaluator] ğŸ“Š ç”Ÿæˆæ´å¯Ÿè¯„ä¼°æŠ¥å‘Š...")
                report = self.insight_evaluator.generate_report(top_n=5)
                
                # ä¿å­˜æŠ¥å‘Š
                report_file = f"data/life_state/insight_report_{int(time.time())}.json"
                with open(report_file, 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False)
                
                print(f"   [Evaluator] ğŸ“ˆ æ€»æ´å¯Ÿ: {report['summary']['total_insights']}, "
                      f"å¥åº·: {report['summary']['healthy']}, "
                      f"è­¦å‘Š: {report['summary']['warning']}, "
                      f"å±æ€¥: {report['summary']['critical']}")
                
                # è‡ªåŠ¨æ¸…ç†å»ºè®®å¼ƒç”¨çš„æ´å¯Ÿ
                deprecated_skills = [item['name'] for item in report['deprecated']]
                if deprecated_skills:
                    print(f"   [Evaluator] ğŸ—‘ï¸  æ¸…ç†{len(deprecated_skills)}ä¸ªä½æ•ˆæ´å¯Ÿ...")
                    self.insight_evaluator.cleanup_deprecated(deprecated_skills)
                
                # å½’æ¡£ä½è¯„åˆ†æ´å¯Ÿï¼ˆä½¿ç”¨Integratorï¼‰
                if hasattr(self, 'insight_integrator'):
                    archived = self.insight_integrator.archive_low_performers(threshold=0.6)
                    if archived:
                        print(f"   [Integrator] ğŸ“¦ å½’æ¡£{len(archived)}ä¸ªä½åˆ†æ´å¯Ÿ")
            
            # Gather Vital Signs
            mem_stats = self.biological_memory.get_stats()
            
            snapshot = {
                "timestamp": time.time(),
                "step_age": self.step_count,
                "memory_nodes": mem_stats.get("nodes", 0),
                "memory_items": mem_stats.get("memories", 0),
                "goals_completed": 0, # TODO: Track this in GoalManager
                "evolution_generation": self.evolution_controller.generation if hasattr(self.evolution_controller, 'generation') else 0
            }
            
            with open(snapshot_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(snapshot) + "\n")
                
        except Exception as e:
            print(f"   [LifeEngine] âš ï¸ Failed to save life snapshot: {e}")

    def run_forever(self, use_existing_loop=False):
        # --- [GEMINI INJECTION] Neural Memory Handshake ---
        if not hasattr(self, "biological_memory") or self.biological_memory is None:
            print("   [System] âš ï¸ æ£€æµ‹åˆ°ç¥ç»è®°å¿†æ–­è£‚ï¼Œå°è¯•ç´§æ€¥é‡è¿...")
            try:
                from core.memory.neural_memory import BiologicalMemorySystem
                self.biological_memory = BiologicalMemorySystem()
                print("   [System] âœ… [NEURAL HANDSHAKE] ç´§æ€¥é‡è¿æˆåŠŸ: Connected to NeuralMemory")
                # é‡æ–°ç»‘å®šå·¥å…·
                if hasattr(self, "system_tools"):
                    self.system_tools.biological_memory = self.biological_memory
            except Exception as e:
                print(f"   [System] âŒ [NEURAL HANDSHAKE] é‡è¿å½»åº•å¤±è´¥: {e}")
        else:
            if hasattr(self.biological_memory, 'topology'):
                print(f"   [System] âœ… [NEURAL HANDSHAKE] ç¥ç»è¿æ¥æ­£å¸¸ (Nodes: {self.biological_memory.topology.size()})")
            else:
                print("   [System] âœ… [NEURAL HANDSHAKE] ç¥ç»è¿æ¥æ­£å¸¸ (Topology stats unavailable)")
        # --------------------------------------------------

        print("   [System] ğŸš€ AGI Life Engine Started (Organic Mode)")
        
        # Get or create event loop
        if use_existing_loop:
            try:
                loop = asyncio.get_running_loop()
                print("   [System] ğŸ”„ Using existing event loop")
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                print("   [System] ğŸ†• Created new event loop")
        else:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            print("   [System] ğŸ†• Created new event loop")
        
        # --- 0. Re-genesis (Memory Anchor Restoration) ---
        print("   [System] ğŸŒŒ Initiating System Re-genesis...")
        if EVOLUTION_AVAILABLE and self.evolution_controller:
            try:
                if use_existing_loop:
                    asyncio.create_task(perform_genesis(self.evolution_controller))
                else:
                    loop.run_until_complete(perform_genesis(self.evolution_controller))
            except Exception as e:
                print(f"   [System] âš ï¸ Re-genesis Warning: {e}")
        else:
            print("   [System] â¸ï¸  Re-genesis è·³è¿‡ (è¿›åŒ–åŠŸèƒ½ä¸å¯ç”¨)")
        
        try:
            if use_existing_loop:
                # Run in existing event loop - create async task
                async def _run_async():
                    while self.is_running:
                        try:
                            await self.run_step()
                            # [Life Cycle] Record Growth Snapshot every step
                            self._save_life_snapshot()
                        except Exception as e:
                            print(f"\n   [System] âš ï¸ Critical Error in Life Cycle: {e}")
                            import traceback
                            traceback.print_exc()
                            # Attempt to recover by resting
                            await asyncio.sleep(2)
                        await asyncio.sleep(2) # Breathe (ğŸ”§ [2026-01-27] é™ä½tické¢‘ç‡: 1s â†’ 2sï¼Œæå‡å¤–éƒ¨è¯·æ±‚å¤„ç†èƒ½åŠ›)
                
                # Create and schedule the async task
                loop.create_task(_run_async())
                print("   [System] âœ… Async task scheduled in existing loop")
            else:
                # Run in own event loop (original behavior)
                while self.is_running:
                    try:
                        loop.run_until_complete(self.run_step())
                        
                        # [Life Cycle] Record Growth Snapshot every step
                        self._save_life_snapshot()
                    except Exception as e:
                        print(f"\n   [System] âš ï¸ Critical Error in Life Cycle: {e}")
                        
                        # --- Phase 2.3: The All-Seeing Eye (Runtime Mapping) ---
                        try:
                            import traceback
                            tb = e.__traceback__
                            while tb:
                                frame = tb.tb_frame
                                # Inspect local variables in this frame
                                for var_name, var_value in frame.f_locals.items():
                                    # Check if this object is registered
                                    info = RuntimeMonitor.inspect_object(var_value)
                                    if info:
                                        print(f"   [Diagnosis] ğŸ‘ï¸ Object involved: '{var_name}' ({info['type']})")
                                        print(f"               Defined at: {info['file_path']}:{info['line_number']}")
                                tb = tb.tb_next
                        except Exception as diag_err:
                            print(f"   [Diagnosis] âš ï¸ Diagnosis failed: {diag_err}")
                        # -------------------------------------------------------

                        import traceback
                        traceback.print_exc()
                        # Attempt to recover by resting
                        time.sleep(2)
                    time.sleep(2) # Breathe (ğŸ”§ [2026-01-27] é™ä½tické¢‘ç‡: 1s â†’ 2sï¼Œæå‡å¤–éƒ¨è¯·æ±‚å¤„ç†èƒ½åŠ›)
        except KeyboardInterrupt:
            print("\n   [System] ğŸ›‘ Life Engine Paused.")
        finally:
            if hasattr(self, 'console_listener'):
                self.console_listener.stop()
            # Cleanup Perception
            if hasattr(self, 'perception') and self.perception:
                print("   [System] ğŸ›‘ Stopping Perception Sensors...")
                self.perception.stop_all()
            if hasattr(self, 'streaming_asr') and self.streaming_asr:
                self.streaming_asr.stop()

            # Cleanup Hardware Capture
            if hasattr(self, 'hardware_capture') and self.hardware_capture:
                print("   [System] ğŸ›‘ Stopping Hardware Capture (Camera & Microphone)...")
                self.hardware_capture.stop_all()

            if hasattr(self, 'meaning_explorer'):
                self.meaning_explorer.save_state()
                print("   [System] ğŸ’¾ Soul State Saved.")

            # ğŸ†• [2026-01-11] Shutdown M1-M4 Adapter
            if hasattr(self, 'm1m4_adapter') and self.m1m4_adapter:
                print("   [System] ğŸ§¬ Shutting down M1-M4 Fractal AGI Components...")
                self.m1m4_adapter.shutdown()
                print("   [System] âœ… M1-M4 Components shutdown complete.")

            # ğŸ†• [2026-01-24] ä¿å­˜ä¼šè¯çŠ¶æ€ - ä¸ºä¸‹æ¬¡å¯åŠ¨æä¾›ä¸Šä¸‹æ–‡
            if hasattr(self, 'session_restorer') and self.session_restorer:
                try:
                    # ç”Ÿæˆä¼šè¯æ‘˜è¦
                    session_summary = f"AGIä¼šè¯äº {time.strftime('%Y-%m-%d %H:%M:%S')} ç»“æŸ"
                    pending_tasks = []

                    # æ”¶é›†æœªå®Œæˆçš„ç›®æ ‡
                    if hasattr(self, 'goal_manager') and self.goal_manager:
                        active_goals = getattr(self.goal_manager, 'active_goals', [])
                        if active_goals:
                            pending_tasks = [g.get('description', str(g)) for g in active_goals[:5]]

                    # ä¿å­˜ä¼šè¯çŠ¶æ€
                    self.session_restorer.save_session_state(
                        summary=session_summary,
                        pending_tasks=pending_tasks
                    )
                    print("   [System] ğŸ’¾ Session Context Saved - ä¸‹æ¬¡å¯åŠ¨å°†è‡ªåŠ¨æ¢å¤")
                except Exception as e:
                    print(f"   [System] âš ï¸ Session state save failed: {e}")

            loop.close()

if __name__ == "__main__":
    # ğŸ”§ [2026-01-29] å•å®ä¾‹æ£€æµ‹
    if SINGLE_INSTANCE_AVAILABLE:
        if ensure_single_instance():
            sys.exit(1)
    
    try:
        engine = AGI_Life_Engine()
        engine.run_forever()
    except Exception:
        import traceback
        traceback.print_exc()
